{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:96: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:220: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:247: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:268: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:96: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:220: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:247: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:268: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/742696372.py:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/742696372.py:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND pe.global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/742696372.py:75: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/742696372.py:96: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/742696372.py:220: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/742696372.py:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND pe.global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/742696372.py:247: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/742696372.py:268: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import db_dtypes\n",
    "import bigframes.pandas as bpd\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def user_details(start_date, end_date,entity_d):\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH listing AS (    \n",
    "        SELECT   global_entity_id\n",
    "                ,country\n",
    "                ,CAST(DATE_TRUNC(injestion_time, MONTH) AS DATE) session_month\n",
    "                ,CAST(DATE_TRUNC(injestion_time, ISOWEEK) AS DATE) session_week\n",
    "                ,CAST(DATE_TRUNC(injestion_time, DAY) AS DATE) session_day\n",
    "                ,EXTRACT(HOUR FROM injestion_time) AS session_hour\n",
    "                ,session_key \n",
    "                ,perseus_session_id\n",
    "                ,chainId \n",
    "                ,shopId\n",
    "                ,userId\n",
    "                ,df_impressions \n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(pe.global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,userId\n",
    "                ,ingestion_timestamp injestion_time\n",
    "                ,country\n",
    "                ,COALESCE(chainId, JSON_VALUE(eventVariables_json, \"$.chainId\") ) AS chainId\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY ingestion_timestamp) row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")) df_raw\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events` pe\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'shop_impressions.loaded'\n",
    "                AND screenType = 'shop_list'\n",
    "                AND pe.global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "                AND locationCity IS NOT NULL\n",
    "                AND shopType = 'restaurants'\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        AND df_raw IS NOT NULL\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), shop_details AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'shop_details.loaded'\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), checkout AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'checkout.loaded'\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), orders AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction IN ('transaction')\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), rates AS (\n",
    "        SELECT cu.country_iso\n",
    "            ,cu.currency_code\n",
    "            ,tmp.fx_rate_eur\n",
    "        FROM `fulfillment-dwh-production.cl.countries` cu\n",
    "        JOIN ( \n",
    "            WITH latest_fx_rate AS (\n",
    "            SELECT \n",
    "                currency_code,\n",
    "                fx_rate_eur,\n",
    "                calculated_at,\n",
    "                ROW_NUMBER() OVER (PARTITION BY currency_code ORDER BY calculated_at DESC) AS rn\n",
    "            FROM `fulfillment-dwh-production.curated_data_shared_coredata.fx_rates`\n",
    "            )\n",
    "            SELECT \n",
    "                currency_code,\n",
    "                fx_rate_eur,\n",
    "                calculated_at AS max_calculated_at\n",
    "            FROM latest_fx_rate\n",
    "            WHERE rn = 1\n",
    "        ) tmp ON tmp.currency_code = cu.currency_code\n",
    "        GROUP BY 1, 2, 3\n",
    "        ORDER BY 1\n",
    "    )\n",
    "    SELECT      l.global_entity_id\n",
    "                ,l.userId\n",
    "                -- Delivery fee listing calculations\n",
    "                ,AVG(l.df_impressions / r.fx_rate_eur) AS delivery_fee_listing_eur_mean\n",
    "                ,APPROX_QUANTILES(l.df_impressions / r.fx_rate_eur, 100)[OFFSET(50)] AS delivery_fee_listing_eur_median\n",
    "                ,MIN(l.df_impressions / r.fx_rate_eur) AS delivery_fee_listing_eur_min\n",
    "                ,MAX(l.df_impressions / r.fx_rate_eur) AS delivery_fee_listing_eur_max\n",
    "                ,STDDEV(l.df_impressions / r.fx_rate_eur) AS delivery_fee_listing_eur_stddev\n",
    "                \n",
    "                -- Delivery fee details page calculations\n",
    "                ,AVG(sd.df_impressions / r.fx_rate_eur) AS delivery_fee_details_eur_mean\n",
    "                ,APPROX_QUANTILES(sd.df_impressions / r.fx_rate_eur, 100)[OFFSET(50)] AS delivery_fee_details_eur_median\n",
    "                ,MIN(sd.df_impressions / r.fx_rate_eur) AS delivery_fee_details_eur_min\n",
    "                ,MAX(sd.df_impressions / r.fx_rate_eur) AS delivery_fee_details_eur_max\n",
    "                ,STDDEV(sd.df_impressions / r.fx_rate_eur) AS delivery_fee_details_eur_stddev\n",
    "                \n",
    "                -- Delivery fee checkout calculations\n",
    "                ,AVG(co.df_impressions / r.fx_rate_eur) AS delivery_fee_checkout_eur_mean\n",
    "                ,APPROX_QUANTILES(co.df_impressions / r.fx_rate_eur, 100)[OFFSET(50)] AS delivery_fee_checkout_eur_median\n",
    "                ,MIN(co.df_impressions / r.fx_rate_eur) AS delivery_fee_checkout_eur_min\n",
    "                ,MAX(co.df_impressions / r.fx_rate_eur) AS delivery_fee_checkout_eur_max\n",
    "                ,STDDEV(co.df_impressions / r.fx_rate_eur) AS delivery_fee_checkout_eur_stddev\n",
    "                \n",
    "                -- Delivery fee order calculations\n",
    "                ,AVG(o.df_impressions / r.fx_rate_eur) AS delivery_fee_order_eur_mean\n",
    "                ,APPROX_QUANTILES(o.df_impressions / r.fx_rate_eur, 100)[OFFSET(50)] AS delivery_fee_order_eur_median\n",
    "                ,MIN(o.df_impressions / r.fx_rate_eur) AS delivery_fee_order_eur_min\n",
    "                ,MAX(o.df_impressions / r.fx_rate_eur) AS delivery_fee_order_eur_ma\n",
    "                ,STDDEV(o.df_impressions / r.fx_rate_eur) AS delivery_fee_order_eur_stddev\n",
    "                ,COUNT(l.shopId) AS total_vendor\n",
    "                ,SUM(CASE WHEN sd.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_details_page\n",
    "                ,SUM(CASE WHEN co.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_checkout\n",
    "                ,SUM(CASE WHEN o.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_order\n",
    "    FROM listing l\n",
    "    LEFT JOIN shop_details sd ON sd.global_entity_id = l.global_entity_id AND sd.session_key = l.session_key AND sd.shopId = l.shopId\n",
    "    LEFT JOIN checkout co ON co.global_entity_id = l.global_entity_id AND co.session_key = l.session_key AND co.shopId = l.shopId\n",
    "    LEFT JOIN orders o ON o.global_entity_id = l.global_entity_id AND o.session_key = l.session_key AND o.shopId = l.shopId\n",
    "    LEFT JOIN rates r ON l.country = r.country_iso\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "\n",
    "def user_conversion(start_date, end_date,entity_d):\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH listing AS (    \n",
    "        SELECT   global_entity_id\n",
    "                ,country\n",
    "                ,CAST(DATE_TRUNC(injestion_time, MONTH) AS DATE) session_month\n",
    "                ,CAST(DATE_TRUNC(injestion_time, ISOWEEK) AS DATE) session_week\n",
    "                ,CAST(DATE_TRUNC(injestion_time, DAY) AS DATE) session_day\n",
    "                ,EXTRACT(HOUR FROM injestion_time) AS session_hour\n",
    "                ,session_key \n",
    "                ,perseus_session_id\n",
    "                ,chainId \n",
    "                ,shopId\n",
    "                ,userId\n",
    "                ,df_impressions \n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(pe.global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,userId\n",
    "                ,ingestion_timestamp injestion_time\n",
    "                ,country\n",
    "                ,COALESCE(chainId, JSON_VALUE(eventVariables_json, \"$.chainId\") ) AS chainId\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY ingestion_timestamp) row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")) df_raw\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events` pe\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'shop_impressions.loaded'\n",
    "                AND screenType = 'shop_list'\n",
    "                AND pe.global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "                AND locationCity IS NOT NULL\n",
    "                AND shopType = 'restaurants'\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        AND df_raw IS NOT NULL\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), shop_details AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'shop_details.loaded'\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), checkout AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'checkout.loaded'\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), orders AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction IN ('transaction')\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), rates AS (\n",
    "        SELECT cu.country_iso\n",
    "            ,cu.currency_code\n",
    "            ,tmp.fx_rate_eur\n",
    "        FROM `fulfillment-dwh-production.cl.countries` cu\n",
    "        JOIN ( \n",
    "            WITH latest_fx_rate AS (\n",
    "            SELECT \n",
    "                currency_code,\n",
    "                fx_rate_eur,\n",
    "                calculated_at,\n",
    "                ROW_NUMBER() OVER (PARTITION BY currency_code ORDER BY calculated_at DESC) AS rn\n",
    "            FROM `fulfillment-dwh-production.curated_data_shared_coredata.fx_rates`\n",
    "            )\n",
    "            SELECT \n",
    "                currency_code,\n",
    "                fx_rate_eur,\n",
    "                calculated_at AS max_calculated_at\n",
    "            FROM latest_fx_rate\n",
    "            WHERE rn = 1\n",
    "        ) tmp ON tmp.currency_code = cu.currency_code\n",
    "        GROUP BY 1, 2, 3\n",
    "        ORDER BY 1\n",
    "    ), impression as (\n",
    "    SELECT       l.global_entity_id\n",
    "                ,l.country\n",
    "                ,l.userId\n",
    "                ,ROUND((l.df_impressions / r.fx_rate_eur) * 5) / 5 AS delivery_fee_listing\n",
    "                ,ROUND((sd.df_impressions / r.fx_rate_eur) * 5) / 5 AS delivery_fee_details\n",
    "                ,ROUND((co.df_impressions / r.fx_rate_eur) * 5) / 5 AS delivery_fee_checkout\n",
    "                ,COUNT(l.shopId) AS total_vendor\n",
    "                ,SUM(CASE WHEN sd.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_details_page\n",
    "                ,SUM(CASE WHEN co.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_checkout\n",
    "                ,SUM(CASE WHEN o.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_order\n",
    "                ,SUM(CASE WHEN sd.shopId IS NOT NULL THEN 1 ELSE 0 END) / COUNT(l.shopId) conversion_details\n",
    "                ,SUM(CASE WHEN co.shopId IS NOT NULL THEN 1 ELSE 0 END) / COUNT(l.shopId) conversion_checkout\n",
    "                ,SUM(CASE WHEN o.shopId IS NOT NULL THEN 1 ELSE 0 END) / COUNT(l.shopId) conversion_order\n",
    "    FROM listing l\n",
    "    LEFT JOIN shop_details sd ON sd.global_entity_id = l.global_entity_id AND sd.session_key = l.session_key AND sd.shopId = l.shopId\n",
    "    LEFT JOIN checkout co ON co.global_entity_id = l.global_entity_id AND co.session_key = l.session_key AND co.shopId = l.shopId\n",
    "    LEFT JOIN orders o ON o.global_entity_id = l.global_entity_id AND o.session_key = l.session_key AND o.shopId = l.shopId\n",
    "    LEFT JOIN rates r ON l.country = r.country_iso\n",
    "    GROUP BY 1, 2, 3, 4, 5, 6\n",
    "    )\n",
    "    select * \n",
    "    from (\n",
    "    select  global_entity_id\n",
    "        ,delivery_fee_listing\n",
    "        ,delivery_fee_details\n",
    "        ,delivery_fee_checkout\n",
    "        ,avg(conversion_details) avg_conversion_details\n",
    "        ,avg(conversion_checkout) avg_conversion_checkout\n",
    "        ,avg(conversion_order) avg_conversion_order\n",
    "        ,sum(total_vendor) vendors\n",
    "        ,count(distinct userId) total_users\n",
    "        ,count(distinct case when converted_details_page > 0 then userId end) converted_user_count_details\n",
    "        ,count(distinct case when converted_checkout > 0 then userId end) converted_user_count_checkout\n",
    "        ,count(distinct case when converted_order > 0 then userId end) converted_user_count_order\n",
    "    FROM impression\n",
    "    group by 1,2,3,4\n",
    "    order by 1,2\n",
    "    )\n",
    "    \n",
    "    where vendors > 100\n",
    "    \"\"\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "# def user_conversion_funnel(df):\n",
    "    \n",
    "#     for i in df['global_entity_id'].unique():\n",
    "\n",
    "#         df_tmp = df[df['global_entity_id'] == i]\n",
    "\n",
    "#         # Groupby and aggregate data for conversion counts\n",
    "#         user_conversion = df_tmp.groupby(['global_entity_id']).agg(\n",
    "#             converted_user_count_order=('userId', lambda x: x[df_tmp['converted_order'] > 0].nunique()),\n",
    "#             converted_user_count_details=('userId', lambda x: x[df_tmp['converted_details_page'] > 0].nunique()),\n",
    "#             converted_user_count_checkout=('userId', lambda x: x[df_tmp['converted_checkout'] > 0].nunique()),\n",
    "#             total_user_count=('userId', 'nunique')\n",
    "#         ).reset_index()\n",
    "\n",
    "#         # Calculate funnel metrics\n",
    "#         user_conversion['user_listing'] = user_conversion['total_user_count'] / user_conversion['total_user_count']\n",
    "#         user_conversion['user_listing_to_details'] = user_conversion['converted_user_count_details'] / user_conversion['total_user_count']\n",
    "#         user_conversion['user_listing_to_checkout'] = user_conversion['converted_user_count_checkout'] / user_conversion['total_user_count']\n",
    "#         user_conversion['user_listing_to_transaction'] = user_conversion['converted_user_count_order'] / user_conversion['total_user_count']\n",
    "\n",
    "    \n",
    "#         # List of conversion stages\n",
    "#         lst = [\n",
    "#         round(user_conversion['user_listing'][0] * 100, 0),  # Convert to percentage and round\n",
    "#         round(user_conversion['user_listing_to_details'][0] * 100, 0),\n",
    "#         round(user_conversion['user_listing_to_checkout'][0] * 100, 0),\n",
    "#         round(user_conversion['user_listing_to_transaction'][0] * 100, 0)\n",
    "#         ]\n",
    "\n",
    "#         # Data for funnel plot\n",
    "#         funnel_data = dict(\n",
    "#             number=lst,\n",
    "#             stage=[\"Listing\", \"Listing_to_details\", \"Listing_to_checkout\", \"Listing_to_transaction\"]\n",
    "#         )\n",
    "\n",
    "#         # Create funnel plot using plotly\n",
    "#         fig = px.funnel(funnel_data, x='number', y='stage')\n",
    "#         fig.update_layout(title='User conversion ' + i, width=750,height=400)\n",
    "#         fig.show()\n",
    "\n",
    "def user_conversion_funnel(df):\n",
    "    \n",
    "    # Create a list to store data for all entities\n",
    "    funnel_data_list = []\n",
    "    \n",
    "    for i in df['global_entity_id'].unique():\n",
    "\n",
    "        df_tmp = df[df['global_entity_id'] == i]\n",
    "\n",
    "        # Groupby and aggregate data for conversion counts\n",
    "        user_conversion = df_tmp.groupby(['global_entity_id']).agg(\n",
    "            converted_user_count_order=('userId', lambda x: x[df_tmp['converted_order'] > 0].nunique()),\n",
    "            converted_user_count_details=('userId', lambda x: x[df_tmp['converted_details_page'] > 0].nunique()),\n",
    "            converted_user_count_checkout=('userId', lambda x: x[df_tmp['converted_checkout'] > 0].nunique()),\n",
    "            total_user_count=('userId', 'nunique')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate funnel metrics\n",
    "        user_conversion['user_listing'] = user_conversion['total_user_count'] / user_conversion['total_user_count']\n",
    "        user_conversion['user_listing_to_details'] = user_conversion['converted_user_count_details'] / user_conversion['total_user_count']\n",
    "        user_conversion['user_listing_to_checkout'] = user_conversion['converted_user_count_checkout'] / user_conversion['total_user_count']\n",
    "        user_conversion['user_listing_to_transaction'] = user_conversion['converted_user_count_order'] / user_conversion['total_user_count']\n",
    "\n",
    "        # Append the conversion stages for the current entity to the list\n",
    "        funnel_data_list.append({\n",
    "            'global_entity_id': i,\n",
    "            'stage': 'Listing',\n",
    "            'conversion_rate': round(user_conversion['user_listing'][0] * 100, 0)\n",
    "        })\n",
    "        funnel_data_list.append({\n",
    "            'global_entity_id': i,\n",
    "            'stage': 'Listing_to_details',\n",
    "            'conversion_rate': round(user_conversion['user_listing_to_details'][0] * 100, 0)\n",
    "        })\n",
    "        funnel_data_list.append({\n",
    "            'global_entity_id': i,\n",
    "            'stage': 'Listing_to_checkout',\n",
    "            'conversion_rate': round(user_conversion['user_listing_to_checkout'][0] * 100, 0)\n",
    "        })\n",
    "        funnel_data_list.append({\n",
    "            'global_entity_id': i,\n",
    "            'stage': 'Listing_to_transaction',\n",
    "            'conversion_rate': round(user_conversion['user_listing_to_transaction'][0] * 100, 0)\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the list\n",
    "    funnel_data_df = pd.DataFrame(funnel_data_list)\n",
    "\n",
    "    # Create a funnel plot for all entities\n",
    "    fig = px.funnel(funnel_data_df, x='conversion_rate', y='stage', color='global_entity_id')\n",
    "    fig.update_layout(title='User Conversion Funnel Across Entities', width=800, height=600)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_delivery_fee_boxplot(df, column_list):\n",
    "    \n",
    "    for i in df['global_entity_id'].unique():\n",
    "        \n",
    "        df_tmp = df[df['global_entity_id'] == i]\n",
    "        \n",
    "        if not all(col in df_tmp.columns for col in column_list):\n",
    "            raise ValueError(f\"Some columns in {column_list} do not exist in the DataFrame for entity {i}\")\n",
    "        \n",
    "        stage_mapping = {col: col.split('_')[2].capitalize() if len(col.split('_')) > 2 else col for col in column_list}\n",
    "        \n",
    "        delivery_fee_mean = df_tmp[column_list]\n",
    "        \n",
    "        df_melted = delivery_fee_mean.melt(var_name='Stage', value_name='Delivery Fee (EUR)')\n",
    "        \n",
    "        df_melted['Stage'] = df_melted['Stage'].replace(stage_mapping)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=df_melted, x='Stage', y='Delivery Fee (EUR)')\n",
    "        \n",
    "        plt.title(f'{i} Delivery Fees Across Different Stages', fontsize=16)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "def descriptive_stats(df):\n",
    "    # Group the data by 'global_entity_id'\n",
    "    grouped = df.groupby('global_entity_id') \n",
    "\n",
    "    # Create an empty dictionary to store the descriptive statistics\n",
    "    grouped_descriptive_stats = {}\n",
    "\n",
    "    # Define the manual order for sorting\n",
    "    manual_order = [\n",
    "        'delivery_fee_listing_eur_mean', 'delivery_fee_details_eur_mean', 'delivery_fee_checkout_eur_mean', 'delivery_fee_order_eur_mean',\n",
    "        'delivery_fee_listing_eur_median', 'delivery_fee_details_eur_median', 'delivery_fee_checkout_eur_median', 'delivery_fee_order_eur_median',\n",
    "        'delivery_fee_listing_eur_min', 'delivery_fee_details_eur_min', 'delivery_fee_checkout_eur_min', 'delivery_fee_order_eur_min',\n",
    "        'delivery_fee_listing_eur_max', 'delivery_fee_details_eur_max', 'delivery_fee_checkout_eur_max', 'delivery_fee_order_eur_ma',\n",
    "        'delivery_fee_listing_eur_stddev', 'delivery_fee_details_eur_stddev', 'delivery_fee_checkout_eur_stddev', 'delivery_fee_order_eur_stddev',\n",
    "        'total_vendor', 'converted_details_page', 'converted_checkout', 'converted_order'\n",
    "    ]\n",
    "\n",
    "    # Loop through each group and calculate descriptive statistics\n",
    "    for name, group in grouped:\n",
    "        descriptive_stats = group.describe().transpose()\n",
    "\n",
    "        # Reorder the DataFrame based on the manual order\n",
    "        descriptive_stats = descriptive_stats.reindex(manual_order)\n",
    "\n",
    "        # Style the descriptive statistics for better readability\n",
    "        styled = descriptive_stats.style \\\n",
    "                        .format(precision=2, thousands=\",\", decimal=\".\") \\\n",
    "                        .format_index(str.upper, axis=1)\n",
    "        \n",
    "        grouped_descriptive_stats[name] = styled\n",
    "\n",
    "    # Display each sorted group's styled descriptive statistics\n",
    "    for name, styled_df in grouped_descriptive_stats.items():\n",
    "        print(f\"Group: {name}\")\n",
    "        display(styled_df)\n",
    "\n",
    "\n",
    "def plot_entity_kde(df, delivery_fee_column, threshold=10, num_cols=4):\n",
    "    \"\"\"\n",
    "    Plots a grid of KDE plots for each unique entity in the dataset filtered by a delivery fee threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    user_summary_df (pd.DataFrame): The DataFrame containing the data.\n",
    "    delivery_fee_column (str): The column name for delivery fee to plot the KDE.\n",
    "    threshold (float, optional): The threshold for filtering the delivery fees. Default is 10.\n",
    "    num_cols (int, optional): Number of columns in the grid. Default is 4.\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays the KDE plots for each entity.\n",
    "    \"\"\"\n",
    "    # Filter the dataset where the delivery fee is less than the specified threshold\n",
    "    filtered_df = df[df[delivery_fee_column] < threshold]\n",
    "\n",
    "    # Get the unique entity IDs\n",
    "    entities = filtered_df['global_entity_id'].unique()\n",
    "\n",
    "    # Define the number of rows for the grid based on the number of entities and columns\n",
    "    num_entities = len(entities)\n",
    "    num_rows = math.ceil(num_entities / num_cols)\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate over each entity and plot a KDE for its values\n",
    "    for i, entity in enumerate(entities):\n",
    "        subset = filtered_df[filtered_df['global_entity_id'] == entity]  # Filter data for each entity\n",
    "        \n",
    "        # Plot KDE for the entity on the corresponding subplot\n",
    "        sns.kdeplot(subset[delivery_fee_column], label=f'Entity {entity}', fill=True, ax=axes[i])\n",
    "        \n",
    "        # Set title and legend\n",
    "        axes[i].set_title(f'Entity {entity}')\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Remove any unused subplots if the number of entities is less than the grid size\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_conversion(df):\n",
    "        \n",
    "    for i in df['global_entity_id'].unique():\n",
    "\n",
    "            df_tmp = df[df['global_entity_id'] == i]\n",
    "    \n",
    "            # Create a figure with four subplots (2x2 layout)\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "            # Plot user_conversion_details as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='user_conversion_details', ax=axes[0, 0], label='Details Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='user_conversion_details', scatter=False, ax=axes[0, 0], color='blue')\n",
    "            axes[0, 0].set_title('Details Conversion vs. Delivery Fee')\n",
    "            axes[0, 0].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 0].set_ylabel('Details Conversion Rate')\n",
    "\n",
    "            # Plot user_conversion_checkout as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='user_conversion_checkout', ax=axes[0, 1], label='Checkout Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='user_conversion_checkout', scatter=False, ax=axes[0, 1], color='blue')\n",
    "            axes[0, 1].set_title('Checkout Conversion vs. Delivery Fee')\n",
    "            axes[0, 1].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 1].set_ylabel('Checkout Conversion Rate')\n",
    "\n",
    "            # Plot user_conversion_order as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='user_conversion_order', ax=axes[0, 2], label='Order Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='user_conversion_order', scatter=False, ax=axes[0, 2], color='blue')\n",
    "            axes[0, 2].set_title('Order Conversion vs. Delivery Fee')\n",
    "            axes[0, 2].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 2].set_ylabel('Order Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_details as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='avg_conversion_details', ax=axes[1, 0], label='Avg Details Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='avg_conversion_details', scatter=False, ax=axes[1, 0], color='orange')\n",
    "            axes[1, 0].set_title('Avg Details Conversion vs. Delivery Fee')\n",
    "            axes[1, 0].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 0].set_ylabel('Avg Details Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_checkout as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='avg_conversion_checkout', ax=axes[1, 1], label='Avg Checkout Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='avg_conversion_checkout', scatter=False, ax=axes[1, 1], color='orange')\n",
    "            axes[1, 1].set_title('Avg Checkout Conversion vs. Delivery Fee')\n",
    "            axes[1, 1].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 1].set_ylabel('Avg Checkout Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_order as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='avg_conversion_order', ax=axes[1, 2], label='Avg Order Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='avg_conversion_order', scatter=False, ax=axes[1, 2], color='orange')\n",
    "            axes[1, 2].set_title('Avg Order Conversion vs. Delivery Fee')\n",
    "            axes[1, 2].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 2].set_ylabel('Avg Order Conversion Rate')\n",
    "\n",
    "            # Adjust layout and show plot\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def plot_ch(df, column_list, num_cols=4):\n",
    "    \"\"\"\n",
    "    Plots a grid of boxplots for delivery fees across different stages for each entity,\n",
    "    keeping only data below the 95th percentile for each column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    column_list (list): The list of columns representing delivery fees at different stages.\n",
    "    num_cols (int, optional): Number of columns in the grid. Default is 4.\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays the boxplots for each entity.\n",
    "    \"\"\"\n",
    "    # Get unique entity IDs\n",
    "    entities = df['global_entity_id'].unique()\n",
    "\n",
    "    # Define the number of rows for the grid based on the number of entities and columns\n",
    "    num_entities = len(entities)\n",
    "    num_rows = math.ceil(num_entities / num_cols)\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, entity in enumerate(entities):\n",
    "        df_tmp = df[df['global_entity_id'] == entity]\n",
    "        \n",
    "        # Check if all columns exist\n",
    "        if not all(col in df_tmp.columns for col in column_list):\n",
    "            raise ValueError(f\"Some columns in {column_list} do not exist in the DataFrame for entity {entity}\")\n",
    "\n",
    "        # Filter values below the 95th percentile for each column\n",
    "        df_filtered = df_tmp[column_list].apply(lambda x: x[x < x.quantile(0.95)])\n",
    "\n",
    "        # Create a mapping for the stages\n",
    "        stage_mapping = {col: col.split('_')[2].capitalize() if len(col.split('_')) > 2 else col for col in column_list}\n",
    "        \n",
    "        # Melt the filtered data for plotting\n",
    "        df_melted = df_filtered.melt(var_name='Stage', value_name='Delivery Fee (EUR)')\n",
    "        \n",
    "        # Replace the stage names with the mapped values\n",
    "        df_melted['Stage'] = df_melted['Stage'].replace(stage_mapping)\n",
    "\n",
    "        # Plot the boxplot for the entity on the corresponding subplot\n",
    "        sns.boxplot(data=df_melted, x='Stage', y='Delivery Fee (EUR)', ax=axes[i])\n",
    "        \n",
    "        # Set title\n",
    "        axes[i].set_title(f'Entity {entity}')\n",
    "    \n",
    "    # Remove any unused subplots if the number of entities is less than the grid size\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_conversion(df):\n",
    "        \n",
    "    for i in df['global_entity_id'].unique():\n",
    "\n",
    "            df_tmp = df[df['global_entity_id'] == i]\n",
    "\n",
    "            # Create a figure with four subplots (2x2 layout)\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "            # Plot user_conversion_details as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_details', ax=axes[0, 0], label='Details Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_details', scatter=False, ax=axes[0, 0], color='blue')\n",
    "            axes[0, 0].set_title('Details Conversion vs. Delivery Fee ' + i )\n",
    "            axes[0, 0].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 0].set_ylabel('Details Conversion Rate')\n",
    "\n",
    "            # Plot user_conversion_checkout as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_checkout', ax=axes[0, 1], label='Checkout Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_checkout', scatter=False, ax=axes[0, 1], color='blue')\n",
    "            axes[0, 1].set_title('Checkout Conversion vs. Delivery Fee ' + i )\n",
    "            axes[0, 1].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 1].set_ylabel('Checkout Conversion Rate')\n",
    "\n",
    "            # Plot user_conversion_order as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_order', ax=axes[0, 2], label='Order Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_order', scatter=False, ax=axes[0, 2], color='blue')\n",
    "            axes[0, 2].set_title('Order Conversion vs. Delivery Fee ' + i )\n",
    "            axes[0, 2].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 2].set_ylabel('Order Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_details as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_details', ax=axes[1, 0], label='Avg Details Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_details', scatter=False, ax=axes[1, 0], color='orange')\n",
    "            axes[1, 0].set_title('Avg Details Conversion vs. Delivery Fee ' + i )\n",
    "            axes[1, 0].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 0].set_ylabel('Avg Details Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_checkout as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_checkout', ax=axes[1, 1], label='Avg Checkout Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_checkout', scatter=False, ax=axes[1, 1], color='orange')\n",
    "            axes[1, 1].set_title('Avg Checkout Conversion vs. Delivery Fee ' + i )\n",
    "            axes[1, 1].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 1].set_ylabel('Avg Checkout Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_order as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_order', ax=axes[1, 2], label='Avg Order Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_order', scatter=False, ax=axes[1, 2], color='orange')\n",
    "            axes[1, 2].set_title('Avg Order Conversion vs. Delivery Fee ' + i )\n",
    "            axes[1, 2].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 2].set_ylabel('Avg Order Conversion Rate')\n",
    "\n",
    "            # Adjust layout and show plot\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "start_date = '2025-02-15'\n",
    "end_date = '2025-02-28'\n",
    "\n",
    "#entity_id = 'DJ_CZ','FO_NO','MJM_AT','NP_HU','OP_SE','PO_FI','YS_TR','EF_GR','FY_CY','FP_BD','FP_HK','FP_KH','FP_LA','FP_MM','FP_MY','FP_PH','FP_PK','FP_SG','FP_TH','FP_TW','HS_SA','AP_PA','PY_AR','PY_BO','PY_CL','PY_CR','PY_DO','PY_EC','PY_GT','PY_HN','PY_NI','PY_PE','PY_PY','PY_SV','PY_UY','PY_VE','HF_EG','TB_AE','TB_BH','TB_IQ','TB_JO','TB_KW','TB_OM','TB_QA'\n",
    "\n",
    "entity_id = \"FP_DE\",\"FP_PK\",\"FP_KH\",\"FP_HK\",\"HS_SA\",\"AP_PA\", \"PY_AR\", \"PY_BO\", \"PY_CL\", \"PY_CR\", \"PY_DO\", \"PY_EC\", \"PY_GT\", \"PY_HN\", \"PY_NI\", \"PY_PE\", \"PY_PY\", \"PY_SV\", \"PY_UY\", \"PY_VE\", \"EF_GR\", \"FY_CY\", \"GV_MD\", \"GV_RO\", \"GV_BG\", \"GV_RS\", \"GV_HR\", \"GV_KE\", \"GV_NG\", \"GV_BA\", \"GV_CI\", \"GV_ME\", \"GV_UG\", \"YS_TR\", \"DJ_CZ\", \"FO_NO\", \"MJM_AT\", \"NP_HU\", \"OP_SE\", \"PO_FI\", \"FP_BD\", \"FP_LA\", \"FP_MM\", \"FP_MY\", \"FP_PH\", \"FP_SG\", \"FP_TH\", \"FP_TW\", \"HF_EG\", \"TB_AE\", \"TB_BH\", \"TB_IQ\", \"TB_JO\", \"TB_KW\", \"TB_OM\", \"TB_QA\"\n",
    "\n",
    "#entity_id = ('TB_OM','DJ_CZ')\n",
    "entity_id_str = \",\".join([f\"'{entity}'\" for entity in entity_id])\n",
    "\n",
    "# PROJECT_ID = \"logistics-customer-staging\"\n",
    "# bpd.options.bigquery.project = PROJECT_ID\n",
    "\n",
    "# define a few things (project id, start date etc.)\n",
    "project_id = \"logistics-customer-staging\"\n",
    "client = bigquery.Client(project = project_id)\n",
    "\n",
    "user_conversion_df = client.query(user_conversion(start_date, end_date,entity_id_str)).to_dataframe()\n",
    "#user_summary_df = client.query(user_details(start_date, end_date,entity_id_str)).to_dataframe()\n",
    "\n",
    "# user_conversion_df = bpd.read_gbq(user_conversion(start_date, end_date,entity_id_str))\n",
    "# user_summary_df = bpd.read_gbq(user_details(start_date, end_date,entity_id_str))\n",
    "\n",
    "user_conversion_df['user_conversion_details'] = user_conversion_df['converted_user_count_details'] / user_conversion_df['total_users']\n",
    "user_conversion_df['user_conversion_checkout'] = user_conversion_df['converted_user_count_checkout'] / user_conversion_df['total_users']\n",
    "user_conversion_df['user_conversion_order'] = user_conversion_df['converted_user_count_order'] / user_conversion_df['total_users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   global_entity_id  avg_conversion_details  avg_conversion_checkout  \\\n",
      "0             DJ_CZ               -0.164986                -0.067360   \n",
      "1             EF_GR               -0.357137                 0.000000   \n",
      "2             FO_NO               -0.089611                -0.109567   \n",
      "3             FP_BD               -0.412253                -0.315583   \n",
      "4             FP_DE               -0.200806                -0.569124   \n",
      "5             FP_HK               -0.098844                -0.043877   \n",
      "6             FP_KH               -0.244750                -0.296380   \n",
      "7             FP_LA               -0.242189                -0.207426   \n",
      "8             FP_MM               -0.253920                 0.050165   \n",
      "9             FP_MY               -0.156446                -0.185381   \n",
      "10            FP_PH               -0.195444                -0.238479   \n",
      "11            FP_PK               -0.271608                -0.266019   \n",
      "12            FP_SG               -0.197738                -0.186600   \n",
      "13            FP_TH               -0.275609                -0.134112   \n",
      "14            FP_TW               -0.039246                -0.027504   \n",
      "15            HF_EG               -0.162212                -0.058679   \n",
      "16            HS_SA               -0.058979                -0.037084   \n",
      "17           MJM_AT               -0.056468                 0.088513   \n",
      "18            NP_HU               -0.202244                -0.146687   \n",
      "19            OP_SE               -0.057333                -0.027713   \n",
      "20            PO_FI               -0.130184                -0.040838   \n",
      "21            TB_AE               -0.164952                -0.091075   \n",
      "22            TB_BH               -0.172292                -0.227331   \n",
      "23            TB_IQ               -0.056311                -0.263095   \n",
      "24            TB_JO               -0.141907                -0.167105   \n",
      "25            TB_KW               -0.122124                -0.101291   \n",
      "26            TB_OM               -0.133112                -0.172848   \n",
      "27            TB_QA               -0.247302                -0.028401   \n",
      "28            YS_TR               -0.141401                -0.282735   \n",
      "\n",
      "    avg_conversion_order  \n",
      "0              -0.040942  \n",
      "1                    NaN  \n",
      "2              -0.032204  \n",
      "3              -0.006774  \n",
      "4               0.000000  \n",
      "5              -0.044798  \n",
      "6              -0.016292  \n",
      "7               0.047090  \n",
      "8               0.207312  \n",
      "9              -0.049438  \n",
      "10             -0.061167  \n",
      "11              0.184860  \n",
      "12             -0.014599  \n",
      "13             -0.053354  \n",
      "14             -0.008968  \n",
      "15             -0.007705  \n",
      "16             -0.172868  \n",
      "17             -0.032019  \n",
      "18             -0.043774  \n",
      "19             -0.037800  \n",
      "20             -0.048225  \n",
      "21             -0.016381  \n",
      "22              0.036114  \n",
      "23             -0.037684  \n",
      "24             -0.016242  \n",
      "25             -0.012172  \n",
      "26              0.010131  \n",
      "27             -0.028562  \n",
      "28             -0.001001  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22421/2841536060.py:73: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  elasticity_df = pd.concat([elasticity_df, pd.DataFrame([elasticity_row])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assuming user_conversion_df is already defined in your environment\n",
    "# and contains the columns: 'global_entity_id', \n",
    "# 'delivery_fee_details', 'delivery_fee_checkout', 'delivery_fee_order',\n",
    "# 'avg_conversion_details', 'avg_conversion_checkout', 'avg_conversion_order'\n",
    "\n",
    "# Initialize a DataFrame to store the elasticity coefficients for each y variable\n",
    "elasticity_df = pd.DataFrame(columns=[\n",
    "    'global_entity_id', \n",
    "    'avg_conversion_details', \n",
    "    'avg_conversion_checkout', \n",
    "    'avg_conversion_order'\n",
    "])\n",
    "\n",
    "# List of dependent variables (y variables)\n",
    "dependent_vars = ['avg_conversion_details', 'avg_conversion_checkout', 'avg_conversion_order']\n",
    "\n",
    "# Mapping from dependent variable to its corresponding stage-specific delivery fee column\n",
    "fee_column_mapping = {\n",
    "    'avg_conversion_details': 'delivery_fee_listing',\n",
    "    'avg_conversion_checkout': 'delivery_fee_details',\n",
    "    'avg_conversion_order': 'delivery_fee_checkout'\n",
    "}\n",
    "\n",
    "# Loop through each unique global_entity_id\n",
    "for i in user_conversion_df['global_entity_id'].unique():\n",
    "    \n",
    "    # Filter data for the current entity and make a copy to avoid warnings\n",
    "    user_df_clean = user_conversion_df[user_conversion_df['global_entity_id'] == i].copy()\n",
    "    \n",
    "    # Initialize a dictionary to store the entity ID and elasticity coefficients\n",
    "    elasticity_row = {'global_entity_id': i}\n",
    "    \n",
    "    # Loop through each dependent variable\n",
    "    for dep_var in dependent_vars:\n",
    "        # Determine the corresponding fee column for the current stage\n",
    "        fee_col = fee_column_mapping[dep_var]\n",
    "        \n",
    "        # Compute the stage-specific log delivery fee (avoiding log(0) by adding 1)\n",
    "        user_df_clean['log_delivery_fee'] = np.log(user_df_clean[fee_col] + 1)\n",
    "        \n",
    "        # Apply log transformation to the dependent variable (avoid log(0) by adding 1)\n",
    "        user_df_clean[dep_var] = np.log(user_df_clean[dep_var] + 1)\n",
    "        \n",
    "        # Add a constant column (intercept) for the regression\n",
    "        user_df_clean['constant'] = 1.0\n",
    "\n",
    "        # Ensure that the relevant columns are numeric\n",
    "        user_df_clean[['log_delivery_fee', dep_var]] = user_df_clean[['log_delivery_fee', dep_var]].astype(float)\n",
    "        \n",
    "        # Replace infinite values with NaN and drop rows with NaNs in the key columns\n",
    "        cleaned_data = user_df_clean.replace([np.inf, -np.inf], np.nan).dropna(subset=['log_delivery_fee', dep_var])\n",
    "        \n",
    "        # Check if there is enough data to run the regression\n",
    "        if cleaned_data.shape[0] == 0:\n",
    "            elasticity_row[dep_var] = np.nan\n",
    "            continue\n",
    "        \n",
    "        # Set up the regression model with the constant and log_delivery_fee as predictors\n",
    "        X_log = cleaned_data[['constant', 'log_delivery_fee']]\n",
    "        y = cleaned_data[dep_var]\n",
    "        \n",
    "        # Fit the log-linear regression model\n",
    "        model_log = sm.OLS(y, X_log).fit()\n",
    "        \n",
    "        # Store the elasticity coefficient (slope corresponding to log_delivery_fee)\n",
    "        elasticity_row[dep_var] = model_log.params['log_delivery_fee']\n",
    "    \n",
    "    # Add the current entity's results to the elasticity DataFrame\n",
    "    elasticity_df = pd.concat([elasticity_df, pd.DataFrame([elasticity_row])], ignore_index=True)\n",
    "\n",
    "# Display the resulting DataFrame with elasticity coefficients\n",
    "print(elasticity_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_entity_id</th>\n",
       "      <th>avg_conversion_details</th>\n",
       "      <th>avg_conversion_checkout</th>\n",
       "      <th>avg_conversion_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DJ_CZ</td>\n",
       "      <td>-0.164986</td>\n",
       "      <td>-0.067360</td>\n",
       "      <td>-0.040942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EF_GR</td>\n",
       "      <td>-0.357137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FO_NO</td>\n",
       "      <td>-0.089611</td>\n",
       "      <td>-0.109567</td>\n",
       "      <td>-0.032204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP_BD</td>\n",
       "      <td>-0.412253</td>\n",
       "      <td>-0.315583</td>\n",
       "      <td>-0.006774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FP_DE</td>\n",
       "      <td>-0.200806</td>\n",
       "      <td>-0.569124</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FP_HK</td>\n",
       "      <td>-0.098844</td>\n",
       "      <td>-0.043877</td>\n",
       "      <td>-0.044798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FP_KH</td>\n",
       "      <td>-0.244750</td>\n",
       "      <td>-0.296380</td>\n",
       "      <td>-0.016292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FP_LA</td>\n",
       "      <td>-0.242189</td>\n",
       "      <td>-0.207426</td>\n",
       "      <td>0.047090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FP_MM</td>\n",
       "      <td>-0.253920</td>\n",
       "      <td>0.050165</td>\n",
       "      <td>0.207312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FP_MY</td>\n",
       "      <td>-0.156446</td>\n",
       "      <td>-0.185381</td>\n",
       "      <td>-0.049438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FP_PH</td>\n",
       "      <td>-0.195444</td>\n",
       "      <td>-0.238479</td>\n",
       "      <td>-0.061167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FP_PK</td>\n",
       "      <td>-0.271608</td>\n",
       "      <td>-0.266019</td>\n",
       "      <td>0.184860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FP_SG</td>\n",
       "      <td>-0.197738</td>\n",
       "      <td>-0.186600</td>\n",
       "      <td>-0.014599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FP_TH</td>\n",
       "      <td>-0.275609</td>\n",
       "      <td>-0.134112</td>\n",
       "      <td>-0.053354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FP_TW</td>\n",
       "      <td>-0.039246</td>\n",
       "      <td>-0.027504</td>\n",
       "      <td>-0.008968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HF_EG</td>\n",
       "      <td>-0.162212</td>\n",
       "      <td>-0.058679</td>\n",
       "      <td>-0.007705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HS_SA</td>\n",
       "      <td>-0.058979</td>\n",
       "      <td>-0.037084</td>\n",
       "      <td>-0.172868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MJM_AT</td>\n",
       "      <td>-0.056468</td>\n",
       "      <td>0.088513</td>\n",
       "      <td>-0.032019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NP_HU</td>\n",
       "      <td>-0.202244</td>\n",
       "      <td>-0.146687</td>\n",
       "      <td>-0.043774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OP_SE</td>\n",
       "      <td>-0.057333</td>\n",
       "      <td>-0.027713</td>\n",
       "      <td>-0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>PO_FI</td>\n",
       "      <td>-0.130184</td>\n",
       "      <td>-0.040838</td>\n",
       "      <td>-0.048225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TB_AE</td>\n",
       "      <td>-0.164952</td>\n",
       "      <td>-0.091075</td>\n",
       "      <td>-0.016381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TB_BH</td>\n",
       "      <td>-0.172292</td>\n",
       "      <td>-0.227331</td>\n",
       "      <td>0.036114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TB_IQ</td>\n",
       "      <td>-0.056311</td>\n",
       "      <td>-0.263095</td>\n",
       "      <td>-0.037684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>TB_JO</td>\n",
       "      <td>-0.141907</td>\n",
       "      <td>-0.167105</td>\n",
       "      <td>-0.016242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TB_KW</td>\n",
       "      <td>-0.122124</td>\n",
       "      <td>-0.101291</td>\n",
       "      <td>-0.012172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TB_OM</td>\n",
       "      <td>-0.133112</td>\n",
       "      <td>-0.172848</td>\n",
       "      <td>0.010131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TB_QA</td>\n",
       "      <td>-0.247302</td>\n",
       "      <td>-0.028401</td>\n",
       "      <td>-0.028562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>YS_TR</td>\n",
       "      <td>-0.141401</td>\n",
       "      <td>-0.282735</td>\n",
       "      <td>-0.001001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   global_entity_id  avg_conversion_details  avg_conversion_checkout  \\\n",
       "0             DJ_CZ               -0.164986                -0.067360   \n",
       "1             EF_GR               -0.357137                 0.000000   \n",
       "2             FO_NO               -0.089611                -0.109567   \n",
       "3             FP_BD               -0.412253                -0.315583   \n",
       "4             FP_DE               -0.200806                -0.569124   \n",
       "5             FP_HK               -0.098844                -0.043877   \n",
       "6             FP_KH               -0.244750                -0.296380   \n",
       "7             FP_LA               -0.242189                -0.207426   \n",
       "8             FP_MM               -0.253920                 0.050165   \n",
       "9             FP_MY               -0.156446                -0.185381   \n",
       "10            FP_PH               -0.195444                -0.238479   \n",
       "11            FP_PK               -0.271608                -0.266019   \n",
       "12            FP_SG               -0.197738                -0.186600   \n",
       "13            FP_TH               -0.275609                -0.134112   \n",
       "14            FP_TW               -0.039246                -0.027504   \n",
       "15            HF_EG               -0.162212                -0.058679   \n",
       "16            HS_SA               -0.058979                -0.037084   \n",
       "17           MJM_AT               -0.056468                 0.088513   \n",
       "18            NP_HU               -0.202244                -0.146687   \n",
       "19            OP_SE               -0.057333                -0.027713   \n",
       "20            PO_FI               -0.130184                -0.040838   \n",
       "21            TB_AE               -0.164952                -0.091075   \n",
       "22            TB_BH               -0.172292                -0.227331   \n",
       "23            TB_IQ               -0.056311                -0.263095   \n",
       "24            TB_JO               -0.141907                -0.167105   \n",
       "25            TB_KW               -0.122124                -0.101291   \n",
       "26            TB_OM               -0.133112                -0.172848   \n",
       "27            TB_QA               -0.247302                -0.028401   \n",
       "28            YS_TR               -0.141401                -0.282735   \n",
       "\n",
       "    avg_conversion_order  \n",
       "0              -0.040942  \n",
       "1                    NaN  \n",
       "2              -0.032204  \n",
       "3              -0.006774  \n",
       "4               0.000000  \n",
       "5              -0.044798  \n",
       "6              -0.016292  \n",
       "7               0.047090  \n",
       "8               0.207312  \n",
       "9              -0.049438  \n",
       "10             -0.061167  \n",
       "11              0.184860  \n",
       "12             -0.014599  \n",
       "13             -0.053354  \n",
       "14             -0.008968  \n",
       "15             -0.007705  \n",
       "16             -0.172868  \n",
       "17             -0.032019  \n",
       "18             -0.043774  \n",
       "19             -0.037800  \n",
       "20             -0.048225  \n",
       "21             -0.016381  \n",
       "22              0.036114  \n",
       "23             -0.037684  \n",
       "24             -0.016242  \n",
       "25             -0.012172  \n",
       "26              0.010131  \n",
       "27             -0.028562  \n",
       "28             -0.001001  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticity_df.to_csv('elasticity_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_data = \"\"\"\n",
    "\n",
    "WITH listings_agg AS (\n",
    "  SELECT\n",
    "    dh_platform,\n",
    "    global_entity_id, \n",
    "    platform,\n",
    "    session_key,\n",
    "    shopId,\n",
    "    eventTimestamp,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'promisedDeliveryTimeRangeUpper' THEN ev.value END IGNORE NULLS) AS promised_upper_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'promisedDeliveryTimeRangeLower' THEN ev.value END IGNORE NULLS) AS promised_lower_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'locationCountry' THEN ev.value END IGNORE NULLS) AS location_country,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'locationCity' THEN ev.value END IGNORE NULLS) AS location_city,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopDeliveryFee' THEN ev.value END IGNORE NULLS) AS delivery_fee_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'serviceFee' THEN ev.value END IGNORE NULLS) AS service_fee_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopMinimumOrderValue' THEN ev.value END IGNORE NULLS) AS mov_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'verticalType' THEN ev.value END IGNORE NULLS) AS verticaltype_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopSponsoring' THEN ev.value END IGNORE NULLS) AS sponsor_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopListType' THEN ev.value END IGNORE NULLS) AS shoplisttype_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopType' THEN ev.value END IGNORE NULLS) AS shoptype_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopName' THEN ev.value END IGNORE NULLS) AS shopname_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopCuisine' THEN ev.value END IGNORE NULLS) AS cuisine_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopRank' THEN ev.value END IGNORE NULLS) AS rank_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopPosition' THEN ev.value END IGNORE NULLS) AS position_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'perseusClientIdNew' THEN ev.value END IGNORE NULLS) AS id_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopCategorySelected' THEN ev.value END IGNORE NULLS) AS category_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'pageType' THEN ev.value END IGNORE NULLS) AS pageType_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'userId' THEN ev.value END IGNORE NULLS) AS user_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'eventAction' THEN ev.value END IGNORE NULLS) AS event_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'hour-of-the-day' THEN ev.value END IGNORE NULLS) AS hotd_array,\n",
    "    ARRAY_AGG(CASE WHEN ev.name = 'shopRatingQuality' THEN ev.value END IGNORE NULLS) AS shopRatingQuality_array\n",
    "  FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events` pe, \n",
    "    UNNEST(pe.eventVariables) AS ev\n",
    "  WHERE partition_date > '2025-02-02' and partition_date < '2025-02-15'\n",
    "    AND eventAction = 'shop_impressions.loaded'\n",
    "    AND pe.global_entity_id = 'EF_GR'\n",
    "  GROUP BY dh_platform,global_entity_id,platform, session_key, shopId, eventTimestamp\n",
    "), \n",
    "listings_raw as (\n",
    "SELECT \n",
    "  dh_platform,\n",
    "  global_entity_id,\n",
    "  platform,\n",
    "  session_key,\n",
    "  shopId,\n",
    "  eventTimestamp,\n",
    "  promised_value AS promisedDeliveryTimeRangeUpper,\n",
    "  IF(pos < ARRAY_LENGTH(promised_lower_array), promised_lower_array[OFFSET(pos)], NULL) AS promisedDeliveryTimeRangeLower,\n",
    "  IF(pos < ARRAY_LENGTH(location_country), location_country[OFFSET(pos)], NULL) AS locationCountry,\n",
    "  IF(pos < ARRAY_LENGTH(location_city), location_city[OFFSET(pos)], NULL) AS locationCity,\n",
    "  IF(pos < ARRAY_LENGTH(delivery_fee_array), delivery_fee_array[OFFSET(pos)], NULL) AS shopDeliveryFee,\n",
    "  IF(pos < ARRAY_LENGTH(service_fee_array), service_fee_array[OFFSET(pos)], NULL) AS serviceFee,\n",
    "  IF(pos < ARRAY_LENGTH(mov_array), mov_array[OFFSET(pos)], NULL) AS shopMinimumOrderValue,\n",
    "  IF(pos < ARRAY_LENGTH(verticaltype_array), verticaltype_array[OFFSET(pos)], NULL) AS verticalType,\n",
    "  IF(pos < ARRAY_LENGTH(shoplisttype_array), shoplisttype_array[OFFSET(pos)], NULL) AS shopListType,\n",
    "  IF(pos < ARRAY_LENGTH(sponsor_array), sponsor_array[OFFSET(pos)], NULL) AS shopSponsoring,\n",
    "  IF(pos < ARRAY_LENGTH(shoptype_array), shoptype_array[OFFSET(pos)], NULL) AS shopType,\n",
    "  IF(pos < ARRAY_LENGTH(shopname_array), shopname_array[OFFSET(pos)], NULL) AS shopName,\n",
    "  IF(pos < ARRAY_LENGTH(cuisine_array), cuisine_array[OFFSET(pos)], NULL) AS shopCuisine,\n",
    "  IF(pos < ARRAY_LENGTH(rank_array), rank_array[OFFSET(pos)], NULL) AS shopRank,\n",
    "  IF(pos < ARRAY_LENGTH(position_array), position_array[OFFSET(pos)], NULL) AS shopPosition,\n",
    "  IF(pos < ARRAY_LENGTH(id_array), id_array[OFFSET(pos)], NULL) AS perseusClientId,\n",
    "  IF(pos < ARRAY_LENGTH(category_array), category_array[OFFSET(pos)], NULL) AS shopCategorySelected,\n",
    "  IF(pos < ARRAY_LENGTH(pageType_array), pageType_array[OFFSET(pos)], NULL) AS pageType,\n",
    "  IF(pos < ARRAY_LENGTH(user_array), user_array[OFFSET(pos)], NULL) AS userId,\n",
    "  IF(pos < ARRAY_LENGTH(event_array), event_array[OFFSET(pos)], NULL) AS eventAction,\n",
    "  IF(pos < ARRAY_LENGTH(hotd_array), hotd_array[OFFSET(pos)], NULL) AS hour_of_the_day,\n",
    "  IF(pos < ARRAY_LENGTH(shopRatingQuality_array), shopRatingQuality_array[OFFSET(pos)], NULL) AS shopRatingQuality,\n",
    "  row_number() over (partition by session_key, shopId order by eventTimestamp asc) as rn\n",
    "FROM listings_agg,\n",
    "UNNEST(promised_upper_array) AS promised_value WITH OFFSET AS pos\n",
    "ORDER BY dh_platform, global_entity_id, platform, session_key, shopId,eventTimestamp, pos\n",
    "), listings_final as (\n",
    "select *\n",
    "from listings_raw\n",
    "where rn = 1\n",
    "),\n",
    "details_agg AS (\n",
    "  SELECT \n",
    "    session_key session_key_details,\n",
    "    shopId shopId_details,\n",
    "    eventTimestamp timestamp_details,\n",
    "    row_number() over (partition by session_key, shopId order by eventTimestamp) as rn_details\n",
    "  FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events` pe, \n",
    "    UNNEST(pe.eventVariables) AS ev\n",
    "  WHERE partition_date > '2025-02-02' and partition_date < '2025-02-15'\n",
    "    AND eventAction = 'shop_details.loaded'\n",
    "    AND pe.global_entity_id = 'EF_GR'\n",
    "  GROUP BY session_key, shopId, eventTimestamp\n",
    "),\n",
    "details_final as (\n",
    "select *\n",
    "from details_agg\n",
    "where rn_details = 1\n",
    ")\n",
    "select *\n",
    "from listings_final lf\n",
    "left join details_final df on df.session_key_details = lf.session_key and df.shopId_details = lf.shopId\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"logistics-customer-staging\"\n",
    "client = bigquery.Client(project = project_id)\n",
    "\n",
    "listings_df = client.query(listings_data).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume listings_df is your DataFrame.\n",
    "\n",
    "# Convert the relevant columns to numeric, coercing non-numeric values to NaN.\n",
    "listings_df['promisedDeliveryTimeRangeUpper'] = pd.to_numeric(\n",
    "    listings_df['promisedDeliveryTimeRangeUpper'], errors='coerce'\n",
    ")\n",
    "listings_df['promisedDeliveryTimeRangeLower'] = pd.to_numeric(\n",
    "    listings_df['promisedDeliveryTimeRangeLower'], errors='coerce'\n",
    ")\n",
    "listings_df['shopDeliveryFee'] = pd.to_numeric(\n",
    "    listings_df['shopDeliveryFee'], errors='coerce'\n",
    ")\n",
    "\n",
    "# Optional: Check if conversion worked as expected.\n",
    "print(listings_df[['promisedDeliveryTimeRangeUpper', 'promisedDeliveryTimeRangeLower', 'shopDeliveryFee']].dtypes)\n",
    "\n",
    "# Compute the average delivery time.\n",
    "# (Here I'm taking the midpoint; if you want the sum, remove the division by 2.)\n",
    "listings_df['average_delivery_time'] = (\n",
    "    listings_df['promisedDeliveryTimeRangeUpper'] + listings_df['promisedDeliveryTimeRangeLower']\n",
    ") / 2\n",
    "\n",
    "epsilon = 1e-6  # small constant to avoid log(0)\n",
    "listings_df['log_delivery_fee'] = np.log(listings_df['shopDeliveryFee'] + epsilon)\n",
    "listings_df['log_promised_time'] = np.log(listings_df['average_delivery_time'] + epsilon)\n",
    "\n",
    "# Add a new column 'click': 1 if session_key_details is not null, else 0\n",
    "listings_df['click'] = listings_df['session_key_details'].notnull().astype(int)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(listings_df[['promisedDeliveryTimeRangeUpper', \n",
    "                   'promisedDeliveryTimeRangeLower', \n",
    "                   'average_delivery_time', \n",
    "                   'log_delivery_fee', \n",
    "                   'log_promised_time', \n",
    "                   'click']].head())\n",
    "\n",
    "\n",
    "# # Aggregate the data at the session level\n",
    "# session_df = listings_df.groupby(['session_key','locationCity']).agg(\n",
    "#     avg_delivery_fee = ('shopDeliveryFee', 'mean'),\n",
    "#     avg_delivery_time = ('average_delivery_time', 'mean'),\n",
    "#     click_rate = ('click', 'mean'),    # proportion of shops clicked per session\n",
    "#     num_exposures = ('session_key', 'count')  # number of shops viewed in the session\n",
    "# ).reset_index()\n",
    "\n",
    "# # For log-transformation, add a small constant to avoid log(0)\n",
    "# epsilon = 1e-6\n",
    "# session_df['log_avg_delivery_fee'] = np.log(session_df['avg_delivery_fee'] + epsilon)\n",
    "# session_df['log_avg_delivery_time'] = np.log(session_df['avg_delivery_time'] + epsilon)\n",
    "\n",
    "# print(session_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Create dummy variables for locationCity, dropping the first category\n",
    "location_dummies = pd.get_dummies(session_df['locationCity'], prefix='locationCity', drop_first=True)\n",
    "\n",
    "# Combine numeric predictors with the dummy variables\n",
    "X = pd.concat([session_df[['log_avg_delivery_fee', 'log_avg_delivery_time']], location_dummies], axis=1)\n",
    "X = sm.add_constant(X)  # Adds the intercept term\n",
    "\n",
    "# Define the dependent variable\n",
    "y = session_df['click_rate']\n",
    "\n",
    "# Debug: Print data types before conversion\n",
    "print(\"Before conversion, X dtypes:\")\n",
    "print(X.dtypes)\n",
    "print(\"\\nBefore conversion, y dtype:\")\n",
    "print(y.dtype)\n",
    "\n",
    "# Convert all columns to numeric, coercing errors to NaN\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "# Debug: Check data types again\n",
    "print(\"\\nAfter pd.to_numeric, X dtypes:\")\n",
    "print(X.dtypes)\n",
    "print(\"\\nAfter pd.to_numeric, y dtype:\")\n",
    "print(y.dtype)\n",
    "\n",
    "# Force conversion to float if needed\n",
    "X = X.astype(float)\n",
    "y = y.astype(float)\n",
    "\n",
    "# Optionally, drop rows with missing values\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# Debug: Check shapes and dtypes one more time\n",
    "print(\"\\nFinal X dtypes:\")\n",
    "print(X.dtypes)\n",
    "print(\"\\nFinal X shape:\", X.shape)\n",
    "print(\"Final y shape:\", y.shape)\n",
    "\n",
    "# Fit the OLS model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the formula: add additional covariates as needed.\n",
    "formula = \"click ~ log_delivery_fee + log_promised_time\"\n",
    "\n",
    "# Fit a GEE model clustering on session_id\n",
    "gee_model = sm.GEE.from_formula(formula,\n",
    "                                groups=\"session_key\",\n",
    "                                data=listings_df,\n",
    "                                family=sm.families.Binomial())\n",
    "gee_result = gee_model.fit()\n",
    "\n",
    "print(gee_result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create design matrices for fixed effects using patsy.\n",
    "formula = \"click ~ log_delivery_fee + log_promised_time\"\n",
    "y, X = patsy.dmatrices(formula, data, return_type='dataframe')\n",
    "\n",
    "# Create a variance components design matrix for the random intercept by session.\n",
    "vc_formula = \"0 + C(session_key)\"\n",
    "vc_matrix = patsy.dmatrix(vc_formula, data, return_type='dataframe')\n",
    "vc = {\"session\": vc_matrix}\n",
    "\n",
    "# Fit the BinomialBayesMixedGLM model using variational Bayes\n",
    "model = BinomialBayesMixedGLM(endog=y, exog=X, exog_vc=vc)\n",
    "fit_result = model.fit_vb()\n",
    "\n",
    "print(fit_result.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
