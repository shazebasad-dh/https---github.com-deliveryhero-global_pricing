{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Doc\n",
        "\n",
        "## What is this for?\n",
        "\n",
        "As of 2023Q2, Hungerstation (HS) data is not joineable with our tables due to BigQuery restriction. HS is stored in Europe, while Pricing is in USA. BigQuery doesn't allow to join data that are not stored in the same place.\n",
        "\n",
        "Given this situation, we must first download the data to a local machine and then upload it back into our dataset dh-logistics-product-ops.pricing to have it available for analysis. \n",
        "\n",
        "This Notebook does that. It takes X days of HS data, starting always from the current date, makes a temporary file with it and loads it back to BigQuery but to our table in the Pricing dataset. \n",
        "\n",
        "## How to use it\n",
        "\n",
        "- the only parameter to set is __DAYS_BACK__ which is the number of lookback days we want to load data. For example, today is 5th April and days_back = 14, then we'd fetch data from 22th March until 5th April.\n",
        "\n",
        "- Once __DAYS_BACK__ is set, click Runtime -> Run all. Accept all permissions request and let it run. At the end of the notebook, there will be log messages indicating the job progress. "
      ],
      "metadata": {
        "id": "zAEBRDf7FYK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DAYS_BACK = 14"
      ],
      "metadata": {
        "id": "IngThqKCwpNW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haS5ntUeFHh-"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "eZ1N2sdKFWbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install polars -q\n",
        "!pip install tqdm -q"
      ],
      "metadata": {
        "id": "raX9ET-VFQ46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a966f2ae-2016-4f9a-bdd6-04f2c7e4907d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9HmQthsrFHh6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "from datetime import datetime, timedelta\n",
        "import pyarrow as pa\n",
        "import polars as pl\n",
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ek2DxJIFHiA"
      },
      "source": [
        "## Queries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "STAGING_QUERY = \"\"\"\n",
        "\n",
        "DECLARE start_date, end_date DATE;\n",
        "\n",
        "SET end_date = \"{0}\";\n",
        "SET start_date = \"{1}\";\n",
        "\n",
        "SELECT \n",
        "    order_id as platform_order_code\n",
        "    , operation_day\n",
        "    , order_created_at_sa\n",
        "    , branch_id\n",
        "    , branch_name_en\n",
        "    , OD_delivery_fee\n",
        "    , is_acquisition\n",
        "    , rdf_offer_applied\n",
        "    , rdf_offer_restaurant_max_charge\n",
        "    , rdf_offer_type\n",
        "    , is_subscribed\n",
        "    , is_user_subscribed\n",
        "    , delivery_fee_discount\n",
        "    , subscribed_discount_amount\n",
        "\n",
        "FROM `dhub-hungerstation.reporting_prod.orders_fact_non_pii` \n",
        "WHERE operation_day BETWEEN start_date AND end_date\n",
        "AND rdf_offer_applied = 1;\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "MERGE_QUERY = \"\"\"\n",
        "MERGE INTO `dh-logistics-product-ops.pricing.{0}` prd\n",
        "  USING  `dh-logistics-product-ops.pricing.{1}` stg\n",
        "    ON prd.platform_order_code = stg.platform_order_code\n",
        "  WHEN MATCHED THEN\n",
        "    UPDATE SET\n",
        "        platform_order_code = stg.platform_order_code\n",
        "        , operation_day = stg.operation_day\n",
        "        , order_created_at_sa = stg.order_created_at_sa\n",
        "        , branch_id = stg.branch_id\n",
        "        , branch_name_en = stg.branch_name_en\n",
        "        , OD_delivery_fee = stg.OD_delivery_fee\n",
        "        , rdf_offer_applied = stg.rdf_offer_applied\n",
        "        , rdf_offer_restaurant_max_charge = stg.rdf_offer_restaurant_max_charge\n",
        "        , rdf_offer_type = stg.rdf_offer_type\n",
        "        , is_subscribed = stg.is_subscribed\n",
        "        , is_user_subscribed = stg.is_user_subscribed\n",
        "        , delivery_fee_discount = stg.delivery_fee_discount\n",
        "        , subscribed_discount_amount = stg.subscribed_discount_amount\n",
        "  WHEN NOT MATCHED THEN\n",
        "    INSERT ROW\n",
        "  ;\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-LK5SISVV8W8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPcgkYiKFHiC"
      },
      "source": [
        "## Operators"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class HSTransferOperator:\n",
        "    \"\"\"\n",
        "    Class contains the logic to run a DAG required to make HungerStation RDF data available in\n",
        "    the pricing dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "    self,\n",
        "    env:str,\n",
        "    project_id:str,\n",
        "    dest_project_id:str,\n",
        "    dataset_id:str,\n",
        "    credentials:str = None\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.project = project_id\n",
        "        self.dest_project_id = dest_project_id\n",
        "        self.dataset_id = dataset_id\n",
        "        self.credentials = credentials\n",
        "        self.init_bq_client()\n",
        "\n",
        "    def init_bq_client(self):\n",
        "        if self.env == \"LOCAL\":\n",
        "            self.credentials = self.credentials\n",
        "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = self.credentials\n",
        "            self.client = bigquery.Client(project=self.project)\n",
        "\n",
        "        if self.env == \"COLAB\":\n",
        "            from google.colab import auth, drive\n",
        "            auth.authenticate_user()\n",
        "            print('Authenticated')\n",
        "            drive.mount('/content/gdrive')\n",
        "            self.client = bigquery.Client(project=self.project)\n",
        "            # set the working directory to the user gdrive\n",
        "            os.chdir(\"/content/gdrive/MyDrive\")\n",
        "\n",
        "    def _load_job_config(self) -> bigquery.LoadJobConfig():\n",
        "        job_config = bigquery.LoadJobConfig()\n",
        "        job_config.source_format = bigquery.SourceFormat.PARQUET\n",
        "        job_config.create_disposition = \"CREATE_IF_NEEDED\"\n",
        "        job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
        "        return job_config\n",
        "\n",
        "\n",
        "    def _get_query_as_arrow_table(self, query:str) -> pa.Table:\n",
        "        \"\"\"Loads a Bigquery table dataframe into a Arrow Table.\n",
        "\n",
        "        Args:\n",
        "            query (str): The query to run\n",
        "\n",
        "        Returns:\n",
        "            (pa.Table)\n",
        "        \"\"\"\n",
        "        return self.client.query(query).to_arrow(progress_bar_type=\"tqdm\")\n",
        "\n",
        "    def load_bigquery_into_polars(self, query:str) -> pl.DataFrame:\n",
        "        \"\"\"Loads a Bigquery table dataframe into a Polars DataFrame.\n",
        "\n",
        "        Args:\n",
        "            query (str): The query to run\n",
        "\n",
        "        Returns:\n",
        "            (pl.DataFrame)\n",
        "        \"\"\"\n",
        "        arrow_data = self._get_query_as_arrow_table(query)\n",
        "        df_polars = pl.from_arrow(arrow_data)\n",
        "        del arrow_data\n",
        "        return df_polars\n",
        "\n",
        "\n",
        "    def load_polars_to_bigquery(\n",
        "        self,\n",
        "        dataframe: pl.DataFrame,\n",
        "        job_config: bigquery.LoadJobConfig(),\n",
        "        table_name:str\n",
        "    ):\n",
        "        \"\"\"Loads a Polars dataframe to BigQuery.\n",
        "\n",
        "        Args:\n",
        "            project_id (str): The project ID for the BigQuery destination.\n",
        "            dataset_id (str): The dataset ID for the BigQuery destination.\n",
        "            table_name (str): The table name for the BigQuery destination.\n",
        "            dataframe (pl.DataFrame): The Polars dataframe to load.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        #save local parquet\n",
        "        with tempfile.NamedTemporaryFile(suffix='.parquet', delete=False) as temp_file:\n",
        "            dataframe.write_parquet(temp_file.name)\n",
        "            file_path = temp_file.name\n",
        "\n",
        "        # set table name\n",
        "        table_ref = f\"{self.dest_project_id}.{self.dataset_id}.{table_name}\"\n",
        "\n",
        "        # Load the data into BigQuery\n",
        "        with open(file_path, \"rb\") as temp_parquet:\n",
        "            job = self.client.load_table_from_file(temp_parquet, table_ref, job_config= job_config)\n",
        "            job.result()\n",
        "\n",
        "        print(f\"Loaded {len(dataframe)} rows to BigQuery table {table_name} in {self.dest_project_id}:{self.dataset_id}\")\n",
        "\n",
        "    def create_staging_data(self, query:str, table_name:str, end_date:datetime, days_back:str):\n",
        "        \"\"\"Operator to triggert the staging part of the DAG.\n",
        "        This runs a query that fetch data from HS local table, save a temporary local copy and then\n",
        "        load such copy to pricing dataset\n",
        "\n",
        "        Args:\n",
        "            query (str): query to load HS data\n",
        "            table_name (str): name of the destination table\n",
        "            end_date (datetime): run date of the task\n",
        "            days_back (str): how many days back we want to fetch data\n",
        "        \"\"\"\n",
        "        print(\"Initiating staging task...\")\n",
        "        query_with_dates = query.format(*self._return_job_dates(end_date, days_back))\n",
        "        polars_df = self.load_bigquery_into_polars(query_with_dates)\n",
        "        job_config = self._load_job_config()\n",
        "\n",
        "        print(\"Loading from BigQuery into Polars successful\")\n",
        "        self.load_polars_to_bigquery(polars_df, job_config, table_name)\n",
        "\n",
        "    def merge_into_prod(self, query:str, staging_table:str, prod_table:str):\n",
        "        \"\"\"Function that creates a BQ job that merge the staging data into production table.\n",
        "\n",
        "        Args:\n",
        "            query (str): Merge query statement\n",
        "            staging_table (str): staging pricing HS table\n",
        "            prod_table (str): production pricing HS table\n",
        "        \"\"\"\n",
        "        print(\"Initiating merging task...\")\n",
        "        job = self.client.query(\n",
        "            query.format(prod_table, staging_table)\n",
        "        )\n",
        "        job.result()\n",
        "        print(\"Merge has finished\")\n",
        "\n",
        "    def _return_job_dates(self, end_date:datetime, days_back:datetime) -> list[str]:\n",
        "        \"\"\"Return the run period  as list of string\n",
        "\n",
        "        Args:\n",
        "            end_date (datetime): run date of the task\n",
        "            days_back (datetime): how many days back we want to fetch data\n",
        "\n",
        "        Returns:\n",
        "            list[str]: list of [start_date, end_date] used to fetch data\n",
        "        \"\"\"\n",
        "        start_date = end_date - timedelta(days=days_back)\n",
        "        return [end_date.strftime(\"%Y-%m-%d\"), start_date.strftime(\"%Y-%m-%d\")]\n",
        "    \n",
        "    def run_dag(\n",
        "        self\n",
        "        , staging_query:str\n",
        "        , merge_query:str\n",
        "        , staging_table_name:str\n",
        "        , production_table_name:str\n",
        "        , end_date:datetime\n",
        "        , days_back:int\n",
        "    ):\n",
        "        \"\"\"Run whole DAG. Updates Pricing HS RDF data\n",
        "\n",
        "        Args:\n",
        "            staging_query (str): Query to create staging table\n",
        "            merge_query (str): Query to merge staging into prod table\n",
        "            staging_table_name (str): Staging table name\n",
        "            production_table_name (str): Production table name\n",
        "            end_date (datetime): run date of the task\n",
        "            days_back (int): how many days back we want to fetch data\n",
        "        \"\"\"\n",
        "        self.create_staging_data(\n",
        "            staging_query\n",
        "            , staging_table_name\n",
        "            , end_date\n",
        "            , days_back\n",
        "        )\n",
        "\n",
        "        self.merge_into_prod(\n",
        "            merge_query\n",
        "            , staging_table_name\n",
        "            , production_table_name\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "2d3d8B9DWE7g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuIJG3E0FHiI"
      },
      "source": [
        "# DAG"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init operator\n",
        "hs_transfer_operator = HSTransferOperator(\n",
        "    project_id = \"logistics-data-staging-flat\"\n",
        "    , dest_project_id = \"dh-logistics-product-ops\"\n",
        "    , dataset_id = \"pricing\"\n",
        "    , env=\"COLAB\"\n",
        ")\n",
        "\n",
        "# create staging data\n",
        "hs_transfer_operator.run_dag(\n",
        "            staging_query = STAGING_QUERY\n",
        "        , merge_query = MERGE_QUERY\n",
        "        , staging_table_name = \"hs_sa_rdf_orders_stg\"\n",
        "        , production_table_name = \"hs_sa_rdf_orders\"\n",
        "        , end_date = datetime.today()\n",
        "        , days_back = DAYS_BACK\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1bF_p1wWM1X",
        "outputId": "c0def432-3df1-4152-e149-abea1d2fa4b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n",
            "Mounted at /content/gdrive\n",
            "Initiating staging task...\n",
            "Job ID 03c07840-57a7-4e80-91a6-0344b166776e successfully executed: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
            "Loading from BigQuery into Polars successful\n",
            "Loaded 1482671 rows to BigQuery table hs_sa_rdf_orders_stg in dh-logistics-product-ops:pricing\n",
            "Initiating merging task...\n",
            "Merge has finished\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.15 ('pmsetup')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5028745414490703593d3ea58562cf54a3337578218da18d15680fd3665f302f"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZPcgkYiKFHiC"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}