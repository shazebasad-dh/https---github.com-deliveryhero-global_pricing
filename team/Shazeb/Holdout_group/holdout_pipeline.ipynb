{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "import plotly.express as px\n",
    "import db_dtypes\n",
    "import bigframes.pandas as bpd\n",
    "from IPython.display import display, HTML\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from typing import Union\n",
    "import logging\n",
    "import sys\n",
    "from google.cloud.exceptions import NotFound\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.auth import default  # <-- Make sure to import this\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "mkt_data = \"\"\"\n",
    "\n",
    "WITH holdout_entities AS (\n",
    "  SELECT\n",
    "         entity_id,\n",
    "        `Release Date` AS release_date\n",
    "  FROM `logistics-data-storage-staging.long_term_pricing.global_holdout_rollout`\n",
    "  WHERE `Release Date` < DATE_TRUNC(CURRENT_DATE(), WEEK)\n",
    "  AND entity_id in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "),\n",
    "orders as (\n",
    "  SELECT\n",
    "       dps.entity_id entity_id\n",
    "      ,dps.dps_customer_id customer_id\n",
    "      ,COUNT(case when mkt.order_date <= e.release_date then mkt.order_id end) AS orders_pre\n",
    "      ,COUNT(case when mkt.order_date > e.release_date then mkt.order_id end) AS orders_post\n",
    "      ,SUM(case when mkt.order_date <= e.release_date then mkt.analytical_profit end) AS analytical_profit_pre\n",
    "      ,SUM(case when mkt.order_date > e.release_date then mkt.analytical_profit end) AS analytical_profit_post\n",
    "  FROM `fulfillment-dwh-production.cl.dps_sessions_mapped_to_orders` dps\n",
    "  JOIN holdout_entities AS e\n",
    "    ON dps.entity_id = e.entity_id\n",
    "  left join fulfillment-dwh-production.curated_data_shared_mkt.bima_order_profitability mkt\n",
    "    ON mkt.global_entity_id = dps.entity_id\n",
    "    AND mkt.order_id = dps.platform_order_code\n",
    "    AND order_date >= DATE_SUB(release_date, INTERVAL 8 WEEK)\n",
    "    AND order_date < CURRENT_DATE\n",
    "    AND global_entity_id in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "  WHERE dps.created_date >= DATE_SUB(e.release_date, INTERVAL 8 WEEK) \n",
    "    AND dps.created_date < CURRENT_DATE\n",
    "    AND dps.platform_order_code IS NOT NULL\n",
    "    AND dps.is_own_delivery\n",
    "    AND dps.is_sent\n",
    "    AND vendor_vertical_parent IN ('Restaurant','restaurant','restaurants')\n",
    "    AND dps.entity_id in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA' ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "  GROUP BY 1, 2\n",
    "), \n",
    "customer_information AS (\n",
    "  SELECT\n",
    "         e.entity_id,\n",
    "         e.release_date,\n",
    "         CASE WHEN d.created_date <= e.release_date THEN FALSE ELSE COALESCE(is_customer_holdout, FALSE) END AS is_customer_holdout,\n",
    "         d.customer_id\n",
    "  FROM `fulfillment-dwh-production.cl.dps_holdout_users` AS d\n",
    "  JOIN holdout_entities AS e\n",
    "        ON d.entity_id = e.entity_id\n",
    "  LEFT JOIN `fulfillment-dwh-production.cl._bad_dps_logs_ids` bad_ids\n",
    "    ON d.customer_id = bad_ids.id\n",
    "  WHERE d.created_date BETWEEN '2025-01-01' AND CURRENT_DATE()\n",
    "    AND d.customer_id IS NOT NULL\n",
    "    AND bad_ids.id IS NULL\n",
    "  GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "SELECT \n",
    "   e.entity_id,\n",
    "   e.customer_id,\n",
    "   e.is_customer_holdout,\n",
    "   o.orders_pre,\n",
    "   o.orders_post,\n",
    "   o.analytical_profit_pre,\n",
    "   o.analytical_profit_post\n",
    "FROM customer_information e\n",
    "LEFT JOIN orders o\n",
    "  ON o.customer_id = e.customer_id\n",
    "  AND o.entity_id = e.entity_id\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dps_data = \"\"\"\n",
    "\n",
    "WITH holdout_entities AS (\n",
    "  SELECT\n",
    "         entity_id,\n",
    "        `Release Date` AS release_date\n",
    "  FROM `logistics-data-storage-staging.long_term_pricing.global_holdout_rollout`\n",
    "  WHERE `Release Date` < DATE_TRUNC(CURRENT_DATE(), WEEK)\n",
    "  AND entity_id not in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA' ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "),\n",
    "orders as (\n",
    "  SELECT\n",
    "       dps.entity_id entity_id\n",
    "      ,dps.dps_customer_id customer_id\n",
    "      ,COUNT(case when dps.created_date <= e.release_date then dps.platform_order_code end) AS orders_pre\n",
    "      ,COUNT(case when dps.created_date > e.release_date then dps.platform_order_code end) AS orders_post\n",
    "      ,SUM(case when dps.created_date <= e.release_date then dps.fully_loaded_gross_profit_eur end) AS analytical_profit_pre\n",
    "      ,SUM(case when dps.created_date > e.release_date then dps.fully_loaded_gross_profit_eur end) AS analytical_profit_post\n",
    "  FROM `fulfillment-dwh-production.cl.dps_sessions_mapped_to_orders` dps\n",
    "  JOIN holdout_entities AS e\n",
    "    ON dps.entity_id = e.entity_id\n",
    "  WHERE dps.created_date >= DATE_SUB(e.release_date, INTERVAL 8 WEEK) \n",
    "    AND dps.created_date < CURRENT_DATE\n",
    "    AND dps.platform_order_code IS NOT NULL\n",
    "    AND dps.is_own_delivery\n",
    "    AND dps.is_sent\n",
    "    AND vendor_vertical_parent IN ('Restaurant','restaurant','restaurants')\n",
    "    AND dps.entity_id not in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA' ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "  GROUP BY 1, 2\n",
    "), \n",
    "customer_information AS (\n",
    "  SELECT\n",
    "         e.entity_id,\n",
    "         e.release_date,\n",
    "         CASE WHEN d.created_date <= e.release_date THEN FALSE ELSE COALESCE(is_customer_holdout, FALSE) END AS is_customer_holdout,\n",
    "         d.customer_id\n",
    "  FROM `fulfillment-dwh-production.cl.dps_holdout_users` AS d\n",
    "  JOIN holdout_entities AS e\n",
    "        ON d.entity_id = e.entity_id\n",
    "  LEFT JOIN `fulfillment-dwh-production.cl._bad_dps_logs_ids` bad_ids\n",
    "    ON d.customer_id = bad_ids.id\n",
    "  WHERE d.created_date BETWEEN '2025-01-01' AND CURRENT_DATE()\n",
    "    AND d.customer_id IS NOT NULL\n",
    "    AND bad_ids.id IS NULL\n",
    "  GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "SELECT \n",
    "   e.entity_id,\n",
    "   e.customer_id,\n",
    "   e.is_customer_holdout,\n",
    "   o.orders_pre,\n",
    "   o.orders_post,\n",
    "   o.analytical_profit_pre,\n",
    "   o.analytical_profit_post\n",
    "FROM customer_information e\n",
    "LEFT JOIN orders o\n",
    "  ON o.customer_id = e.customer_id\n",
    "  AND o.entity_id = e.entity_id\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# mkt_data = \"\"\"\n",
    "\n",
    "# WITH holdout_entities AS (\n",
    "#   SELECT\n",
    "#          entity_id,\n",
    "#         `Release Date` AS release_date\n",
    "#   FROM `logistics-data-storage-staging.long_term_pricing.global_holdout_rollout`\n",
    "#   WHERE `Release Date` < DATE_TRUNC('2025-02-10', WEEK)\n",
    "#   AND entity_id in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "# ),\n",
    "# orders as (\n",
    "#   SELECT\n",
    "#        dps.entity_id entity_id\n",
    "#       ,dps.dps_customer_id customer_id\n",
    "#       ,COUNT(case when mkt.order_date <= e.release_date then mkt.order_id end) AS orders_pre\n",
    "#       ,COUNT(case when mkt.order_date > e.release_date then mkt.order_id end) AS orders_post\n",
    "#       ,SUM(case when mkt.order_date <= e.release_date then mkt.analytical_profit end) AS analytical_profit_pre\n",
    "#       ,SUM(case when mkt.order_date > e.release_date then mkt.analytical_profit end) AS analytical_profit_post\n",
    "#   FROM `fulfillment-dwh-production.cl.dps_sessions_mapped_to_orders` dps\n",
    "#   JOIN holdout_entities AS e\n",
    "#     ON dps.entity_id = e.entity_id\n",
    "#   left join fulfillment-dwh-production.curated_data_shared_mkt.bima_order_profitability mkt\n",
    "#     ON mkt.global_entity_id = dps.entity_id\n",
    "#     AND mkt.order_id = dps.platform_order_code\n",
    "#     AND order_date >= DATE_SUB(release_date, INTERVAL 8 WEEK)\n",
    "#     AND order_date < '2025-02-10'\n",
    "#     AND global_entity_id in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "#   WHERE dps.created_date >= DATE_SUB(e.release_date, INTERVAL 8 WEEK) \n",
    "#     AND dps.created_date < '2025-02-10'\n",
    "#     AND dps.platform_order_code IS NOT NULL\n",
    "#     AND dps.is_own_delivery\n",
    "#     AND dps.is_sent\n",
    "#     AND vendor_vertical_parent IN ('Restaurant','restaurant','restaurants')\n",
    "#     AND dps.entity_id in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA' ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "#   GROUP BY 1, 2\n",
    "# ), \n",
    "# customer_information AS (\n",
    "#   SELECT\n",
    "#          e.entity_id,\n",
    "#          e.release_date,\n",
    "#          CASE WHEN d.created_date <= e.release_date THEN FALSE ELSE COALESCE(is_customer_holdout, FALSE) END AS is_customer_holdout,\n",
    "#          d.customer_id\n",
    "#   FROM `fulfillment-dwh-production.cl.dps_holdout_users` AS d\n",
    "#   JOIN holdout_entities AS e\n",
    "#         ON d.entity_id = e.entity_id\n",
    "#   LEFT JOIN `fulfillment-dwh-production.cl._bad_dps_logs_ids` bad_ids\n",
    "#     ON d.customer_id = bad_ids.id\n",
    "#   WHERE d.created_date BETWEEN '2025-01-01' AND '2025-02-09'\n",
    "#     AND d.customer_id IS NOT NULL\n",
    "#     AND bad_ids.id IS NULL\n",
    "#   GROUP BY 1, 2, 3, 4\n",
    "# )\n",
    "# SELECT \n",
    "#    e.entity_id,\n",
    "#    e.customer_id,\n",
    "#    e.is_customer_holdout,\n",
    "#    o.orders_pre,\n",
    "#    o.orders_post,\n",
    "#    o.analytical_profit_pre,\n",
    "#    o.analytical_profit_post\n",
    "# FROM customer_information e\n",
    "# LEFT JOIN orders o\n",
    "#   ON o.customer_id = e.customer_id\n",
    "#   AND o.entity_id = e.entity_id\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# dps_data = \"\"\"\n",
    "\n",
    "# WITH holdout_entities AS (\n",
    "#   SELECT\n",
    "#          entity_id,\n",
    "#         `Release Date` AS release_date\n",
    "#   FROM `logistics-data-storage-staging.long_term_pricing.global_holdout_rollout`\n",
    "#   WHERE `Release Date` < DATE_TRUNC('2025-02-10', WEEK)\n",
    "#   AND entity_id not in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA' ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "# ),\n",
    "# orders as (\n",
    "#   SELECT\n",
    "#        dps.entity_id entity_id\n",
    "#       ,dps.dps_customer_id customer_id\n",
    "#       ,COUNT(case when dps.created_date <= e.release_date then dps.platform_order_code end) AS orders_pre\n",
    "#       ,COUNT(case when dps.created_date > e.release_date then dps.platform_order_code end) AS orders_post\n",
    "#       ,SUM(case when dps.created_date <= e.release_date then dps.fully_loaded_gross_profit_eur end) AS analytical_profit_pre\n",
    "#       ,SUM(case when dps.created_date > e.release_date then dps.fully_loaded_gross_profit_eur end) AS analytical_profit_post\n",
    "#   FROM `fulfillment-dwh-production.cl.dps_sessions_mapped_to_orders` dps\n",
    "#   JOIN holdout_entities AS e\n",
    "#     ON dps.entity_id = e.entity_id\n",
    "#   WHERE dps.created_date >= DATE_SUB(e.release_date, INTERVAL 8 WEEK) \n",
    "#     AND dps.created_date < '2025-02-10'\n",
    "#     AND dps.platform_order_code IS NOT NULL\n",
    "#     AND dps.is_own_delivery\n",
    "#     AND dps.is_sent\n",
    "#     AND vendor_vertical_parent IN ('Restaurant','restaurant','restaurants')\n",
    "#     AND dps.entity_id not in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA' ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI','TB_QA','TB_OM','TB_KW','TB_JO','TB_IQ','TB_BH','TB_AE','HS_SA')\n",
    "#   GROUP BY 1, 2\n",
    "# ), \n",
    "# customer_information AS (\n",
    "#   SELECT\n",
    "#          e.entity_id,\n",
    "#          e.release_date,\n",
    "#          CASE WHEN d.created_date <= e.release_date THEN FALSE ELSE COALESCE(is_customer_holdout, FALSE) END AS is_customer_holdout,\n",
    "#          d.customer_id\n",
    "#   FROM `fulfillment-dwh-production.cl.dps_holdout_users` AS d\n",
    "#   JOIN holdout_entities AS e\n",
    "#         ON d.entity_id = e.entity_id\n",
    "#   LEFT JOIN `fulfillment-dwh-production.cl._bad_dps_logs_ids` bad_ids\n",
    "#     ON d.customer_id = bad_ids.id\n",
    "#   WHERE d.created_date BETWEEN '2025-01-01' AND '2025-02-09'\n",
    "#     AND d.customer_id IS NOT NULL\n",
    "#     AND bad_ids.id IS NULL\n",
    "#   GROUP BY 1, 2, 3, 4\n",
    "# )\n",
    "# SELECT \n",
    "#    e.entity_id,\n",
    "#    e.customer_id,\n",
    "#    e.is_customer_holdout,\n",
    "#    o.orders_pre,\n",
    "#    o.orders_post,\n",
    "#    o.analytical_profit_pre,\n",
    "#    o.analytical_profit_post\n",
    "# FROM customer_information e\n",
    "# LEFT JOIN orders o\n",
    "#   ON o.customer_id = e.customer_id\n",
    "#   AND o.entity_id = e.entity_id\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "def initialize_bigquery_client(project_id_pass):\n",
    "   \n",
    "    # Define your Google Cloud project ID\n",
    "    project_id = project_id_pass \n",
    "\n",
    "    logging.info(f\"Initializing BigQuery client for project: {project_id}\")\n",
    "\n",
    "    # Determine credentials path (GitHub Actions vs. Local)\n",
    "    if os.getenv(\"GITHUB_ACTIONS\"):\n",
    "        credentials_path = \"/tmp/credentials.json\" \n",
    "    else:\n",
    "        credentials, project = default()  \n",
    "        project_id = project if project else project_id \n",
    "\n",
    "    if os.getenv(\"GITHUB_ACTIONS\") and not os.path.exists(credentials_path):\n",
    "        raise FileNotFoundError(f\"Credentials file not found at {credentials_path}. Make sure to set up authentication in GitHub Actions.\")\n",
    "\n",
    "    if os.getenv(\"GITHUB_ACTIONS\"):\n",
    "        with open(credentials_path, \"r\") as f:\n",
    "            creds_data = json.load(f)\n",
    "\n",
    "        credentials = Credentials.from_authorized_user_info(creds_data)\n",
    "\n",
    "        if credentials.expired and credentials.refresh_token:\n",
    "            credentials.refresh(Request())\n",
    "\n",
    "    # Initialize the BigQuery client with explicit credentials and project_id\n",
    "    try:\n",
    "        client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "        logging.info(f\"BigQuery client initialized successfully for project: {project_id}\")\n",
    "        return client  # Return the BigQuery client for further use\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def combined_data(client, mkt, dps):\n",
    "    \n",
    "    try:\n",
    "        mkt_df = client.query(mkt).to_dataframe()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing marketing data query: {e}\")\n",
    "        mkt_df = pd.DataFrame() \n",
    "\n",
    "    try:\n",
    "        dps_df = client.query(dps).to_dataframe()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing DPS data query: {e}\")\n",
    "        dps_df = pd.DataFrame()\n",
    "\n",
    "    # Combine the DataFrames\n",
    "    combined = pd.concat([mkt_df, dps_df], ignore_index=True)\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "#Function to create a dataset in Bigquery\n",
    "def bq_create_dataset(client, dataset):\n",
    "    dataset_ref = client.dataset(dataset)\n",
    "\n",
    "    try:\n",
    "        dataset = client.get_dataset(dataset_ref)\n",
    "        print('Dataset {} already exists.'.format(dataset))\n",
    "    except NotFound:\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = 'US'\n",
    "        dataset = client.create_dataset(dataset)\n",
    "        print('Dataset {} created.'.format(dataset.dataset_id))\n",
    "    return dataset\n",
    "\n",
    "#Function to create a Table\n",
    "def bq_create_table(client, dataset, table_name, schema):\n",
    "    dataset_ref = client.dataset(dataset)\n",
    "    table_ref = dataset_ref.table(table_name)\n",
    "\n",
    "    try:\n",
    "        table = client.get_table(table_ref)\n",
    "        print(f'Table {table.table_id} already exists.')\n",
    "    except NotFound:\n",
    "        table = bigquery.Table(table_ref, schema=schema)  # Create a Table object\n",
    "        table = client.create_table(table)  # Create the table in BigQuery\n",
    "        print(f'Table {table.table_id} created.')\n",
    "\n",
    "    return table\n",
    "\n",
    "# Function to drop a table \n",
    "def drop_table(client, dataset_id, table_id):\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    client.delete_table(table_ref, not_found_ok=True)  # not_found_ok=True prevents errors if the table doesn't exist.\n",
    "    print(f\"Table {dataset_id}.{table_id} deleted successfully.\")\n",
    "\n",
    "# Function to insert rows to a table\n",
    "def insert_df_rows_bigquery(client, dataset_id, table_id, df):\n",
    "    \n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    rows_to_insert = df.to_dict(orient='records')\n",
    "\n",
    "    errors = client.insert_rows(table, rows_to_insert)\n",
    "\n",
    "    if errors:\n",
    "        print(\"Encountered errors while inserting rows: \", errors)\n",
    "    else:\n",
    "        print(f\"Successfully inserted {len(rows_to_insert)} rows into {dataset_id}.{table_id}.\")\n",
    "\n",
    "\n",
    "def apply_nan_to_orders(df):\n",
    "    \n",
    "    mask_post = df['analytical_profit_post'].isna() & (df['orders_post'] == 0)\n",
    "    df.loc[mask_post, 'orders_post'] = np.nan\n",
    "    df['orders_post'] = df['orders_post'].astype(float)\n",
    "    \n",
    "    mask_pre = df['analytical_profit_pre'].isna() & (df['orders_pre'] == 0)\n",
    "    df.loc[mask_pre, 'orders_pre'] = np.nan\n",
    "    df['orders_pre'] = df['orders_pre'].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_missing_data(df, columns):\n",
    "    \n",
    "    return df.dropna(subset=columns)\n",
    "\n",
    "  \n",
    "def check_missing_users_data(df, groupby_col='entity_id'):\n",
    "   \n",
    "    result = (\n",
    "        df\n",
    "        .groupby(groupby_col)\n",
    "        .apply(lambda g: pd.Series({\n",
    "            'total_customers': g['customer_id'].nunique(),\n",
    "            'missing_pre': g.loc[g['orders_pre'].isna(), 'customer_id'].nunique(),\n",
    "            'missing_post': g.loc[g['orders_post'].isna(), 'customer_id'].nunique(),\n",
    "            'missing_pre_or_post': g.loc[\n",
    "                g['orders_pre'].isna() | g['orders_post'].isna(), \n",
    "                'customer_id'\n",
    "            ].nunique(),\n",
    "            'missing_pre_and_post': g.loc[\n",
    "                g['orders_pre'].isna() & g['orders_post'].isna(), \n",
    "                'customer_id'\n",
    "            ].nunique(),\n",
    "        }))\n",
    "    ).reset_index()\n",
    "\n",
    "    result['missing_pre_percentage'] = result['missing_pre'] / result['total_customers']\n",
    "    result['missing_post_percentage'] = result['missing_post'] / result['total_customers']\n",
    "    result['missing_pre_or_post_percentage'] = result['missing_pre_or_post'] / result['total_customers']\n",
    "    result['missing_pre_and_post_percentage'] = result['missing_pre_and_post'] / result['total_customers']\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_csv(df, name):\n",
    "    \n",
    "    today = datetime.today().date()\n",
    "    week_start = today - timedelta(days=today.weekday())\n",
    "    df['updated_date'] = week_start\n",
    "    \n",
    "    csv_filename = f\"{name}_{week_start}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    logging.info(f\"CSV saved as {csv_filename}\")\n",
    "\n",
    "  \n",
    "def create_holdout_table(project_id, dataset, table_name):\n",
    "\n",
    "    df = pd.read_csv('global_holdout_rollout_dates - rollout.csv')\n",
    "        \n",
    "    df = df.dropna(subset= 'entity_id')\n",
    "\n",
    "    df_final = df[['Region','Country','entity_id','Release Date','Release Status']]\n",
    "    df_final['Release Date'] = pd.to_datetime(df_final['Release Date'])\n",
    "    df_final['Release Date'] = df_final['Release Date'].dt.date\n",
    "\n",
    "\n",
    "    schema = [\n",
    "                bigquery.SchemaField('Region', 'STRING', mode='REQUIRED'),\n",
    "                bigquery.SchemaField('Country', 'STRING', mode='REQUIRED'),\n",
    "                bigquery.SchemaField('entity_id', 'STRING', mode='REQUIRED'),\n",
    "                bigquery.SchemaField('Release Date', 'DATE', mode='REQUIRED'),\n",
    "                bigquery.SchemaField('Release Status', 'STRING', mode='REQUIRED'),\n",
    "            ]\n",
    "\n",
    "    project_id = project_id\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    #drop_table(client, dataset, table_name)\n",
    "    \n",
    "    bq_create_table(client, dataset, table_name, schema)\n",
    "    insert_df_rows_bigquery(client, dataset, table_name, df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sustainable_growth(data, pre_profit, post_profit):\n",
    "    \n",
    "    data = data.copy()\n",
    "    entity = data['entity_id'].iloc[0]\n",
    "\n",
    "    # Apply CUPED to FLGP\n",
    "    data_flgp = data.dropna(subset=[pre_profit, post_profit])\n",
    "    theta_flgp = np.cov(data_flgp[pre_profit], data_flgp[post_profit])[0, 1] / np.var(data_flgp[pre_profit])\n",
    "    data_flgp['flgp_post_cuped'] = data_flgp[post_profit] - theta_flgp * (data_flgp[pre_profit] - data_flgp[pre_profit].mean())\n",
    "\n",
    "    # Apply CUPED to Orders\n",
    "    data_orders = data.dropna(subset=['orders_pre', 'orders_post'])\n",
    "    theta_orders = np.cov(data_orders['orders_pre'], data_orders['orders_post'])[0, 1] / np.var(data_orders['orders_pre'])\n",
    "    data_orders['orders_post_cuped'] = data_orders['orders_post'] - theta_orders * (data_orders['orders_pre'] - data_orders['orders_pre'].mean())\n",
    "\n",
    "    # Per User Metrics\n",
    "    holdout_flgpu_post = data_flgp.loc[data_flgp['is_customer_holdout'], 'flgp_post_cuped'].mean()\n",
    "    holdout_flgpu_pre = data_flgp.loc[data_flgp['is_customer_holdout'], pre_profit].mean()\n",
    "    \n",
    "    non_holdout_flgpu_post = data_flgp.loc[~data_flgp['is_customer_holdout'], 'flgp_post_cuped'].mean()\n",
    "    non_holdout_flgpu_pre = data_flgp.loc[~data_flgp['is_customer_holdout'], pre_profit].mean()\n",
    "\n",
    "    holdout_orders_per_user_post = data_orders.loc[data_orders['is_customer_holdout'], 'orders_post_cuped'].mean()\n",
    "    holdout_orders_per_user_pre = data_orders.loc[data_orders['is_customer_holdout'], 'orders_pre'].mean()\n",
    "    \n",
    "    non_holdout_orders_per_user_post = data_orders.loc[~data_orders['is_customer_holdout'], 'orders_post_cuped'].mean()\n",
    "    non_holdout_orders_per_user_pre = data_orders.loc[~data_orders['is_customer_holdout'], 'orders_pre'].mean()\n",
    "\n",
    "    # Apply DID\n",
    "    # DID to flgpu\n",
    "    d_flgpu_pre = non_holdout_flgpu_pre - holdout_flgpu_pre\n",
    "    d_flgpu_post = non_holdout_flgpu_post - holdout_flgpu_post\n",
    "\n",
    "    non_holdout_flgpu_adjusted_post = non_holdout_flgpu_post - d_flgpu_pre\n",
    "    holdout_flgpu_adjusted_post = holdout_flgpu_post \n",
    "\n",
    "    did_flgpu  = non_holdout_flgpu_adjusted_post - holdout_flgpu_adjusted_post\n",
    "\n",
    "    # DID to orders_per_user\n",
    "    d_orders_per_user_pre = non_holdout_orders_per_user_pre - holdout_orders_per_user_pre\n",
    "    d_orders_per_user_post = non_holdout_orders_per_user_post - holdout_orders_per_user_post\n",
    "\n",
    "    non_holdout_orders_per_user_adjusted_post = non_holdout_orders_per_user_post - d_orders_per_user_pre\n",
    "    holdout_orders_per_user_adjusted_post = holdout_orders_per_user_post\n",
    "\n",
    "    did_orders_per_user  = non_holdout_orders_per_user_adjusted_post - holdout_orders_per_user_adjusted_post\n",
    "\n",
    "    # user count\n",
    "    holdout_user_count_flgp = data_flgp['is_customer_holdout'].sum()\n",
    "    non_holdout_user_count_flgp = (~data_flgp['is_customer_holdout']).sum()\n",
    "\n",
    "    holdout_user_count_orders = data_orders['is_customer_holdout'].sum()\n",
    "    non_holdout_user_count_orders = (~data_orders['is_customer_holdout']).sum()\n",
    "\n",
    "    # Total FLGP and Orders\n",
    "    holdout_total_flgp_cuped = holdout_flgpu_adjusted_post * holdout_user_count_flgp\n",
    "    non_holdout_total_flgp_cuped = non_holdout_flgpu_adjusted_post * non_holdout_user_count_flgp\n",
    "\n",
    "    holdout_total_orders_cuped = holdout_orders_per_user_adjusted_post * holdout_user_count_orders\n",
    "    non_holdout_total_orders_cuped = non_holdout_orders_per_user_adjusted_post * non_holdout_user_count_orders\n",
    "\n",
    "    # Normalize for Population Differences\n",
    "    scaled_holdout_total_flgp_cuped = (holdout_total_flgp_cuped / holdout_user_count_flgp) * non_holdout_user_count_flgp if holdout_user_count_flgp != 0 else np.nan\n",
    "    scaled_holdout_total_orders_cuped = (holdout_total_orders_cuped / holdout_user_count_orders) * non_holdout_user_count_orders if holdout_user_count_orders != 0 else np.nan\n",
    "\n",
    "    #Calculate Per order Metrics\n",
    "    holdout_flgp_per_order_cuped = (\n",
    "    np.nan if pd.isna(holdout_total_orders_cuped) or holdout_total_orders_cuped == 0\n",
    "    else holdout_total_flgp_cuped / holdout_total_orders_cuped\n",
    "    )\n",
    "       \n",
    "    non_holdout_flgp_per_order_cuped = (\n",
    "    np.nan if pd.isna(non_holdout_total_orders_cuped) or non_holdout_total_orders_cuped == 0\n",
    "    else non_holdout_total_flgp_cuped / non_holdout_total_orders_cuped\n",
    "    )\n",
    "\n",
    "    holdout_orders_per_user_cuped = holdout_orders_per_user_adjusted_post\n",
    "    non_holdout_orders_per_user_cuped = non_holdout_orders_per_user_adjusted_post\n",
    "\n",
    "    # Incremental Differences (CUPED)\n",
    "    incremental_orders_cuped = non_holdout_total_orders_cuped - scaled_holdout_total_orders_cuped\n",
    "    incremental_flgp_cuped = non_holdout_total_flgp_cuped - scaled_holdout_total_flgp_cuped\n",
    "\n",
    "    # Percentage Changes (CUPED)\n",
    "    percentage_change_orders_cuped = ((incremental_orders_cuped) / abs(scaled_holdout_total_orders_cuped)) * 100 if scaled_holdout_total_orders_cuped != 0 else np.nan\n",
    "    percentage_change_flgp_cuped = ((incremental_flgp_cuped) / abs(scaled_holdout_total_flgp_cuped)) * 100 if scaled_holdout_total_flgp_cuped != 0 else np.nan\n",
    "\n",
    "    # Sustainable Growth Calculation\n",
    "    sustainable_growth = ((incremental_orders_cuped + (incremental_flgp_cuped / abs(non_holdout_flgp_per_order_cuped))) / scaled_holdout_total_orders_cuped) * 100 if scaled_holdout_total_orders_cuped != 0 else np.nan\n",
    "\n",
    "    # T-tests for significance\n",
    "    t_stat_orders, p_value_orders = ttest_ind(\n",
    "        data_orders.loc[data_orders['is_customer_holdout'], 'orders_post_cuped'],\n",
    "        data_orders.loc[~data_orders['is_customer_holdout'], 'orders_post_cuped'],\n",
    "        equal_var=False\n",
    "    )\n",
    "\n",
    "    t_stat_flgp, p_value_flgp = ttest_ind(\n",
    "        data_flgp.loc[data_flgp['is_customer_holdout'], 'flgp_post_cuped'],\n",
    "        data_flgp.loc[~data_flgp['is_customer_holdout'], 'flgp_post_cuped'],\n",
    "        equal_var=False\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'entity': entity,\n",
    "        'sustainable_growth': sustainable_growth,\n",
    "        'percentage_change_orders_cuped': percentage_change_orders_cuped,\n",
    "        'incremental_orders_cuped': incremental_orders_cuped,\n",
    "        'non_holdout_total_orders_cuped':non_holdout_total_orders_cuped,\n",
    "        'holdout_total_orders_cuped':scaled_holdout_total_orders_cuped,\n",
    "        't_stat_orders': t_stat_orders,\n",
    "        'p_value_orders': p_value_orders,\n",
    "        'percentage_change_flgp_cuped': percentage_change_flgp_cuped,\n",
    "        'incremental_flgp_cuped': incremental_flgp_cuped,\n",
    "        'non_holdout_total_flgp_cuped': non_holdout_total_flgp_cuped,\n",
    "        'holdout_total_flgp_cuped':scaled_holdout_total_flgp_cuped,\n",
    "        't_stat_flgp': t_stat_flgp,\n",
    "        'p_value_flgp': p_value_flgp,\n",
    "        'holdout_flgp_per_order_cuped': holdout_flgp_per_order_cuped,\n",
    "        'non_holdout_flgp_per_order_cuped': non_holdout_flgp_per_order_cuped,\n",
    "        'holdout_orders_per_user_cuped': holdout_orders_per_user_cuped,\n",
    "        'non_holdout_orders_per_user_cuped': non_holdout_orders_per_user_cuped\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(project , mkt_data, dps_data):\n",
    "    \n",
    "    # project_id = project\n",
    "    # logging.info(f\"Initializing BigQuery client for project: {project_id}\")\n",
    "    # try:\n",
    "    #     client = bigquery.Client(project=project_id)\n",
    "    # except Exception as e:\n",
    "    #     logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "    #     raise e\n",
    "\n",
    "    client = initialize_bigquery_client(project)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    combined_df = combined_data(client,mkt_data, dps_data)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"Time to extract and combine data from DB: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return combined_df \n",
    "\n",
    "def create_growth_dataframe(df):\n",
    "    \n",
    "    metric_pairs = [\n",
    "        ('analytical_profit_pre', 'analytical_profit_post'),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for pre_metric, post_metric in metric_pairs:\n",
    "        for entity in df['entity_id'].unique():\n",
    "            entity_data = df[df['entity_id'] == entity]\n",
    "            try:\n",
    "                result = calculate_sustainable_growth(entity_data, pre_metric, post_metric)\n",
    "                result['metric_used'] = f\"{pre_metric}_vs_{post_metric}\"\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Skipping entity {entity} due to error: {e}\")\n",
    "                continue\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"Time to calculate sustainable growth: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    final_results_df = pd.DataFrame(results)\n",
    "    final_results_df = final_results_df.dropna(subset=['sustainable_growth'])\n",
    "    \n",
    "    # Determine the current week's start (Monday)\n",
    "    today = datetime.today().date()\n",
    "    week_start = today - timedelta(days=today.weekday())\n",
    "    #week_start = pd.to_datetime('2025-02-10').date()\n",
    "    final_results_df['updated_date'] = week_start\n",
    "  \n",
    "    csv_filename = f\"profitable_growth_{week_start}.csv\"\n",
    "    final_results_df.to_csv(csv_filename, index=False)\n",
    "    logging.info(f\"CSV saved as {csv_filename}\")\n",
    "    \n",
    "    return final_results_df\n",
    "\n",
    "def push_data_to_bigquery(project,df, dataset, table_name, schema):\n",
    "    \n",
    "    # project_id = project\n",
    "    # try:\n",
    "    #     bigquery_client = bigquery.Client(project=project_id)\n",
    "    # except Exception as e:\n",
    "    #     logging.error(f\"Failed to initialize BigQuery client for project {project_id_new}: {e}\")\n",
    "    #     raise e\n",
    "\n",
    "    bigquery_client = initialize_bigquery_client(project)\n",
    "    \n",
    "    try:\n",
    "        bq_create_dataset(bigquery_client, dataset)\n",
    "        bq_create_table(bigquery_client, dataset, table_name, schema)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating dataset/table: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    try:\n",
    "        insert_df_rows_bigquery(\n",
    "            client=bigquery_client,\n",
    "            dataset_id=dataset,\n",
    "            table_id=table_name,\n",
    "            df=df\n",
    "        )\n",
    "        logging.info(\"Data inserted successfully into BigQuery table.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inserting data into BigQuery: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 13:40:42,732 INFO:Initializing BigQuery client for project: logistics-data-storage-staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shazeb.asad/global_pricing/venv_bayesian/lib/python3.13/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 13:40:43,387 INFO:BigQuery client initialized successfully for project: logistics-data-storage-staging\n",
      "Dataset Dataset(DatasetReference('logistics-data-storage-staging', 'long_term_pricing')) already exists.\n",
      "Table abc_performance already exists.\n",
      "Successfully inserted 53 rows into long_term_pricing.abc_performance.\n",
      "2025-04-07 13:42:00,350 INFO:Data inserted successfully into BigQuery table.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # GET DATA FROM DB\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    #raw_data = extract_data(\"logistics-customer-staging\",mkt_data, dps_data)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # CLEAN DATA\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    #make sure 0's are converted to NaN's where applicable\n",
    "    #raw_data_cleaned = apply_nan_to_orders(raw_data)\n",
    "    \n",
    "    #remove customers with no data in the pre period or the post period or in both periods\n",
    "    #needed_cols = ['analytical_profit_pre', 'analytical_profit_post','orders_pre','orders_post']\n",
    "    #raw_data_final = drop_missing_data(raw_data_cleaned, needed_cols)\n",
    "\n",
    "    #create csv with data\n",
    "    #create_csv(raw_data_final, \"profitable_growth_raw\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Calculate Sustainable Growth\n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    #sustainable_df = create_growth_dataframe(raw_data_final)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Push Data to DB\n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    schema = [\n",
    "            bigquery.SchemaField('entity', 'STRING', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('sustainable_growth', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('percentage_change_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('incremental_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_total_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_total_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('t_stat_orders', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('p_value_orders', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('percentage_change_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('incremental_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_total_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_total_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('t_stat_flgp', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('p_value_flgp', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_flgp_per_order_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_flgp_per_order_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_orders_per_user_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_orders_per_user_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('metric_used', 'STRING', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('updated_date', 'DATE', mode='REQUIRED'),\n",
    "        ]\n",
    "\n",
    "    #folder = \"shazeb\"\n",
    "    #table_name = \"abc_performance\"\n",
    "\n",
    "    #folder = \"long_term_pricing\"\n",
    "    #table_name = \"abc_performance\"\n",
    "    #push_data_to_bigquery('logistics-data-storage-staging', sustainable_df, folder, table_name, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data.groupby('entity_id').apply(lambda group: group.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data.groupby('entity_id').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_id = \"logistics-data-storage-staging\"\n",
    "# dataset_id = \"shazeb\"\n",
    "# table_name = \"abc_performance_backup\"\n",
    "# push_data_to_bigquery('logistics-data-storage-staging', sustainable_df, folder, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_id = 'logistics-data-storage-staging'\n",
    "# try:\n",
    "#     bigquery_client = bigquery.Client(project=project_id)\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Failed to initialize BigQuery client for project {project_id_new}: {e}\")\n",
    "#     raise e\n",
    "\n",
    "# drop_table(bigquery_client, \"shazeb\", \"abc_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_id = 'logistics-data-storage-staging'\n",
    "# try:\n",
    "#     bigquery_client = bigquery.Client(project=project_id)\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Failed to initialize BigQuery client for project {project_id_new}: {e}\")\n",
    "#     raise e\n",
    "\n",
    "# drop_table(bigquery_client, \"shazeb\", \"abc_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
