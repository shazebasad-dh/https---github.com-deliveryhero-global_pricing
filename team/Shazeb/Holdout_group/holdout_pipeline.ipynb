{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "import plotly.express as px\n",
    "import db_dtypes\n",
    "import bigframes.pandas as bpd\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from datetime import datetime\n",
    "from typing import Union\n",
    "import logging\n",
    "import sys\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "mkt_data = \"\"\"\n",
    "\n",
    "WITH holdout_entities AS (\n",
    "  SELECT\n",
    "         entity_id,\n",
    "        `Release Date` AS release_date\n",
    "  FROM `logistics-data-storage-staging.long_term_pricing.global_holdout_rollout`\n",
    "  WHERE `Release Date` < DATE_TRUNC(CURRENT_DATE(), WEEK)\n",
    "  AND entity_id in ('PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA'\n",
    "  ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA' ,'PO_FI','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE')\n",
    "),\n",
    "orders as (\n",
    "  SELECT\n",
    "      dps.entity_id entity_id\n",
    "      ,dps.dps_customer_id customer_id\n",
    "      ,COUNT(case when mkt.order_date <= e.release_date then mkt.order_id end) AS orders_pre\n",
    "      ,COUNT(case when mkt.order_date > e.release_date then mkt.order_id end) AS orders_post\n",
    "      ,SUM(case when mkt.order_date <= e.release_date then mkt.analytical_profit end) AS analytical_profit_pre\n",
    "      ,SUM(case when mkt.order_date > e.release_date then mkt.analytical_profit end) AS analytical_profit_post\n",
    "  FROM `fulfillment-dwh-production.cl.dps_sessions_mapped_to_orders` dps\n",
    "  JOIN holdout_entities AS e\n",
    "    ON dps.entity_id = e.entity_id\n",
    "  left join fulfillment-dwh-production.curated_data_shared_mkt.bima_order_profitability mkt\n",
    "    ON mkt.global_entity_id = dps.entity_id\n",
    "    AND mkt.order_id = dps.platform_order_code\n",
    "    AND order_date >= DATE_SUB(release_date, INTERVAL 8 WEEK)\n",
    "    AND order_date < CURRENT_DATE\n",
    "    AND global_entity_id in ('PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA' ,'PO_FI','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE')\n",
    "  WHERE dps.created_date BETWEEN DATE_SUB(e.release_date, INTERVAL 8 WEEK) AND CURRENT_DATE\n",
    "    AND dps.platform_order_code IS NOT NULL\n",
    "    AND dps.is_own_delivery\n",
    "    AND dps.is_sent\n",
    "    AND vendor_vertical_parent IN ('Restaurant','restaurant','restaurants')\n",
    "    AND dps.entity_id in ('PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA' ,'PO_FI','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE')\n",
    "  GROUP BY 1, 2\n",
    "), \n",
    "customer_information AS (\n",
    "  SELECT\n",
    "         e.entity_id,\n",
    "         e.release_date,\n",
    "         CASE WHEN d.created_date <= e.release_date THEN FALSE ELSE COALESCE(is_customer_holdout, FALSE) END AS is_customer_holdout,\n",
    "         d.customer_id\n",
    "  FROM `fulfillment-dwh-production.cl.dps_holdout_users` AS d\n",
    "  JOIN holdout_entities AS e\n",
    "        ON d.entity_id = e.entity_id\n",
    "  LEFT JOIN `fulfillment-dwh-production.cl._bad_dps_logs_ids` bad_ids\n",
    "    ON d.customer_id = bad_ids.id\n",
    "  WHERE d.created_date BETWEEN '2025-01-01' AND CURRENT_DATE()\n",
    "    AND d.customer_id IS NOT NULL\n",
    "    AND bad_ids.id IS NULL\n",
    "  GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "SELECT \n",
    "   e.entity_id,\n",
    "   e.customer_id,\n",
    "   e.is_customer_holdout,\n",
    "   o.orders_pre,\n",
    "   o.orders_post,\n",
    "   o.analytical_profit_pre,\n",
    "   o.analytical_profit_post\n",
    "FROM customer_information e\n",
    "LEFT JOIN orders o\n",
    "  ON o.customer_id = e.customer_id\n",
    "  AND o.entity_id = e.entity_id\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dps_data = \"\"\"\n",
    "\n",
    "WITH holdout_entities AS (\n",
    "  SELECT\n",
    "         entity_id,\n",
    "        `Release Date` AS release_date\n",
    "  FROM `logistics-data-storage-staging.long_term_pricing.global_holdout_rollout`\n",
    "  WHERE `Release Date` < DATE_TRUNC(CURRENT_DATE(), WEEK)\n",
    "  AND entity_id not in ('PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA'\n",
    "  ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA' ,'PO_FI','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE')\n",
    "),\n",
    "orders as (\n",
    "  SELECT\n",
    "      dps.entity_id entity_id\n",
    "      ,dps.dps_customer_id customer_id\n",
    "      ,COUNT(case when dps.created_date <= e.release_date then dps.platform_order_code end) AS orders_pre\n",
    "      ,COUNT(case when dps.created_date > e.release_date then dps.platform_order_code end) AS orders_post\n",
    "      ,SUM(case when dps.created_date <= e.release_date then dps.fully_loaded_gross_profit_eur end) AS analytical_profit_pre\n",
    "      ,SUM(case when dps.created_date > e.release_date then dps.fully_loaded_gross_profit_eur end) AS analytical_profit_post\n",
    "  FROM `fulfillment-dwh-production.cl.dps_sessions_mapped_to_orders` dps\n",
    "  JOIN holdout_entities AS e\n",
    "    ON dps.entity_id = e.entity_id\n",
    "  WHERE dps.created_date BETWEEN DATE_SUB(e.release_date, INTERVAL 8 WEEK) AND CURRENT_DATE\n",
    "    AND dps.platform_order_code IS NOT NULL\n",
    "    AND dps.is_own_delivery\n",
    "    AND dps.is_sent\n",
    "    AND vendor_vertical_parent IN ('Restaurant','restaurant','restaurants')\n",
    "    AND dps.entity_id not in ('PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA' ,'PO_FI','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE')\n",
    "  GROUP BY 1, 2\n",
    "), \n",
    "customer_information AS (\n",
    "  SELECT\n",
    "         e.entity_id,\n",
    "         e.release_date,\n",
    "         CASE WHEN d.created_date <= e.release_date THEN FALSE ELSE COALESCE(is_customer_holdout, FALSE) END AS is_customer_holdout,\n",
    "         d.customer_id\n",
    "  FROM `fulfillment-dwh-production.cl.dps_holdout_users` AS d\n",
    "  JOIN holdout_entities AS e\n",
    "        ON d.entity_id = e.entity_id\n",
    "  LEFT JOIN `fulfillment-dwh-production.cl._bad_dps_logs_ids` bad_ids\n",
    "    ON d.customer_id = bad_ids.id\n",
    "  WHERE d.created_date BETWEEN '2025-01-01' AND CURRENT_DATE()\n",
    "    AND d.customer_id IS NOT NULL\n",
    "    AND bad_ids.id IS NULL\n",
    "  GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "SELECT \n",
    "   e.entity_id,\n",
    "   e.customer_id,\n",
    "   e.is_customer_holdout,\n",
    "   o.orders_pre,\n",
    "   o.orders_post,\n",
    "   o.analytical_profit_pre,\n",
    "   o.analytical_profit_post\n",
    "FROM customer_information e\n",
    "LEFT JOIN orders o\n",
    "  ON o.customer_id = e.customer_id\n",
    "  AND o.entity_id = e.entity_id\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def combined_data(mkt, dps):\n",
    "\n",
    "  mkt_df = client.query(mkt).to_dataframe()\n",
    "  dps_df = client.query(dps).to_dataframe()\n",
    "\n",
    "  # Append DataFrames\n",
    "  combined = pd.concat([mkt_df, dps_df], ignore_index=True)\n",
    "\n",
    "  return combined\n",
    "\n",
    "\n",
    "#Function to create a dataset in Bigquery\n",
    "def bq_create_dataset(client, dataset):\n",
    "    dataset_ref = bigquery_client.dataset(dataset)\n",
    "\n",
    "    try:\n",
    "        dataset = bigquery_client.get_dataset(dataset_ref)\n",
    "        print('Dataset {} already exists.'.format(dataset))\n",
    "    except NotFound:\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = 'US'\n",
    "        dataset = bigquery_client.create_dataset(dataset)\n",
    "        print('Dataset {} created.'.format(dataset.dataset_id))\n",
    "    return dataset\n",
    "\n",
    "#Function to create a Table\n",
    "def bq_create_table(client, dataset, table_name):\n",
    "    dataset_ref = bigquery_client.dataset(dataset)\n",
    "\n",
    "    table_ref = dataset_ref.table(table_name)\n",
    "\n",
    "    try:\n",
    "        table =  bigquery_client.get_table(table_ref)\n",
    "        print('table {} already exists.'.format(table))\n",
    "    except NotFound:\n",
    "        schema = [\n",
    "            bigquery.SchemaField('entity', 'STRING', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('sustainable_growth', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('percentage_change_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('incremental_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_total_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_total_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('t_stat_orders', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('p_value_orders', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('percentage_change_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('incremental_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_total_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_total_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('t_stat_flgp', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('p_value_flgp', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_flgp_per_order_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_flgp_per_order_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_orders_per_user_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_orders_per_user_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('metric_used', 'STRING', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('updated_date', 'DATE', mode='REQUIRED'),\n",
    "        ]\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table = bigquery_client.create_table(table)\n",
    "        print('table {} created.'.format(table.table_id))\n",
    "    return table\n",
    "\n",
    "# Function to drop a table \n",
    "def drop_table(client, dataset_id, table_id):\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    client.delete_table(table_ref, not_found_ok=True)  # not_found_ok=True prevents errors if the table doesn't exist.\n",
    "    print(f\"Table {dataset_id}.{table_id} deleted successfully.\")\n",
    "\n",
    "# Function to insert rows to a table\n",
    "def insert_df_rows_bigquery(client, dataset_id, table_id, df):\n",
    "    \n",
    "    # 1. Reference the table\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    # 2. Fetch the table object to get its schema/order\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    # 3. Convert the DataFrame to a list of dictionaries\n",
    "    # Each dict corresponds to a row with column names as keys\n",
    "    # These keys must match your tableâ€™s column names\n",
    "    rows_to_insert = df.to_dict(orient='records')\n",
    "\n",
    "    # 4. Insert the rows\n",
    "    errors = client.insert_rows(table, rows_to_insert)\n",
    "\n",
    "    # 5. Check for errors\n",
    "    if errors:\n",
    "        print(\"Encountered errors while inserting rows: \", errors)\n",
    "    else:\n",
    "        print(f\"Successfully inserted {len(rows_to_insert)} rows into {dataset_id}.{table_id}.\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sustainable_growth(data, pre_profit, post_profit):\n",
    "    \n",
    "    data = data.copy()\n",
    "    entity = data['entity_id'].iloc[0]\n",
    "\n",
    "\n",
    "    # Apply CUPED to FLGP\n",
    "    data_flgp = data.dropna(subset=[pre_profit, post_profit])\n",
    "    theta_flgp = np.cov(data_flgp[pre_profit], data_flgp[post_profit])[0, 1] / np.var(data_flgp[pre_profit])\n",
    "    data_flgp['flgp_post_cuped'] = data_flgp[post_profit] - theta_flgp * (data_flgp[pre_profit] - data_flgp[pre_profit].mean())\n",
    "\n",
    "    # Apply CUPED to Orders\n",
    "    data_orders = data.dropna(subset=['orders_pre', 'orders_post'])\n",
    "    theta_orders = np.cov(data_orders['orders_pre'], data_orders['orders_post'])[0, 1] / np.var(data_orders['orders_pre'])\n",
    "    data_orders['orders_post_cuped'] = data_orders['orders_post'] - theta_orders * (data_orders['orders_pre'] - data_orders['orders_pre'].mean())\n",
    "\n",
    "    # Per User Metrics\n",
    "    holdout_flgpu_post = data_flgp.loc[data_flgp['is_customer_holdout'], 'flgp_post_cuped'].mean()\n",
    "    holdout_flgpu_pre = data_flgp.loc[data_flgp['is_customer_holdout'], pre_profit].mean()\n",
    "    \n",
    "    non_holdout_flgpu_post = data_flgp.loc[~data_flgp['is_customer_holdout'], 'flgp_post_cuped'].mean()\n",
    "    non_holdout_flgpu_pre = data_flgp.loc[~data_flgp['is_customer_holdout'], pre_profit].mean()\n",
    "\n",
    "    holdout_orders_per_user_post = data_orders.loc[data_orders['is_customer_holdout'], 'orders_post_cuped'].mean()\n",
    "    holdout_orders_per_user_pre = data_orders.loc[data_orders['is_customer_holdout'], 'orders_pre'].mean()\n",
    "    \n",
    "    non_holdout_orders_per_user_post = data_orders.loc[~data_orders['is_customer_holdout'], 'orders_post_cuped'].mean()\n",
    "    non_holdout_orders_per_user_pre = data_orders.loc[~data_orders['is_customer_holdout'], 'orders_pre'].mean()\n",
    "\n",
    "    # Apply DID\n",
    "    # DID to flgpu\n",
    "    d_flgpu_pre = non_holdout_flgpu_pre - holdout_flgpu_pre\n",
    "    d_flgpu_post = non_holdout_flgpu_post - holdout_flgpu_post\n",
    "\n",
    "    non_holdout_flgpu_adjusted_post = non_holdout_flgpu_post - d_flgpu_pre\n",
    "    holdout_flgpu_adjusted_post = holdout_flgpu_post \n",
    "\n",
    "    did_flgpu  = non_holdout_flgpu_adjusted_post - holdout_flgpu_adjusted_post\n",
    "\n",
    "    # DID to orders_per_user\n",
    "    d_orders_per_user_pre = non_holdout_orders_per_user_pre - holdout_orders_per_user_pre\n",
    "    d_orders_per_user_post = non_holdout_orders_per_user_post - holdout_orders_per_user_post\n",
    "\n",
    "    non_holdout_orders_per_user_adjusted_post = non_holdout_orders_per_user_post - d_orders_per_user_pre\n",
    "    holdout_orders_per_user_adjusted_post = holdout_orders_per_user_post\n",
    "\n",
    "    did_orders_per_user  = non_holdout_orders_per_user_adjusted_post - holdout_orders_per_user_adjusted_post\n",
    "\n",
    "    # Total FLGP and Orders\n",
    "    holdout_user_count = data['is_customer_holdout'].sum()\n",
    "    non_holdout_user_count = (~data['is_customer_holdout']).sum()\n",
    "\n",
    "    holdout_total_flgp_cuped = holdout_flgpu_adjusted_post * holdout_user_count\n",
    "    non_holdout_total_flgp_cuped = non_holdout_flgpu_adjusted_post * non_holdout_user_count\n",
    "\n",
    "    holdout_total_orders_cuped = holdout_orders_per_user_adjusted_post * holdout_user_count\n",
    "    non_holdout_total_orders_cuped = non_holdout_orders_per_user_adjusted_post * non_holdout_user_count\n",
    "\n",
    "    # Normalize for Population Differences\n",
    "    holdout_user_count = data['is_customer_holdout'].sum()\n",
    "    non_holdout_user_count = (~data['is_customer_holdout']).sum()\n",
    "\n",
    "    scaled_holdout_total_flgp_cuped = (holdout_total_flgp_cuped / holdout_user_count) * non_holdout_user_count if holdout_user_count != 0 else np.nan\n",
    "    scaled_holdout_total_orders_cuped = (holdout_total_orders_cuped / holdout_user_count) * non_holdout_user_count if holdout_user_count != 0 else np.nan\n",
    "\n",
    "    #Calculate Per order Metrics\n",
    "    holdout_flgp_per_order_cuped = (\n",
    "    np.nan if pd.isna(holdout_total_orders_cuped) or holdout_total_orders_cuped == 0\n",
    "    else holdout_total_flgp_cuped / holdout_total_orders_cuped\n",
    "    )\n",
    "       \n",
    "    non_holdout_flgp_per_order_cuped = (\n",
    "    np.nan if pd.isna(non_holdout_total_orders_cuped) or non_holdout_total_orders_cuped == 0\n",
    "    else non_holdout_total_flgp_cuped / non_holdout_total_orders_cuped\n",
    "    )\n",
    "\n",
    "    holdout_orders_per_user_cuped = holdout_orders_per_user_adjusted_post\n",
    "    non_holdout_orders_per_user_cuped = non_holdout_orders_per_user_adjusted_post\n",
    "\n",
    "    # Incremental Differences (CUPED)\n",
    "    incremental_orders_cuped = non_holdout_total_orders_cuped - scaled_holdout_total_orders_cuped\n",
    "    incremental_flgp_cuped = non_holdout_total_flgp_cuped - scaled_holdout_total_flgp_cuped\n",
    "\n",
    "    # Percentage Changes (CUPED)\n",
    "    percentage_change_orders_cuped = ((incremental_orders_cuped) / abs(scaled_holdout_total_orders_cuped)) * 100 if scaled_holdout_total_orders_cuped != 0 else np.nan\n",
    "    percentage_change_flgp_cuped = ((incremental_flgp_cuped) / abs(scaled_holdout_total_flgp_cuped)) * 100 if scaled_holdout_total_flgp_cuped != 0 else np.nan\n",
    "\n",
    "    # Sustainable Growth Calculation\n",
    "    sustainable_growth = ((incremental_orders_cuped + (incremental_flgp_cuped / non_holdout_flgp_per_order_cuped)) / scaled_holdout_total_orders_cuped) * 100 if scaled_holdout_total_orders_cuped != 0 else np.nan\n",
    "\n",
    "    # T-tests for significance\n",
    "    t_stat_orders, p_value_orders = ttest_ind(\n",
    "        data_orders.loc[data_orders['is_customer_holdout'], 'orders_post_cuped'],\n",
    "        data_orders.loc[~data_orders['is_customer_holdout'], 'orders_post_cuped'],\n",
    "        equal_var=False\n",
    "    )\n",
    "\n",
    "    t_stat_flgp, p_value_flgp = ttest_ind(\n",
    "        data_flgp.loc[data_flgp['is_customer_holdout'], 'flgp_post_cuped'],\n",
    "        data_flgp.loc[~data_flgp['is_customer_holdout'], 'flgp_post_cuped'],\n",
    "        equal_var=False\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'entity': entity,\n",
    "        'sustainable_growth': sustainable_growth,\n",
    "        'percentage_change_orders_cuped': percentage_change_orders_cuped,\n",
    "        'incremental_orders_cuped': incremental_orders_cuped,\n",
    "        'non_holdout_total_orders_cuped':non_holdout_total_orders_cuped,\n",
    "        'holdout_total_orders_cuped':scaled_holdout_total_orders_cuped,\n",
    "        't_stat_orders': t_stat_orders,\n",
    "        'p_value_orders': p_value_orders,\n",
    "        'percentage_change_flgp_cuped': percentage_change_flgp_cuped,\n",
    "        'incremental_flgp_cuped': incremental_flgp_cuped,\n",
    "        'non_holdout_total_flgp_cuped': non_holdout_total_flgp_cuped,\n",
    "        'holdout_total_flgp_cuped':scaled_holdout_total_flgp_cuped,\n",
    "        't_stat_flgp': t_stat_flgp,\n",
    "        'p_value_flgp': p_value_flgp,\n",
    "        'holdout_flgp_per_order_cuped': holdout_flgp_per_order_cuped,\n",
    "        'non_holdout_flgp_per_order_cuped': non_holdout_flgp_per_order_cuped,\n",
    "        'holdout_orders_per_user_cuped': holdout_orders_per_user_cuped,\n",
    "        'non_holdout_orders_per_user_cuped': non_holdout_orders_per_user_cuped\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(project , mkt_data, dps_data):\n",
    "    \n",
    "    project_id = project\n",
    "    logging.info(f\"Initializing BigQuery client for project: {project_id}\")\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    start_time = time.time()\n",
    "    combined_df = combined_data(mkt_data, dps_data)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"Time to extract and combine data from DB: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def create_growth_dataframe(df):\n",
    "    \n",
    "    metric_pairs = [\n",
    "        ('analytical_profit_pre', 'analytical_profit_post'),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for pre_metric, post_metric in metric_pairs:\n",
    "        for entity in df['entity_id'].unique():\n",
    "            entity_data = df[df['entity_id'] == entity]\n",
    "            try:\n",
    "                result = calculate_sustainable_growth(entity_data, pre_metric, post_metric)\n",
    "                result['metric_used'] = f\"{pre_metric}_vs_{post_metric}\"\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Skipping entity {entity} due to error: {e}\")\n",
    "                continue\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"Time to calculate sustainable growth: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    final_results_df = pd.DataFrame(results)\n",
    "    final_results_df = final_results_df.dropna(subset=['sustainable_growth'])\n",
    "    \n",
    "    # Determine the current week's start (Monday)\n",
    "    today = datetime.today().date()\n",
    "    week_start = today - timedelta(days=today.weekday())\n",
    "    final_results_df['updated_date'] = week_start\n",
    "    \n",
    "    csv_filename = f\"profitable_growth_{week_start}.csv\"\n",
    "    final_results_df.to_csv(csv_filename, index=False)\n",
    "    logging.info(f\"CSV saved as {csv_filename}\")\n",
    "    \n",
    "    return final_results_df\n",
    "\n",
    "def push_data_to_bigquery(project,df, dataset=\"shazeb\", table_name=\"abc_performance\"):\n",
    "    \n",
    "    project_id = project\n",
    "    try:\n",
    "        bigquery_client = bigquery.Client(project=project_id_new)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client for project {project_id_new}: {e}\")\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        bq_create_dataset(bigquery_client, dataset)\n",
    "        bq_create_table(bigquery_client, dataset, table_name)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating dataset/table: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    try:\n",
    "        insert_df_rows_bigquery(\n",
    "            client=bigquery_client,\n",
    "            dataset_id=dataset,\n",
    "            table_id=table_name,\n",
    "            df=final_results_df\n",
    "        )\n",
    "        logging.info(\"Data inserted successfully into BigQuery table.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inserting data into BigQuery: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/_default.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">76</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> UserWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a </span><span style=\"color: #808000; text-decoration-color: #808000\">\"quota exceeded\"</span><span style=\"color: #808000; text-decoration-color: #808000\"> or </span><span style=\"color: #808000; text-decoration-color: #808000\">\"API not enabled\"</span><span style=\"color: #808000; text-decoration-color: #808000\"> error. See the following page for troubleshooting: </span><span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/\u001b[0m\u001b[1;33m_default.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m76\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \u001b[0m\u001b[33m\"quota exceeded\"\u001b[0m\u001b[33m or \u001b[0m\u001b[33m\"API not enabled\"\u001b[0m\u001b[33m error. See the following page for troubleshooting: \u001b[0m\u001b[4;33mhttps://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-02-17T20:27:51.136+0100\u001b[0m] {\u001b[34m_default.py:\u001b[0m683} \u001b[33mWARNING\u001b[0m - \u001b[33mNo project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\u001b[0m\n",
      "Dataset Dataset(DatasetReference('logistics-data-storage-staging', 'shazeb')) already exists.\n",
      "table logistics-data-storage-staging.shazeb.abc_performance already exists.\n",
      "Successfully inserted 52 rows into shazeb.abc_performance.\n",
      "[\u001b[34m2025-02-17T20:27:52.565+0100\u001b[0m] {\u001b[34m2397728300.py:\u001b[0m75} INFO\u001b[0m - Data inserted successfully into BigQuery table.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # GET DATA FROM DB\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    raw_data = extract_data(\"logistics-customer-staging\",mkt_data, dps_data)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Calculate Sustainable Growth\n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    sustainable_df = create_growth_dataframe(raw_data)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Push Data to DB\n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    folder = \"shazeb\"\n",
    "    table_name = \"abc_performance\"\n",
    "    push_data_to_bigquery('logistics-data-storage-staging', sustainable_df, folder, table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/_default.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">76</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> UserWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a </span><span style=\"color: #808000; text-decoration-color: #808000\">\"quota exceeded\"</span><span style=\"color: #808000; text-decoration-color: #808000\"> or </span><span style=\"color: #808000; text-decoration-color: #808000\">\"API not enabled\"</span><span style=\"color: #808000; text-decoration-color: #808000\"> error. See the following page for troubleshooting: </span><span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/\u001b[0m\u001b[1;33m_default.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m76\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \u001b[0m\u001b[33m\"quota exceeded\"\u001b[0m\u001b[33m or \u001b[0m\u001b[33m\"API not enabled\"\u001b[0m\u001b[33m error. See the following page for troubleshooting: \u001b[0m\u001b[4;33mhttps://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-02-17T20:27:29.719+0100\u001b[0m] {\u001b[34m_default.py:\u001b[0m683} \u001b[33mWARNING\u001b[0m - \u001b[33mNo project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\u001b[0m\n",
      "Table shazeb.abc_performance deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# project_id = 'logistics-data-storage-staging'\n",
    "# try:\n",
    "#     bigquery_client = bigquery.Client(project=project_id)\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Failed to initialize BigQuery client for project {project_id_new}: {e}\")\n",
    "#     raise e\n",
    "\n",
    "# drop_table(bigquery_client, \"shazeb\", \"abc_performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
