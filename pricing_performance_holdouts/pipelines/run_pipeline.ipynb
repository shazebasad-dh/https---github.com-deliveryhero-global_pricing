{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# Setup logging early\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Set up paths so local modules work\n",
    "# ---------------------------------------------------\n",
    "sys.path.append(str(Path(__file__).resolve().parent))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Import local modules\n",
    "# ---------------------------------------------------\n",
    "from data.extract import initialize_bigquery_client, extract_data\n",
    "from data.bigquery_queries import get_marketing_data, get_dps_data\n",
    "from data.transform import apply_cleanup\n",
    "from data.cuped import apply_cuped_adjustment\n",
    "from data.store import store_data_cloud\n",
    "from utils.dates import get_iso_week_mondays\n",
    "from utils.summary_stats import summarize_columns\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Pipeline function\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def run_pipeline(project_id: str,\n",
    "                 entities: List[str],\n",
    "                 weeks: List[pd.Timestamp],\n",
    "                 restaurant_flag: str = 'IN',\n",
    "                 pre_post_metric_pairs: List = [(\"orders_pre\", \"orders_post\"), \n",
    "                                                (\"analytical_profit_pre\", \"analytical_profit_post\")],\n",
    "                 save_cloud: bool = False,\n",
    "                 save_local: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Full holdout CUPED analysis pipeline.\n",
    "\n",
    "    Args:\n",
    "        project_id (str): GCP project ID.\n",
    "        entities (List[str]): List of entity IDs.\n",
    "        weeks (List[pd.Timestamp]): List of weeks to iterate.\n",
    "        restaurant_flag (str): 'IN' or 'NOT IN' for restaurant filtering.\n",
    "        pre_post_metric_pairs (List[tuple]): Pre/post metrics for CUPED.\n",
    "        save_cloud (bool): If True, save output to GCS.\n",
    "        save_local (bool): If True, also save weekly parquet locally.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Final combined DataFrame after CUPED adjustment.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Starting pipeline...\")\n",
    "    client = initialize_bigquery_client(project_id)\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for week in weeks:\n",
    "        logger.info(f\"Processing week: {week}\")\n",
    "\n",
    "        mkt_query = get_marketing_data(entities, week, restaurant_flag=restaurant_flag)\n",
    "        dps_query = get_dps_data(entities, week, restaurant_flag=restaurant_flag)\n",
    "\n",
    "        # Extract\n",
    "        raw_data = extract_data(client, mkt_query, dps_query)\n",
    "        raw_data[\"as_of_date\"] = week\n",
    "\n",
    "        # Transform\n",
    "        cleaned_data = apply_cleanup(raw_data)\n",
    "\n",
    "        final_df = pd.concat([final_df, cleaned_data], ignore_index=True)\n",
    "\n",
    "    # CUPED\n",
    "    logger.info(\"Applying CUPED adjustment...\")\n",
    "    cuped_data = apply_cuped_adjustment(final_df, pre_post_metric_pairs=pre_post_metric_pairs)\n",
    "\n",
    "    # Store results\n",
    "    if save_cloud or save_local:\n",
    "        logger.info(\"Saving data...\")\n",
    "        store_data_cloud(\n",
    "            df=cuped_data,\n",
    "            week_dates=weeks,\n",
    "            save_cloud_storage=save_cloud,\n",
    "            save_local=save_local\n",
    "        )\n",
    "\n",
    "    logger.info(\"Pipeline complete.\")\n",
    "    return cuped_data\n",
    "\n",
    "\n",
    "def store_data_historically():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import date\n",
    "\n",
    "from pipeline.historical_pipeline import store_data_historically\n",
    "\n",
    "# Set up logging globally\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "def main():\n",
    "    \n",
    "    project_id = \"your-gcp-project-id\"\n",
    "    entities = ['FP_PK', 'PY_DO']   # example entity list\n",
    "    year = 2025\n",
    "\n",
    "    min_date = date(2025, 2,2)    \n",
    "    max_date = date(2025, 2, 3)    \n",
    "\n",
    "    store_data_historically(\n",
    "        project_id=project_id,\n",
    "        entities=entities,\n",
    "        year=year,\n",
    "        min_date=min_date,\n",
    "        max_date=max_date,\n",
    "        restaurant_flag='IN',\n",
    "        save_local=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
