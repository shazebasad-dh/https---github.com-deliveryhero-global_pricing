{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:96: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:220: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:247: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:268: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:75: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:96: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:220: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:247: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:268: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22461/2319762987.py:48: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22461/2319762987.py:51: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND pe.global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22461/2319762987.py:75: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22461/2319762987.py:96: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22461/2319762987.py:220: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22461/2319762987.py:223: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND pe.global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22461/2319762987.py:247: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
      "/var/folders/7c/hjrbzbpn61jgnsn14y5f3l7c0000gq/T/ipykernel_22461/2319762987.py:268: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import db_dtypes\n",
    "import bigframes.pandas as bpd\n",
    "from IPython.display import display, HTML\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def user_details(start_date, end_date,entity_d):\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH listing AS (    \n",
    "        SELECT   global_entity_id\n",
    "                ,country\n",
    "                ,CAST(DATE_TRUNC(injestion_time, MONTH) AS DATE) session_month\n",
    "                ,CAST(DATE_TRUNC(injestion_time, ISOWEEK) AS DATE) session_week\n",
    "                ,CAST(DATE_TRUNC(injestion_time, DAY) AS DATE) session_day\n",
    "                ,EXTRACT(HOUR FROM injestion_time) AS session_hour\n",
    "                ,session_key \n",
    "                ,perseus_session_id\n",
    "                ,chainId \n",
    "                ,shopId\n",
    "                ,userId\n",
    "                ,df_impressions \n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(pe.global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,userId\n",
    "                ,ingestion_timestamp injestion_time\n",
    "                ,country\n",
    "                ,COALESCE(chainId, JSON_VALUE(eventVariables_json, \"$.chainId\") ) AS chainId\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY ingestion_timestamp) row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")) df_raw\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events` pe\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'shop_impressions.loaded'\n",
    "                AND screenType = 'shop_list'\n",
    "                AND pe.global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "                AND locationCity IS NOT NULL\n",
    "                AND shopType = 'restaurants'\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        AND df_raw IS NOT NULL\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), shop_details AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'shop_details.loaded'\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), checkout AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'checkout.loaded'\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), orders AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction IN ('transaction')\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), rates AS (\n",
    "        SELECT cu.country_iso\n",
    "            ,cu.currency_code\n",
    "            ,tmp.fx_rate_eur\n",
    "        FROM `fulfillment-dwh-production.cl.countries` cu\n",
    "        JOIN ( \n",
    "            WITH latest_fx_rate AS (\n",
    "            SELECT \n",
    "                currency_code,\n",
    "                fx_rate_eur,\n",
    "                calculated_at,\n",
    "                ROW_NUMBER() OVER (PARTITION BY currency_code ORDER BY calculated_at DESC) AS rn\n",
    "            FROM `fulfillment-dwh-production.curated_data_shared_coredata.fx_rates`\n",
    "            )\n",
    "            SELECT \n",
    "                currency_code,\n",
    "                fx_rate_eur,\n",
    "                calculated_at AS max_calculated_at\n",
    "            FROM latest_fx_rate\n",
    "            WHERE rn = 1\n",
    "        ) tmp ON tmp.currency_code = cu.currency_code\n",
    "        GROUP BY 1, 2, 3\n",
    "        ORDER BY 1\n",
    "    )\n",
    "    SELECT      l.global_entity_id\n",
    "                ,l.userId\n",
    "                -- Delivery fee listing calculations\n",
    "                ,AVG(l.df_impressions / r.fx_rate_eur) AS delivery_fee_listing_eur_mean\n",
    "                ,APPROX_QUANTILES(l.df_impressions / r.fx_rate_eur, 100)[OFFSET(50)] AS delivery_fee_listing_eur_median\n",
    "                ,MIN(l.df_impressions / r.fx_rate_eur) AS delivery_fee_listing_eur_min\n",
    "                ,MAX(l.df_impressions / r.fx_rate_eur) AS delivery_fee_listing_eur_max\n",
    "                ,STDDEV(l.df_impressions / r.fx_rate_eur) AS delivery_fee_listing_eur_stddev\n",
    "                \n",
    "                -- Delivery fee details page calculations\n",
    "                ,AVG(sd.df_impressions / r.fx_rate_eur) AS delivery_fee_details_eur_mean\n",
    "                ,APPROX_QUANTILES(sd.df_impressions / r.fx_rate_eur, 100)[OFFSET(50)] AS delivery_fee_details_eur_median\n",
    "                ,MIN(sd.df_impressions / r.fx_rate_eur) AS delivery_fee_details_eur_min\n",
    "                ,MAX(sd.df_impressions / r.fx_rate_eur) AS delivery_fee_details_eur_max\n",
    "                ,STDDEV(sd.df_impressions / r.fx_rate_eur) AS delivery_fee_details_eur_stddev\n",
    "                \n",
    "                -- Delivery fee checkout calculations\n",
    "                ,AVG(co.df_impressions / r.fx_rate_eur) AS delivery_fee_checkout_eur_mean\n",
    "                ,APPROX_QUANTILES(co.df_impressions / r.fx_rate_eur, 100)[OFFSET(50)] AS delivery_fee_checkout_eur_median\n",
    "                ,MIN(co.df_impressions / r.fx_rate_eur) AS delivery_fee_checkout_eur_min\n",
    "                ,MAX(co.df_impressions / r.fx_rate_eur) AS delivery_fee_checkout_eur_max\n",
    "                ,STDDEV(co.df_impressions / r.fx_rate_eur) AS delivery_fee_checkout_eur_stddev\n",
    "                \n",
    "                -- Delivery fee order calculations\n",
    "                ,AVG(o.df_impressions / r.fx_rate_eur) AS delivery_fee_order_eur_mean\n",
    "                ,APPROX_QUANTILES(o.df_impressions / r.fx_rate_eur, 100)[OFFSET(50)] AS delivery_fee_order_eur_median\n",
    "                ,MIN(o.df_impressions / r.fx_rate_eur) AS delivery_fee_order_eur_min\n",
    "                ,MAX(o.df_impressions / r.fx_rate_eur) AS delivery_fee_order_eur_ma\n",
    "                ,STDDEV(o.df_impressions / r.fx_rate_eur) AS delivery_fee_order_eur_stddev\n",
    "                ,COUNT(l.shopId) AS total_vendor\n",
    "                ,SUM(CASE WHEN sd.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_details_page\n",
    "                ,SUM(CASE WHEN co.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_checkout\n",
    "                ,SUM(CASE WHEN o.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_order\n",
    "    FROM listing l\n",
    "    LEFT JOIN shop_details sd ON sd.global_entity_id = l.global_entity_id AND sd.session_key = l.session_key AND sd.shopId = l.shopId\n",
    "    LEFT JOIN checkout co ON co.global_entity_id = l.global_entity_id AND co.session_key = l.session_key AND co.shopId = l.shopId\n",
    "    LEFT JOIN orders o ON o.global_entity_id = l.global_entity_id AND o.session_key = l.session_key AND o.shopId = l.shopId\n",
    "    LEFT JOIN rates r ON l.country = r.country_iso\n",
    "    GROUP BY 1, 2\n",
    "    \"\"\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "\n",
    "def user_conversion(start_date, end_date,entity_d):\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH listing AS (    \n",
    "        SELECT   global_entity_id\n",
    "                ,country\n",
    "                ,CAST(DATE_TRUNC(injestion_time, MONTH) AS DATE) session_month\n",
    "                ,CAST(DATE_TRUNC(injestion_time, ISOWEEK) AS DATE) session_week\n",
    "                ,CAST(DATE_TRUNC(injestion_time, DAY) AS DATE) session_day\n",
    "                ,EXTRACT(HOUR FROM injestion_time) AS session_hour\n",
    "                ,session_key \n",
    "                ,perseus_session_id\n",
    "                ,chainId \n",
    "                ,shopId\n",
    "                ,userId\n",
    "                ,df_impressions \n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(pe.global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,userId\n",
    "                ,ingestion_timestamp injestion_time\n",
    "                ,country\n",
    "                ,COALESCE(chainId, JSON_VALUE(eventVariables_json, \"$.chainId\") ) AS chainId\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY ingestion_timestamp) row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")) df_raw\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events` pe\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'shop_impressions.loaded'\n",
    "                AND screenType = 'shop_list'\n",
    "                AND pe.global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "                AND locationCity IS NOT NULL\n",
    "                AND shopType = 'restaurants'\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        AND df_raw IS NOT NULL\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), shop_details AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'shop_details.loaded'\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), checkout AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction = 'checkout.loaded'\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), orders AS (\n",
    "        SELECT  global_entity_id\n",
    "                ,session_key\n",
    "                ,perseus_session_id\n",
    "                ,shopId\n",
    "                ,df_impressions\n",
    "        FROM (\n",
    "        SELECT  session_key\n",
    "                ,COALESCE(global_entity_id, JSON_VALUE(eventVariables_json, \"$.globalEntityId\") ) AS global_entity_id\n",
    "                ,platform AS platform\n",
    "                ,sessionId AS perseus_session_id\n",
    "                ,ROW_NUMBER() OVER (PARTITION BY session_key, shopId ORDER BY \"timestamp\") row_num\n",
    "                ,COALESCE(shopId, JSON_VALUE(eventVariables_json, \"$.shopId\") ) AS shopId\n",
    "                ,COALESCE(CAST(NULLIF(REGEXP_EXTRACT(COALESCE(JSON_VALUE(eventVariables_json, \"$.vendorDeliveryFee\"), JSON_VALUE(eventVariables_json, \"$.shopDeliveryFee\")), r'([0-9]+\\.?[0-9]*)'), '') AS FLOAT64), 0) df_impressions\n",
    "        FROM `fulfillment-dwh-production.curated_data_shared_coredata_tracking.perseus_events`\n",
    "        WHERE partition_date BETWEEN \\\"\"\"\" + start_date + \"\"\"\\\" and \\\"\"\"\" + end_date + \"\"\"\\\"\n",
    "                AND eventAction IN ('transaction')\n",
    "                AND global_entity_id IN (\"\"\" + entity_d + \"\"\")\n",
    "        )\n",
    "        WHERE row_num = 1\n",
    "        ORDER BY global_entity_id, session_key\n",
    "    ), rates AS (\n",
    "        SELECT cu.country_iso\n",
    "            ,cu.currency_code\n",
    "            ,tmp.fx_rate_eur\n",
    "        FROM `fulfillment-dwh-production.cl.countries` cu\n",
    "        JOIN ( \n",
    "            WITH latest_fx_rate AS (\n",
    "            SELECT \n",
    "                currency_code,\n",
    "                fx_rate_eur,\n",
    "                calculated_at,\n",
    "                ROW_NUMBER() OVER (PARTITION BY currency_code ORDER BY calculated_at DESC) AS rn\n",
    "            FROM `fulfillment-dwh-production.curated_data_shared_coredata.fx_rates`\n",
    "            )\n",
    "            SELECT \n",
    "                currency_code,\n",
    "                fx_rate_eur,\n",
    "                calculated_at AS max_calculated_at\n",
    "            FROM latest_fx_rate\n",
    "            WHERE rn = 1\n",
    "        ) tmp ON tmp.currency_code = cu.currency_code\n",
    "        GROUP BY 1, 2, 3\n",
    "        ORDER BY 1\n",
    "    ), impression as (\n",
    "    SELECT      l.global_entity_id\n",
    "                ,l.country\n",
    "                ,l.userId\n",
    "                ,ROUND((l.df_impressions / r.fx_rate_eur) * 5) / 5 AS delivery_fee_listing\n",
    "                ,COUNT(l.shopId) AS total_vendor\n",
    "                ,SUM(CASE WHEN sd.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_details_page\n",
    "                ,SUM(CASE WHEN co.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_checkout\n",
    "                ,SUM(CASE WHEN o.shopId IS NOT NULL THEN 1 ELSE 0 END) AS converted_order\n",
    "                ,SUM(CASE WHEN sd.shopId IS NOT NULL THEN 1 ELSE 0 END) / COUNT(l.shopId) conversion_details\n",
    "                ,SUM(CASE WHEN co.shopId IS NOT NULL THEN 1 ELSE 0 END) / COUNT(l.shopId) conversion_checkout\n",
    "                ,SUM(CASE WHEN o.shopId IS NOT NULL THEN 1 ELSE 0 END) / COUNT(l.shopId) conversion_order\n",
    "    FROM listing l\n",
    "    LEFT JOIN shop_details sd ON sd.global_entity_id = l.global_entity_id AND sd.session_key = l.session_key AND sd.shopId = l.shopId\n",
    "    LEFT JOIN checkout co ON co.global_entity_id = l.global_entity_id AND co.session_key = l.session_key AND co.shopId = l.shopId\n",
    "    LEFT JOIN orders o ON o.global_entity_id = l.global_entity_id AND o.session_key = l.session_key AND o.shopId = l.shopId\n",
    "    LEFT JOIN rates r ON l.country = r.country_iso\n",
    "    GROUP BY 1, 2, 3, 4\n",
    "    )\n",
    "    select * \n",
    "    from (\n",
    "    select  global_entity_id\n",
    "        ,delivery_fee_listing\n",
    "        ,avg(conversion_details) avg_conversion_details\n",
    "        ,avg(conversion_checkout) avg_conversion_checkout\n",
    "        ,avg(conversion_order) avg_conversion_order\n",
    "        ,sum(total_vendor) vendors\n",
    "        ,count(distinct userId) total_users\n",
    "        ,count(distinct case when converted_details_page > 0 then userId end) converted_user_count_details\n",
    "        ,count(distinct case when converted_checkout > 0 then userId end) converted_user_count_checkout\n",
    "        ,count(distinct case when converted_order > 0 then userId end) converted_user_count_order\n",
    "    FROM impression\n",
    "    group by 1,2\n",
    "    order by 1,2\n",
    "    )\n",
    "    \n",
    "    where vendors > 100\n",
    "    \"\"\"\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "# def user_conversion_funnel(df):\n",
    "    \n",
    "#     for i in df['global_entity_id'].unique():\n",
    "\n",
    "#         df_tmp = df[df['global_entity_id'] == i]\n",
    "\n",
    "#         # Groupby and aggregate data for conversion counts\n",
    "#         user_conversion = df_tmp.groupby(['global_entity_id']).agg(\n",
    "#             converted_user_count_order=('userId', lambda x: x[df_tmp['converted_order'] > 0].nunique()),\n",
    "#             converted_user_count_details=('userId', lambda x: x[df_tmp['converted_details_page'] > 0].nunique()),\n",
    "#             converted_user_count_checkout=('userId', lambda x: x[df_tmp['converted_checkout'] > 0].nunique()),\n",
    "#             total_user_count=('userId', 'nunique')\n",
    "#         ).reset_index()\n",
    "\n",
    "#         # Calculate funnel metrics\n",
    "#         user_conversion['user_listing'] = user_conversion['total_user_count'] / user_conversion['total_user_count']\n",
    "#         user_conversion['user_listing_to_details'] = user_conversion['converted_user_count_details'] / user_conversion['total_user_count']\n",
    "#         user_conversion['user_listing_to_checkout'] = user_conversion['converted_user_count_checkout'] / user_conversion['total_user_count']\n",
    "#         user_conversion['user_listing_to_transaction'] = user_conversion['converted_user_count_order'] / user_conversion['total_user_count']\n",
    "\n",
    "    \n",
    "#         # List of conversion stages\n",
    "#         lst = [\n",
    "#         round(user_conversion['user_listing'][0] * 100, 0),  # Convert to percentage and round\n",
    "#         round(user_conversion['user_listing_to_details'][0] * 100, 0),\n",
    "#         round(user_conversion['user_listing_to_checkout'][0] * 100, 0),\n",
    "#         round(user_conversion['user_listing_to_transaction'][0] * 100, 0)\n",
    "#         ]\n",
    "\n",
    "#         # Data for funnel plot\n",
    "#         funnel_data = dict(\n",
    "#             number=lst,\n",
    "#             stage=[\"Listing\", \"Listing_to_details\", \"Listing_to_checkout\", \"Listing_to_transaction\"]\n",
    "#         )\n",
    "\n",
    "#         # Create funnel plot using plotly\n",
    "#         fig = px.funnel(funnel_data, x='number', y='stage')\n",
    "#         fig.update_layout(title='User conversion ' + i, width=750,height=400)\n",
    "#         fig.show()\n",
    "\n",
    "def user_conversion_funnel(df):\n",
    "    \n",
    "    # Create a list to store data for all entities\n",
    "    funnel_data_list = []\n",
    "    \n",
    "    for i in df['global_entity_id'].unique():\n",
    "\n",
    "        df_tmp = df[df['global_entity_id'] == i]\n",
    "\n",
    "        # Groupby and aggregate data for conversion counts\n",
    "        user_conversion = df_tmp.groupby(['global_entity_id']).agg(\n",
    "            converted_user_count_order=('userId', lambda x: x[df_tmp['converted_order'] > 0].nunique()),\n",
    "            converted_user_count_details=('userId', lambda x: x[df_tmp['converted_details_page'] > 0].nunique()),\n",
    "            converted_user_count_checkout=('userId', lambda x: x[df_tmp['converted_checkout'] > 0].nunique()),\n",
    "            total_user_count=('userId', 'nunique')\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate funnel metrics\n",
    "        user_conversion['user_listing'] = user_conversion['total_user_count'] / user_conversion['total_user_count']\n",
    "        user_conversion['user_listing_to_details'] = user_conversion['converted_user_count_details'] / user_conversion['total_user_count']\n",
    "        user_conversion['user_listing_to_checkout'] = user_conversion['converted_user_count_checkout'] / user_conversion['total_user_count']\n",
    "        user_conversion['user_listing_to_transaction'] = user_conversion['converted_user_count_order'] / user_conversion['total_user_count']\n",
    "\n",
    "        # Append the conversion stages for the current entity to the list\n",
    "        funnel_data_list.append({\n",
    "            'global_entity_id': i,\n",
    "            'stage': 'Listing',\n",
    "            'conversion_rate': round(user_conversion['user_listing'][0] * 100, 0)\n",
    "        })\n",
    "        funnel_data_list.append({\n",
    "            'global_entity_id': i,\n",
    "            'stage': 'Listing_to_details',\n",
    "            'conversion_rate': round(user_conversion['user_listing_to_details'][0] * 100, 0)\n",
    "        })\n",
    "        funnel_data_list.append({\n",
    "            'global_entity_id': i,\n",
    "            'stage': 'Listing_to_checkout',\n",
    "            'conversion_rate': round(user_conversion['user_listing_to_checkout'][0] * 100, 0)\n",
    "        })\n",
    "        funnel_data_list.append({\n",
    "            'global_entity_id': i,\n",
    "            'stage': 'Listing_to_transaction',\n",
    "            'conversion_rate': round(user_conversion['user_listing_to_transaction'][0] * 100, 0)\n",
    "        })\n",
    "\n",
    "    # Create a DataFrame from the list\n",
    "    funnel_data_df = pd.DataFrame(funnel_data_list)\n",
    "\n",
    "    # Create a funnel plot for all entities\n",
    "    fig = px.funnel(funnel_data_df, x='conversion_rate', y='stage', color='global_entity_id')\n",
    "    fig.update_layout(title='User Conversion Funnel Across Entities', width=800, height=600)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_delivery_fee_boxplot(df, column_list):\n",
    "    \n",
    "    for i in df['global_entity_id'].unique():\n",
    "        \n",
    "        df_tmp = df[df['global_entity_id'] == i]\n",
    "        \n",
    "        if not all(col in df_tmp.columns for col in column_list):\n",
    "            raise ValueError(f\"Some columns in {column_list} do not exist in the DataFrame for entity {i}\")\n",
    "        \n",
    "        stage_mapping = {col: col.split('_')[2].capitalize() if len(col.split('_')) > 2 else col for col in column_list}\n",
    "        \n",
    "        delivery_fee_mean = df_tmp[column_list]\n",
    "        \n",
    "        df_melted = delivery_fee_mean.melt(var_name='Stage', value_name='Delivery Fee (EUR)')\n",
    "        \n",
    "        df_melted['Stage'] = df_melted['Stage'].replace(stage_mapping)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(data=df_melted, x='Stage', y='Delivery Fee (EUR)')\n",
    "        \n",
    "        plt.title(f'{i} Delivery Fees Across Different Stages', fontsize=16)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "def descriptive_stats(df):\n",
    "    # Group the data by 'global_entity_id'\n",
    "    grouped = df.groupby('global_entity_id') \n",
    "\n",
    "    # Create an empty dictionary to store the descriptive statistics\n",
    "    grouped_descriptive_stats = {}\n",
    "\n",
    "    # Define the manual order for sorting\n",
    "    manual_order = [\n",
    "        'delivery_fee_listing_eur_mean', 'delivery_fee_details_eur_mean', 'delivery_fee_checkout_eur_mean', 'delivery_fee_order_eur_mean',\n",
    "        'delivery_fee_listing_eur_median', 'delivery_fee_details_eur_median', 'delivery_fee_checkout_eur_median', 'delivery_fee_order_eur_median',\n",
    "        'delivery_fee_listing_eur_min', 'delivery_fee_details_eur_min', 'delivery_fee_checkout_eur_min', 'delivery_fee_order_eur_min',\n",
    "        'delivery_fee_listing_eur_max', 'delivery_fee_details_eur_max', 'delivery_fee_checkout_eur_max', 'delivery_fee_order_eur_ma',\n",
    "        'delivery_fee_listing_eur_stddev', 'delivery_fee_details_eur_stddev', 'delivery_fee_checkout_eur_stddev', 'delivery_fee_order_eur_stddev',\n",
    "        'total_vendor', 'converted_details_page', 'converted_checkout', 'converted_order'\n",
    "    ]\n",
    "\n",
    "    # Loop through each group and calculate descriptive statistics\n",
    "    for name, group in grouped:\n",
    "        descriptive_stats = group.describe().transpose()\n",
    "\n",
    "        # Reorder the DataFrame based on the manual order\n",
    "        descriptive_stats = descriptive_stats.reindex(manual_order)\n",
    "\n",
    "        # Style the descriptive statistics for better readability\n",
    "        styled = descriptive_stats.style \\\n",
    "                        .format(precision=2, thousands=\",\", decimal=\".\") \\\n",
    "                        .format_index(str.upper, axis=1)\n",
    "        \n",
    "        grouped_descriptive_stats[name] = styled\n",
    "\n",
    "    # Display each sorted group's styled descriptive statistics\n",
    "    for name, styled_df in grouped_descriptive_stats.items():\n",
    "        print(f\"Group: {name}\")\n",
    "        display(styled_df)\n",
    "\n",
    "\n",
    "def plot_entity_kde(df, delivery_fee_column, threshold=10, num_cols=4):\n",
    "    \"\"\"\n",
    "    Plots a grid of KDE plots for each unique entity in the dataset filtered by a delivery fee threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    user_summary_df (pd.DataFrame): The DataFrame containing the data.\n",
    "    delivery_fee_column (str): The column name for delivery fee to plot the KDE.\n",
    "    threshold (float, optional): The threshold for filtering the delivery fees. Default is 10.\n",
    "    num_cols (int, optional): Number of columns in the grid. Default is 4.\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays the KDE plots for each entity.\n",
    "    \"\"\"\n",
    "    # Filter the dataset where the delivery fee is less than the specified threshold\n",
    "    filtered_df = df[df[delivery_fee_column] < threshold]\n",
    "\n",
    "    # Get the unique entity IDs\n",
    "    entities = filtered_df['global_entity_id'].unique()\n",
    "\n",
    "    # Define the number of rows for the grid based on the number of entities and columns\n",
    "    num_entities = len(entities)\n",
    "    num_rows = math.ceil(num_entities / num_cols)\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Iterate over each entity and plot a KDE for its values\n",
    "    for i, entity in enumerate(entities):\n",
    "        subset = filtered_df[filtered_df['global_entity_id'] == entity]  # Filter data for each entity\n",
    "        \n",
    "        # Plot KDE for the entity on the corresponding subplot\n",
    "        sns.kdeplot(subset[delivery_fee_column], label=f'Entity {entity}', fill=True, ax=axes[i])\n",
    "        \n",
    "        # Set title and legend\n",
    "        axes[i].set_title(f'Entity {entity}')\n",
    "        axes[i].legend()\n",
    "\n",
    "    # Remove any unused subplots if the number of entities is less than the grid size\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_conversion(df):\n",
    "        \n",
    "    for i in df['global_entity_id'].unique():\n",
    "\n",
    "            df_tmp = df[df['global_entity_id'] == i]\n",
    "    \n",
    "            # Create a figure with four subplots (2x2 layout)\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "            # Plot user_conversion_details as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='user_conversion_details', ax=axes[0, 0], label='Details Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='user_conversion_details', scatter=False, ax=axes[0, 0], color='blue')\n",
    "            axes[0, 0].set_title('Details Conversion vs. Delivery Fee')\n",
    "            axes[0, 0].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 0].set_ylabel('Details Conversion Rate')\n",
    "\n",
    "            # Plot user_conversion_checkout as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='user_conversion_checkout', ax=axes[0, 1], label='Checkout Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='user_conversion_checkout', scatter=False, ax=axes[0, 1], color='blue')\n",
    "            axes[0, 1].set_title('Checkout Conversion vs. Delivery Fee')\n",
    "            axes[0, 1].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 1].set_ylabel('Checkout Conversion Rate')\n",
    "\n",
    "            # Plot user_conversion_order as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='user_conversion_order', ax=axes[0, 2], label='Order Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='user_conversion_order', scatter=False, ax=axes[0, 2], color='blue')\n",
    "            axes[0, 2].set_title('Order Conversion vs. Delivery Fee')\n",
    "            axes[0, 2].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 2].set_ylabel('Order Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_details as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='avg_conversion_details', ax=axes[1, 0], label='Avg Details Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='avg_conversion_details', scatter=False, ax=axes[1, 0], color='orange')\n",
    "            axes[1, 0].set_title('Avg Details Conversion vs. Delivery Fee')\n",
    "            axes[1, 0].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 0].set_ylabel('Avg Details Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_checkout as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='avg_conversion_checkout', ax=axes[1, 1], label='Avg Checkout Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='avg_conversion_checkout', scatter=False, ax=axes[1, 1], color='orange')\n",
    "            axes[1, 1].set_title('Avg Checkout Conversion vs. Delivery Fee')\n",
    "            axes[1, 1].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 1].set_ylabel('Avg Checkout Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_order as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee', y='avg_conversion_order', ax=axes[1, 2], label='Avg Order Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee', y='avg_conversion_order', scatter=False, ax=axes[1, 2], color='orange')\n",
    "            axes[1, 2].set_title('Avg Order Conversion vs. Delivery Fee')\n",
    "            axes[1, 2].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 2].set_ylabel('Avg Order Conversion Rate')\n",
    "\n",
    "            # Adjust layout and show plot\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def plot_ch(df, column_list, num_cols=4):\n",
    "    \"\"\"\n",
    "    Plots a grid of boxplots for delivery fees across different stages for each entity,\n",
    "    keeping only data below the 95th percentile for each column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    column_list (list): The list of columns representing delivery fees at different stages.\n",
    "    num_cols (int, optional): Number of columns in the grid. Default is 4.\n",
    "    \n",
    "    Returns:\n",
    "    None: Displays the boxplots for each entity.\n",
    "    \"\"\"\n",
    "    # Get unique entity IDs\n",
    "    entities = df['global_entity_id'].unique()\n",
    "\n",
    "    # Define the number of rows for the grid based on the number of entities and columns\n",
    "    num_entities = len(entities)\n",
    "    num_rows = math.ceil(num_entities / num_cols)\n",
    "\n",
    "    # Create a grid of subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))\n",
    "\n",
    "    # Flatten the axes array for easy iteration\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, entity in enumerate(entities):\n",
    "        df_tmp = df[df['global_entity_id'] == entity]\n",
    "        \n",
    "        # Check if all columns exist\n",
    "        if not all(col in df_tmp.columns for col in column_list):\n",
    "            raise ValueError(f\"Some columns in {column_list} do not exist in the DataFrame for entity {entity}\")\n",
    "\n",
    "        # Filter values below the 95th percentile for each column\n",
    "        df_filtered = df_tmp[column_list].apply(lambda x: x[x < x.quantile(0.95)])\n",
    "\n",
    "        # Create a mapping for the stages\n",
    "        stage_mapping = {col: col.split('_')[2].capitalize() if len(col.split('_')) > 2 else col for col in column_list}\n",
    "        \n",
    "        # Melt the filtered data for plotting\n",
    "        df_melted = df_filtered.melt(var_name='Stage', value_name='Delivery Fee (EUR)')\n",
    "        \n",
    "        # Replace the stage names with the mapped values\n",
    "        df_melted['Stage'] = df_melted['Stage'].replace(stage_mapping)\n",
    "\n",
    "        # Plot the boxplot for the entity on the corresponding subplot\n",
    "        sns.boxplot(data=df_melted, x='Stage', y='Delivery Fee (EUR)', ax=axes[i])\n",
    "        \n",
    "        # Set title\n",
    "        axes[i].set_title(f'Entity {entity}')\n",
    "    \n",
    "    # Remove any unused subplots if the number of entities is less than the grid size\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_conversion(df):\n",
    "        \n",
    "    for i in df['global_entity_id'].unique():\n",
    "\n",
    "            df_tmp = df[df['global_entity_id'] == i]\n",
    "\n",
    "            # Create a figure with four subplots (2x2 layout)\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(14, 10))\n",
    "\n",
    "            # Plot user_conversion_details as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_details', ax=axes[0, 0], label='Details Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_details', scatter=False, ax=axes[0, 0], color='blue')\n",
    "            axes[0, 0].set_title('Details Conversion vs. Delivery Fee ' + i )\n",
    "            axes[0, 0].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 0].set_ylabel('Details Conversion Rate')\n",
    "\n",
    "            # Plot user_conversion_checkout as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_checkout', ax=axes[0, 1], label='Checkout Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_checkout', scatter=False, ax=axes[0, 1], color='blue')\n",
    "            axes[0, 1].set_title('Checkout Conversion vs. Delivery Fee ' + i )\n",
    "            axes[0, 1].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 1].set_ylabel('Checkout Conversion Rate')\n",
    "\n",
    "            # Plot user_conversion_order as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_order', ax=axes[0, 2], label='Order Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='user_conversion_order', scatter=False, ax=axes[0, 2], color='blue')\n",
    "            axes[0, 2].set_title('Order Conversion vs. Delivery Fee ' + i )\n",
    "            axes[0, 2].set_xlabel('Delivery Fee (€)')\n",
    "            axes[0, 2].set_ylabel('Order Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_details as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_details', ax=axes[1, 0], label='Avg Details Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_details', scatter=False, ax=axes[1, 0], color='orange')\n",
    "            axes[1, 0].set_title('Avg Details Conversion vs. Delivery Fee ' + i )\n",
    "            axes[1, 0].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 0].set_ylabel('Avg Details Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_checkout as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_checkout', ax=axes[1, 1], label='Avg Checkout Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_checkout', scatter=False, ax=axes[1, 1], color='orange')\n",
    "            axes[1, 1].set_title('Avg Checkout Conversion vs. Delivery Fee ' + i )\n",
    "            axes[1, 1].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 1].set_ylabel('Avg Checkout Conversion Rate')\n",
    "\n",
    "            # Plot avg_conversion_order as scatter plot with regression\n",
    "            sns.scatterplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_order', ax=axes[1, 2], label='Avg Order Conversion')\n",
    "            sns.regplot(data=df_tmp, x='delivery_fee_listing', y='avg_conversion_order', scatter=False, ax=axes[1, 2], color='orange')\n",
    "            axes[1, 2].set_title('Avg Order Conversion vs. Delivery Fee ' + i )\n",
    "            axes[1, 2].set_xlabel('Delivery Fee (€)')\n",
    "            axes[1, 2].set_ylabel('Avg Order Conversion Rate')\n",
    "\n",
    "            # Adjust layout and show plot\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "ename": "BadRequest",
     "evalue": "400 GET https://bigquery.googleapis.com/bigquery/v2/projects/logistics-customer-staging/queries/0ba426ae-cd03-4694-ab2d-604d30f5ec2c?maxResults=0&location=US&prettyPrint=false: Resources exceeded during query execution: The query could not be executed in the allotted memory. Peak usage: 129% of limit.\nTop memory consumer(s):\n  query parsing and optimization: 93%\n  other/unattributed: 7%\n\n\nLocation: US\nJob ID: 0ba426ae-cd03-4694-ab2d-604d30f5ec2c\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m project_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistics-customer-staging\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m client \u001b[38;5;241m=\u001b[39m bigquery\u001b[38;5;241m.\u001b[39mClient(project \u001b[38;5;241m=\u001b[39m project_id)\n\u001b[0;32m---> 17\u001b[0m user_conversion_df \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43mentity_id_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#user_summary_df = client.query(user_details(start_date, end_date,entity_id_str)).to_dataframe()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# user_conversion_df = bpd.read_gbq(user_conversion(start_date, end_date,entity_id_str))\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# user_summary_df = bpd.read_gbq(user_details(start_date, end_date,entity_id_str))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m user_conversion_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_conversion_details\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m user_conversion_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconverted_user_count_details\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m user_conversion_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_users\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:2052\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dataframe\u001b[39m(\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1824\u001b[0m     bqstorage_client: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     ] \u001b[38;5;241m=\u001b[39m DefaultPandasDTypes\u001b[38;5;241m.\u001b[39mRANGE_TIMESTAMP_DTYPE,\n\u001b[1;32m   1845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m \n\u001b[1;32m   1848\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;124;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2052\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[43mwait_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result\u001b[38;5;241m.\u001b[39mto_dataframe(\n\u001b[1;32m   2054\u001b[0m         bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[1;32m   2055\u001b[0m         dtypes\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2069\u001b[0m         range_timestamp_dtype\u001b[38;5;241m=\u001b[39mrange_timestamp_dtype,\n\u001b[1;32m   2070\u001b[0m     )\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/_tqdm_helpers.py:107\u001b[0m, in \u001b[0;36mwait_for_query\u001b[0;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[1;32m    103\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m get_progress_bar(\n\u001b[1;32m    104\u001b[0m     progress_bar_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery is running\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_total, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:1676\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1671\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1674\u001b[0m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[0;32m-> 1676\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1677\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:1645\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.is_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1639\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;66;03m# Call jobs.getQueryResults with max results set to 0 just to\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;66;03m# wait for the query to finish. Unlike most methods,\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m \u001b[38;5;66;03m# jobs.getQueryResults hangs as long as it can to ensure we\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# know when the query has finished as soon as possible.\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reload_query_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreload_query_results_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;66;03m# Even if the query is finished now according to\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;66;03m# jobs.getQueryResults, we'll want to reload the job status if\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;66;03m# it's not already DONE.\u001b[39;00m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:1443\u001b[0m, in \u001b[0;36mQueryJob._reload_query_results\u001b[0;34m(self, retry, timeout, page_size)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transport_timeout, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m   1441\u001b[0m         transport_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_query_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/client.py:2024\u001b[0m, in \u001b[0;36mClient._get_query_results\u001b[0;34m(self, job_id, retry, project, timeout_ms, location, timeout, page_size)\u001b[0m\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;66;03m# This call is typically made in a polling loop that checks whether the\u001b[39;00m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;66;03m# job is complete (from QueryJob.done(), called ultimately from\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;66;03m# QueryJob.result()). So we don't need to poll here.\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[0;32m-> 2024\u001b[0m resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBigQuery.getQueryResults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _QueryResults\u001b[38;5;241m.\u001b[39mfrom_api_repr(resource)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/client.py:833\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[1;32m    831\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[1;32m    832\u001b[0m     ):\n\u001b[0;32m--> 833\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/_http/__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 GET https://bigquery.googleapis.com/bigquery/v2/projects/logistics-customer-staging/queries/0ba426ae-cd03-4694-ab2d-604d30f5ec2c?maxResults=0&location=US&prettyPrint=false: Resources exceeded during query execution: The query could not be executed in the allotted memory. Peak usage: 129% of limit.\nTop memory consumer(s):\n  query parsing and optimization: 93%\n  other/unattributed: 7%\n\n\nLocation: US\nJob ID: 0ba426ae-cd03-4694-ab2d-604d30f5ec2c\n"
     ]
    }
   ],
   "source": [
    "start_date = '2025-01-15'\n",
    "end_date = '2024-02-15'\n",
    "\n",
    "# entity_id = 'DJ_CZ','FO_NO','MJM_AT','NP_HU','OP_SE','PO_FI','YS_TR','EF_GR','FY_CY','FP_BD','FP_HK','FP_KH','FP_LA','FP_MM','FP_MY','FP_PH','FP_PK','FP_SG','FP_TH','FP_TW','HS_SA','AP_PA','PY_AR','PY_BO','PY_CL','PY_CR','PY_DO','PY_EC','PY_GT','PY_HN','PY_NI','PY_PE','PY_PY','PY_SV','PY_UY','PY_VE','HF_EG','TB_AE',\n",
    "# 'TB_BH','TB_IQ','TB_JO','TB_KW','TB_OM','TB_QA'\n",
    "\n",
    "entity_id = ('TB_OM','DJ_CZ')\n",
    "entity_id_str = \",\".join([f\"'{entity}'\" for entity in entity_id])\n",
    "\n",
    "# PROJECT_ID = \"logistics-customer-staging\"\n",
    "# bpd.options.bigquery.project = PROJECT_ID\n",
    "\n",
    "# define a few things (project id, start date etc.)\n",
    "project_id = \"logistics-customer-staging\"\n",
    "client = bigquery.Client(project = project_id)\n",
    "\n",
    "user_conversion_df = client.query(user_conversion(start_date, end_date,entity_id_str)).to_dataframe()\n",
    "#user_summary_df = client.query(user_details(start_date, end_date,entity_id_str)).to_dataframe()\n",
    "\n",
    "# user_conversion_df = bpd.read_gbq(user_conversion(start_date, end_date,entity_id_str))\n",
    "# user_summary_df = bpd.read_gbq(user_details(start_date, end_date,entity_id_str))\n",
    "\n",
    "user_conversion_df['user_conversion_details'] = user_conversion_df['converted_user_count_details'] / user_conversion_df['total_users']\n",
    "user_conversion_df['user_conversion_checkout'] = user_conversion_df['converted_user_count_checkout'] / user_conversion_df['total_users']\n",
    "user_conversion_df['user_conversion_order'] = user_conversion_df['converted_user_count_order'] / user_conversion_df['total_users']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
