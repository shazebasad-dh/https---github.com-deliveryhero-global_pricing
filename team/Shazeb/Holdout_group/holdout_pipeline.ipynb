{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Get the GCP credentials from environment variables\n",
    "gcp_credentials_json = os.getenv('GCP_CREDENTIALS_JSON')\n",
    "\n",
    "if not gcp_credentials_json:\n",
    "    raise ValueError(\"The GCP_CREDENTIALS_JSON environment variable is not set\")\n",
    "\n",
    "# Save the credentials to a temporary file\n",
    "with open('/tmp/gcp-key.json', 'w') as f:\n",
    "    f.write(gcp_credentials_json)\n",
    "\n",
    "# Authenticate with the Google Cloud service account key\n",
    "credentials = service_account.Credentials.from_service_account_file('/tmp/gcp-key.json')\n",
    "\n",
    "# Set the project ID (hardcoded or passed as an environment variable)\n",
    "project_id = \"your-google-cloud-project-id\"  # Hardcoded project ID\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Now you can use the BigQuery client as needed\n",
    "print(f\"Successfully authenticated to project: {project_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "import plotly.express as px\n",
    "import db_dtypes\n",
    "import bigframes.pandas as bpd\n",
    "from IPython.display import display, HTML\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from typing import Union\n",
    "import logging\n",
    "import sys\n",
    "from google.cloud.exceptions import NotFound\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "mkt_data = \"\"\"\n",
    "\n",
    "WITH holdout_entities AS (\n",
    "  SELECT\n",
    "         entity_id,\n",
    "        `Release Date` AS release_date\n",
    "  FROM `logistics-data-storage-staging.long_term_pricing.global_holdout_rollout`\n",
    "  WHERE `Release Date` < DATE_TRUNC(CURRENT_DATE(), WEEK)\n",
    "  AND entity_id in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA'\n",
    "  ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI')\n",
    "),\n",
    "orders as (\n",
    "  SELECT\n",
    "       dps.entity_id entity_id\n",
    "      ,dps.dps_customer_id customer_id\n",
    "      ,COUNT(case when mkt.order_date <= e.release_date then mkt.order_id end) AS orders_pre\n",
    "      ,COUNT(case when mkt.order_date > e.release_date then mkt.order_id end) AS orders_post\n",
    "      ,SUM(case when mkt.order_date <= e.release_date then mkt.analytical_profit end) AS analytical_profit_pre\n",
    "      ,SUM(case when mkt.order_date > e.release_date then mkt.analytical_profit end) AS analytical_profit_post\n",
    "  FROM `fulfillment-dwh-production.cl.dps_sessions_mapped_to_orders` dps\n",
    "  JOIN holdout_entities AS e\n",
    "    ON dps.entity_id = e.entity_id\n",
    "  left join fulfillment-dwh-production.curated_data_shared_mkt.bima_order_profitability mkt\n",
    "    ON mkt.global_entity_id = dps.entity_id\n",
    "    AND mkt.order_id = dps.platform_order_code\n",
    "    AND order_date >= DATE_SUB(release_date, INTERVAL 8 WEEK)\n",
    "    AND order_date < CURRENT_DATE\n",
    "    AND global_entity_id in ('FP_PK','PO_FI','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE')\n",
    "  WHERE dps.created_date >= DATE_SUB(e.release_date, INTERVAL 8 WEEK) \n",
    "    AND dps.created_date < CURRENT_DATE\n",
    "    AND dps.platform_order_code IS NOT NULL\n",
    "    AND dps.is_own_delivery\n",
    "    AND dps.is_sent\n",
    "    AND vendor_vertical_parent IN ('Restaurant','restaurant','restaurants')\n",
    "    AND dps.entity_id in ('FP_PK','PO_FI','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA' ,'PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE')\n",
    "  GROUP BY 1, 2\n",
    "), \n",
    "customer_information AS (\n",
    "  SELECT\n",
    "         e.entity_id,\n",
    "         e.release_date,\n",
    "         CASE WHEN d.created_date <= e.release_date THEN FALSE ELSE COALESCE(is_customer_holdout, FALSE) END AS is_customer_holdout,\n",
    "         d.customer_id\n",
    "  FROM `fulfillment-dwh-production.cl.dps_holdout_users` AS d\n",
    "  JOIN holdout_entities AS e\n",
    "        ON d.entity_id = e.entity_id\n",
    "  LEFT JOIN `fulfillment-dwh-production.cl._bad_dps_logs_ids` bad_ids\n",
    "    ON d.customer_id = bad_ids.id\n",
    "  WHERE d.created_date BETWEEN '2025-01-01' AND CURRENT_DATE()\n",
    "    AND d.customer_id IS NOT NULL\n",
    "    AND bad_ids.id IS NULL\n",
    "  GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "SELECT \n",
    "   e.entity_id,\n",
    "   e.customer_id,\n",
    "   e.is_customer_holdout,\n",
    "   o.orders_pre,\n",
    "   o.orders_post,\n",
    "   o.analytical_profit_pre,\n",
    "   o.analytical_profit_post\n",
    "FROM customer_information e\n",
    "LEFT JOIN orders o\n",
    "  ON o.customer_id = e.customer_id\n",
    "  AND o.entity_id = e.entity_id\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dps_data = \"\"\"\n",
    "\n",
    "WITH holdout_entities AS (\n",
    "  SELECT\n",
    "         entity_id,\n",
    "        `Release Date` AS release_date\n",
    "  FROM `logistics-data-storage-staging.long_term_pricing.global_holdout_rollout`\n",
    "  WHERE `Release Date` < DATE_TRUNC(CURRENT_DATE(), WEEK)\n",
    "  AND entity_id not in ('FP_PK','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA'\n",
    "  ,'YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE','PO_FI')\n",
    "),\n",
    "orders as (\n",
    "  SELECT\n",
    "       dps.entity_id entity_id\n",
    "      ,dps.dps_customer_id customer_id\n",
    "      ,COUNT(case when dps.created_date <= e.release_date then dps.platform_order_code end) AS orders_pre\n",
    "      ,COUNT(case when dps.created_date > e.release_date then dps.platform_order_code end) AS orders_post\n",
    "      ,SUM(case when dps.created_date <= e.release_date then dps.fully_loaded_gross_profit_eur end) AS analytical_profit_pre\n",
    "      ,SUM(case when dps.created_date > e.release_date then dps.fully_loaded_gross_profit_eur end) AS analytical_profit_post\n",
    "  FROM `fulfillment-dwh-production.cl.dps_sessions_mapped_to_orders` dps\n",
    "  JOIN holdout_entities AS e\n",
    "    ON dps.entity_id = e.entity_id\n",
    "  WHERE dps.created_date >= DATE_SUB(e.release_date, INTERVAL 8 WEEK) \n",
    "    AND dps.created_date < CURRENT_DATE\n",
    "    AND dps.platform_order_code IS NOT NULL\n",
    "    AND dps.is_own_delivery\n",
    "    AND dps.is_sent\n",
    "    AND vendor_vertical_parent IN ('Restaurant','restaurant','restaurants')\n",
    "    AND dps.entity_id not in ('FP_PK','PO_FI','PY_DO','PY_BO', 'FP_TW', 'PY_PY', 'DJ_CZ', 'PY_EC','MJM_AT' ,'PY_PE', 'PY_AR' ,'PY_GT','PY_SV' ,'FP_PH','PY_NI' ,'NP_HU' ,'FP_MM','EF_GR' ,'AP_PA','YS_TR', 'PY_UY' ,'OP_SE' ,'PY_CL' ,'FP_BD' ,'FP_SG' ,'FO_NO' ,'PY_CR', 'FP_LA','PY_HN', 'FP_MY' ,'FP_TH', 'FY_CY', 'PY_VE')\n",
    "  GROUP BY 1, 2\n",
    "), \n",
    "customer_information AS (\n",
    "  SELECT\n",
    "         e.entity_id,\n",
    "         e.release_date,\n",
    "         CASE WHEN d.created_date <= e.release_date THEN FALSE ELSE COALESCE(is_customer_holdout, FALSE) END AS is_customer_holdout,\n",
    "         d.customer_id\n",
    "  FROM `fulfillment-dwh-production.cl.dps_holdout_users` AS d\n",
    "  JOIN holdout_entities AS e\n",
    "        ON d.entity_id = e.entity_id\n",
    "  LEFT JOIN `fulfillment-dwh-production.cl._bad_dps_logs_ids` bad_ids\n",
    "    ON d.customer_id = bad_ids.id\n",
    "  WHERE d.created_date BETWEEN '2025-01-01' AND CURRENT_DATE()\n",
    "    AND d.customer_id IS NOT NULL\n",
    "    AND bad_ids.id IS NULL\n",
    "  GROUP BY 1, 2, 3, 4\n",
    ")\n",
    "SELECT \n",
    "   e.entity_id,\n",
    "   e.customer_id,\n",
    "   e.is_customer_holdout,\n",
    "   o.orders_pre,\n",
    "   o.orders_post,\n",
    "   o.analytical_profit_pre,\n",
    "   o.analytical_profit_post\n",
    "FROM customer_information e\n",
    "LEFT JOIN orders o\n",
    "  ON o.customer_id = e.customer_id\n",
    "  AND o.entity_id = e.entity_id\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def combined_data(client,mkt, dps):\n",
    "\n",
    "  mkt_df = client.query(mkt).to_dataframe()\n",
    "  dps_df = client.query(dps).to_dataframe()\n",
    "\n",
    "  # Append DataFrames\n",
    "  combined = pd.concat([mkt_df, dps_df], ignore_index=True)\n",
    "\n",
    "  return combined\n",
    "\n",
    "\n",
    "#Function to create a dataset in Bigquery\n",
    "def bq_create_dataset(client, dataset):\n",
    "    dataset_ref = bigquery_client.dataset(dataset)\n",
    "\n",
    "    try:\n",
    "        dataset = bigquery_client.get_dataset(dataset_ref)\n",
    "        print('Dataset {} already exists.'.format(dataset))\n",
    "    except NotFound:\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = 'US'\n",
    "        dataset = bigquery_client.create_dataset(dataset)\n",
    "        print('Dataset {} created.'.format(dataset.dataset_id))\n",
    "    return dataset\n",
    "\n",
    "#Function to create a Table\n",
    "def bq_create_table(client, dataset, table_name, schema):\n",
    "    dataset_ref = client.dataset(dataset)\n",
    "    table_ref = dataset_ref.table(table_name)\n",
    "\n",
    "    try:\n",
    "        table = client.get_table(table_ref)\n",
    "        print('Table {} already exists.'.format(table.table_id))\n",
    "    except NotFound:\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table = client.create_table(table)\n",
    "        print('Table {} created.'.format(table.table_id))\n",
    "    return table\n",
    "\n",
    "# Function to drop a table \n",
    "def drop_table(client, dataset_id, table_id):\n",
    "    \n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    client.delete_table(table_ref, not_found_ok=True)  # not_found_ok=True prevents errors if the table doesn't exist.\n",
    "    print(f\"Table {dataset_id}.{table_id} deleted successfully.\")\n",
    "\n",
    "# Function to insert rows to a table\n",
    "def insert_df_rows_bigquery(client, dataset_id, table_id, df):\n",
    "    \n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    rows_to_insert = df.to_dict(orient='records')\n",
    "\n",
    "    errors = client.insert_rows(table, rows_to_insert)\n",
    "\n",
    "    if errors:\n",
    "        print(\"Encountered errors while inserting rows: \", errors)\n",
    "    else:\n",
    "        print(f\"Successfully inserted {len(rows_to_insert)} rows into {dataset_id}.{table_id}.\")\n",
    "\n",
    "\n",
    "def apply_nan_to_orders(df):\n",
    "    \n",
    "    mask_post = df['analytical_profit_post'].isna() & (df['orders_post'] == 0)\n",
    "    df.loc[mask_post, 'orders_post'] = np.nan\n",
    "    df['orders_post'] = df['orders_post'].astype(float)\n",
    "    \n",
    "    mask_pre = df['analytical_profit_pre'].isna() & (df['orders_pre'] == 0)\n",
    "    df.loc[mask_pre, 'orders_pre'] = np.nan\n",
    "    df['orders_pre'] = df['orders_pre'].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "def drop_missing_data(df, columns):\n",
    "    \n",
    "    return df.dropna(subset=columns)\n",
    "\n",
    "  \n",
    "def check_missing_users_data(df, groupby_col='entity_id'):\n",
    "   \n",
    "    result = (\n",
    "        df\n",
    "        .groupby(groupby_col)\n",
    "        .apply(lambda g: pd.Series({\n",
    "            'total_customers': g['customer_id'].nunique(),\n",
    "            'missing_pre': g.loc[g['orders_pre'].isna(), 'customer_id'].nunique(),\n",
    "            'missing_post': g.loc[g['orders_post'].isna(), 'customer_id'].nunique(),\n",
    "            'missing_pre_or_post': g.loc[\n",
    "                g['orders_pre'].isna() | g['orders_post'].isna(), \n",
    "                'customer_id'\n",
    "            ].nunique(),\n",
    "            'missing_pre_and_post': g.loc[\n",
    "                g['orders_pre'].isna() & g['orders_post'].isna(), \n",
    "                'customer_id'\n",
    "            ].nunique(),\n",
    "        }))\n",
    "    ).reset_index()\n",
    "\n",
    "    result['missing_pre_percentage'] = result['missing_pre'] / result['total_customers']\n",
    "    result['missing_post_percentage'] = result['missing_post'] / result['total_customers']\n",
    "    result['missing_pre_or_post_percentage'] = result['missing_pre_or_post'] / result['total_customers']\n",
    "    result['missing_pre_and_post_percentage'] = result['missing_pre_and_post'] / result['total_customers']\n",
    "    \n",
    "    return result\n",
    "\n",
    "def create_csv(df, name):\n",
    "    \n",
    "    today = datetime.today().date()\n",
    "    week_start = today - timedelta(days=today.weekday())\n",
    "    df['updated_date'] = week_start\n",
    "\n",
    "    csv_filename = f\"{name}_{week_start}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    logging.info(f\"CSV saved as {csv_filename}\")\n",
    "\n",
    "  \n",
    "def create_holdout_table(project_id, dataset, table_name):\n",
    "\n",
    "    df = pd.read_csv('global_holdout_rollout_dates - rollout.csv')\n",
    "        \n",
    "    df = df.dropna(subset= 'entity_id')\n",
    "\n",
    "    df_final = df[['Region','Country','entity_id','Release Date','Release Status']]\n",
    "    df_final['Release Date'] = pd.to_datetime(df_final['Release Date'])\n",
    "    df_final['Release Date'] = df_final['Release Date'].dt.date\n",
    "\n",
    "\n",
    "    schema = [\n",
    "                bigquery.SchemaField('Region', 'STRING', mode='REQUIRED'),\n",
    "                bigquery.SchemaField('Country', 'STRING', mode='REQUIRED'),\n",
    "                bigquery.SchemaField('entity_id', 'STRING', mode='REQUIRED'),\n",
    "                bigquery.SchemaField('Release Date', 'DATE', mode='REQUIRED'),\n",
    "                bigquery.SchemaField('Release Status', 'STRING', mode='REQUIRED'),\n",
    "            ]\n",
    "\n",
    "    project_id = project_id\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    #drop_table(client, dataset, table_name)\n",
    "    \n",
    "    bq_create_table(client, dataset, table_name, schema)\n",
    "    insert_df_rows_bigquery(client, dataset, table_name, df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sustainable_growth(data, pre_profit, post_profit):\n",
    "    \n",
    "    data = data.copy()\n",
    "    entity = data['entity_id'].iloc[0]\n",
    "\n",
    "    # Apply CUPED to FLGP\n",
    "    data_flgp = data.dropna(subset=[pre_profit, post_profit])\n",
    "    theta_flgp = np.cov(data_flgp[pre_profit], data_flgp[post_profit])[0, 1] / np.var(data_flgp[pre_profit])\n",
    "    data_flgp['flgp_post_cuped'] = data_flgp[post_profit] - theta_flgp * (data_flgp[pre_profit] - data_flgp[pre_profit].mean())\n",
    "\n",
    "    # Apply CUPED to Orders\n",
    "    data_orders = data.dropna(subset=['orders_pre', 'orders_post'])\n",
    "    theta_orders = np.cov(data_orders['orders_pre'], data_orders['orders_post'])[0, 1] / np.var(data_orders['orders_pre'])\n",
    "    data_orders['orders_post_cuped'] = data_orders['orders_post'] - theta_orders * (data_orders['orders_pre'] - data_orders['orders_pre'].mean())\n",
    "\n",
    "    # Per User Metrics\n",
    "    holdout_flgpu_post = data_flgp.loc[data_flgp['is_customer_holdout'], 'flgp_post_cuped'].mean()\n",
    "    holdout_flgpu_pre = data_flgp.loc[data_flgp['is_customer_holdout'], pre_profit].mean()\n",
    "    \n",
    "    non_holdout_flgpu_post = data_flgp.loc[~data_flgp['is_customer_holdout'], 'flgp_post_cuped'].mean()\n",
    "    non_holdout_flgpu_pre = data_flgp.loc[~data_flgp['is_customer_holdout'], pre_profit].mean()\n",
    "\n",
    "    holdout_orders_per_user_post = data_orders.loc[data_orders['is_customer_holdout'], 'orders_post_cuped'].mean()\n",
    "    holdout_orders_per_user_pre = data_orders.loc[data_orders['is_customer_holdout'], 'orders_pre'].mean()\n",
    "    \n",
    "    non_holdout_orders_per_user_post = data_orders.loc[~data_orders['is_customer_holdout'], 'orders_post_cuped'].mean()\n",
    "    non_holdout_orders_per_user_pre = data_orders.loc[~data_orders['is_customer_holdout'], 'orders_pre'].mean()\n",
    "\n",
    "    # Apply DID\n",
    "    # DID to flgpu\n",
    "    d_flgpu_pre = non_holdout_flgpu_pre - holdout_flgpu_pre\n",
    "    d_flgpu_post = non_holdout_flgpu_post - holdout_flgpu_post\n",
    "\n",
    "    non_holdout_flgpu_adjusted_post = non_holdout_flgpu_post - d_flgpu_pre\n",
    "    holdout_flgpu_adjusted_post = holdout_flgpu_post \n",
    "\n",
    "    did_flgpu  = non_holdout_flgpu_adjusted_post - holdout_flgpu_adjusted_post\n",
    "\n",
    "    # DID to orders_per_user\n",
    "    d_orders_per_user_pre = non_holdout_orders_per_user_pre - holdout_orders_per_user_pre\n",
    "    d_orders_per_user_post = non_holdout_orders_per_user_post - holdout_orders_per_user_post\n",
    "\n",
    "    non_holdout_orders_per_user_adjusted_post = non_holdout_orders_per_user_post - d_orders_per_user_pre\n",
    "    holdout_orders_per_user_adjusted_post = holdout_orders_per_user_post\n",
    "\n",
    "    did_orders_per_user  = non_holdout_orders_per_user_adjusted_post - holdout_orders_per_user_adjusted_post\n",
    "\n",
    "    # user count\n",
    "    holdout_user_count_flgp = data_flgp['is_customer_holdout'].sum()\n",
    "    non_holdout_user_count_flgp = (~data_flgp['is_customer_holdout']).sum()\n",
    "\n",
    "    holdout_user_count_orders = data_orders['is_customer_holdout'].sum()\n",
    "    non_holdout_user_count_orders = (~data_orders['is_customer_holdout']).sum()\n",
    "\n",
    "    # Total FLGP and Orders\n",
    "    holdout_total_flgp_cuped = holdout_flgpu_adjusted_post * holdout_user_count_flgp\n",
    "    non_holdout_total_flgp_cuped = non_holdout_flgpu_adjusted_post * non_holdout_user_count_flgp\n",
    "\n",
    "    holdout_total_orders_cuped = holdout_orders_per_user_adjusted_post * holdout_user_count_orders\n",
    "    non_holdout_total_orders_cuped = non_holdout_orders_per_user_adjusted_post * non_holdout_user_count_orders\n",
    "\n",
    "    # Normalize for Population Differences\n",
    "    scaled_holdout_total_flgp_cuped = (holdout_total_flgp_cuped / holdout_user_count_flgp) * non_holdout_user_count_flgp if holdout_user_count_flgp != 0 else np.nan\n",
    "    scaled_holdout_total_orders_cuped = (holdout_total_orders_cuped / holdout_user_count_orders) * non_holdout_user_count_orders if holdout_user_count_orders != 0 else np.nan\n",
    "\n",
    "    #Calculate Per order Metrics\n",
    "    holdout_flgp_per_order_cuped = (\n",
    "    np.nan if pd.isna(holdout_total_orders_cuped) or holdout_total_orders_cuped == 0\n",
    "    else holdout_total_flgp_cuped / holdout_total_orders_cuped\n",
    "    )\n",
    "       \n",
    "    non_holdout_flgp_per_order_cuped = (\n",
    "    np.nan if pd.isna(non_holdout_total_orders_cuped) or non_holdout_total_orders_cuped == 0\n",
    "    else non_holdout_total_flgp_cuped / non_holdout_total_orders_cuped\n",
    "    )\n",
    "\n",
    "    holdout_orders_per_user_cuped = holdout_orders_per_user_adjusted_post\n",
    "    non_holdout_orders_per_user_cuped = non_holdout_orders_per_user_adjusted_post\n",
    "\n",
    "    # Incremental Differences (CUPED)\n",
    "    incremental_orders_cuped = non_holdout_total_orders_cuped - scaled_holdout_total_orders_cuped\n",
    "    incremental_flgp_cuped = non_holdout_total_flgp_cuped - scaled_holdout_total_flgp_cuped\n",
    "\n",
    "    # Percentage Changes (CUPED)\n",
    "    percentage_change_orders_cuped = ((incremental_orders_cuped) / abs(scaled_holdout_total_orders_cuped)) * 100 if scaled_holdout_total_orders_cuped != 0 else np.nan\n",
    "    percentage_change_flgp_cuped = ((incremental_flgp_cuped) / abs(scaled_holdout_total_flgp_cuped)) * 100 if scaled_holdout_total_flgp_cuped != 0 else np.nan\n",
    "\n",
    "    # Sustainable Growth Calculation\n",
    "    sustainable_growth = ((incremental_orders_cuped + (incremental_flgp_cuped / abs(non_holdout_flgp_per_order_cuped))) / scaled_holdout_total_orders_cuped) * 100 if scaled_holdout_total_orders_cuped != 0 else np.nan\n",
    "\n",
    "    # T-tests for significance\n",
    "    t_stat_orders, p_value_orders = ttest_ind(\n",
    "        data_orders.loc[data_orders['is_customer_holdout'], 'orders_post_cuped'],\n",
    "        data_orders.loc[~data_orders['is_customer_holdout'], 'orders_post_cuped'],\n",
    "        equal_var=False\n",
    "    )\n",
    "\n",
    "    t_stat_flgp, p_value_flgp = ttest_ind(\n",
    "        data_flgp.loc[data_flgp['is_customer_holdout'], 'flgp_post_cuped'],\n",
    "        data_flgp.loc[~data_flgp['is_customer_holdout'], 'flgp_post_cuped'],\n",
    "        equal_var=False\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'entity': entity,\n",
    "        'sustainable_growth': sustainable_growth,\n",
    "        'percentage_change_orders_cuped': percentage_change_orders_cuped,\n",
    "        'incremental_orders_cuped': incremental_orders_cuped,\n",
    "        'non_holdout_total_orders_cuped':non_holdout_total_orders_cuped,\n",
    "        'holdout_total_orders_cuped':scaled_holdout_total_orders_cuped,\n",
    "        't_stat_orders': t_stat_orders,\n",
    "        'p_value_orders': p_value_orders,\n",
    "        'percentage_change_flgp_cuped': percentage_change_flgp_cuped,\n",
    "        'incremental_flgp_cuped': incremental_flgp_cuped,\n",
    "        'non_holdout_total_flgp_cuped': non_holdout_total_flgp_cuped,\n",
    "        'holdout_total_flgp_cuped':scaled_holdout_total_flgp_cuped,\n",
    "        't_stat_flgp': t_stat_flgp,\n",
    "        'p_value_flgp': p_value_flgp,\n",
    "        'holdout_flgp_per_order_cuped': holdout_flgp_per_order_cuped,\n",
    "        'non_holdout_flgp_per_order_cuped': non_holdout_flgp_per_order_cuped,\n",
    "        'holdout_orders_per_user_cuped': holdout_orders_per_user_cuped,\n",
    "        'non_holdout_orders_per_user_cuped': non_holdout_orders_per_user_cuped\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(project , mkt_data, dps_data):\n",
    "    \n",
    "    project_id = project\n",
    "    logging.info(f\"Initializing BigQuery client for project: {project_id}\")\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    start_time = time.time()\n",
    "    combined_df = combined_data(client,mkt_data, dps_data)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"Time to extract and combine data from DB: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def create_growth_dataframe(df):\n",
    "    \n",
    "    metric_pairs = [\n",
    "        ('analytical_profit_pre', 'analytical_profit_post'),\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    for pre_metric, post_metric in metric_pairs:\n",
    "        for entity in df['entity_id'].unique():\n",
    "            entity_data = df[df['entity_id'] == entity]\n",
    "            try:\n",
    "                result = calculate_sustainable_growth(entity_data, pre_metric, post_metric)\n",
    "                result['metric_used'] = f\"{pre_metric}_vs_{post_metric}\"\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Skipping entity {entity} due to error: {e}\")\n",
    "                continue\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"Time to calculate sustainable growth: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    final_results_df = pd.DataFrame(results)\n",
    "    final_results_df = final_results_df.dropna(subset=['sustainable_growth'])\n",
    "    \n",
    "    # Determine the current week's start (Monday)\n",
    "    today = datetime.today().date()\n",
    "    week_start = today - timedelta(days=today.weekday())\n",
    "    final_results_df['updated_date'] = week_start\n",
    "    \n",
    "    csv_filename = f\"profitable_growth_{week_start}.csv\"\n",
    "    final_results_df.to_csv(csv_filename, index=False)\n",
    "    logging.info(f\"CSV saved as {csv_filename}\")\n",
    "    \n",
    "    return final_results_df\n",
    "\n",
    "def push_data_to_bigquery(project,df, dataset, table_name, schema):\n",
    "    \n",
    "    project_id = project\n",
    "    try:\n",
    "        bigquery_client = bigquery.Client(project=project_id)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to initialize BigQuery client for project {project_id_new}: {e}\")\n",
    "        raise e\n",
    "\n",
    "    try:\n",
    "        bq_create_dataset(bigquery_client, dataset)\n",
    "        bq_create_table(bigquery_client, dataset, table_name, schema)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating dataset/table: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    try:\n",
    "        insert_df_rows_bigquery(\n",
    "            client=bigquery_client,\n",
    "            dataset_id=dataset,\n",
    "            table_id=table_name,\n",
    "            df=df\n",
    "        )\n",
    "        logging.info(\"Data inserted successfully into BigQuery table.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inserting data into BigQuery: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-24 10:52:07,559 INFO:Initializing BigQuery client for project: logistics-customer-staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-24 10:52:07,945 WARNING:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
     ]
    },
    {
     "ename": "BadRequest",
     "evalue": "400 Name gmv_paid_online not found inside t; failed to parse view 'fulfillment-dwh-production.curated_data_filtered_mkt.bima_order_profitability' at [9:7]; reason: invalidQuery, location: query, message: Name gmv_paid_online not found inside t; failed to parse view 'fulfillment-dwh-production.curated_data_filtered_mkt.bima_order_profitability' at [9:7]\n\nLocation: US\nJob ID: 216e5f9e-c5a3-4230-9f67-199d9273171b\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# GET DATA FROM DB\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     raw_data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogistics-customer-staging\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmkt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdps_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# CLEAN DATA\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#make sure 0's are converted to NaN's where applicable\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     raw_data_cleaned \u001b[38;5;241m=\u001b[39m apply_nan_to_orders(raw_data)\n",
      "Cell \u001b[0;32mIn[98], line 12\u001b[0m, in \u001b[0;36mextract_data\u001b[0;34m(project, mkt_data, dps_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 12\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmkt_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdps_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     14\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime to extract and combine data from DB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[96], line 165\u001b[0m, in \u001b[0;36mcombined_data\u001b[0;34m(client, mkt, dps)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombined_data\u001b[39m(client,mkt, dps):\n\u001b[0;32m--> 165\u001b[0m   mkt_df \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmkt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m   dps_df \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(dps)\u001b[38;5;241m.\u001b[39mto_dataframe()\n\u001b[1;32m    168\u001b[0m   \u001b[38;5;66;03m# Append DataFrames\u001b[39;00m\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:2052\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dataframe\u001b[39m(\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1824\u001b[0m     bqstorage_client: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     ] \u001b[38;5;241m=\u001b[39m DefaultPandasDTypes\u001b[38;5;241m.\u001b[39mRANGE_TIMESTAMP_DTYPE,\n\u001b[1;32m   1845\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m \n\u001b[1;32m   1848\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;124;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2052\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[43mwait_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result\u001b[38;5;241m.\u001b[39mto_dataframe(\n\u001b[1;32m   2054\u001b[0m         bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[1;32m   2055\u001b[0m         dtypes\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2069\u001b[0m         range_timestamp_dtype\u001b[38;5;241m=\u001b[39mrange_timestamp_dtype,\n\u001b[1;32m   2070\u001b[0m     )\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/_tqdm_helpers.py:107\u001b[0m, in \u001b[0;36mwait_for_query\u001b[0;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[1;32m    103\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m get_progress_bar(\n\u001b[1;32m    104\u001b[0m     progress_bar_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery is running\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_total, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:1676\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1671\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1674\u001b[0m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[0;32m-> 1676\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1677\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/global_pricing/.venv/lib/python3.12/site-packages/google/cloud/bigquery/job/query.py:1625\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.is_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[1;32m   1604\u001b[0m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[1;32m   1623\u001b[0m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[1;32m   1624\u001b[0m     restart_query_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reload_query_results(\n\u001b[1;32m   1637\u001b[0m         retry\u001b[38;5;241m=\u001b[39mretry, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreload_query_results_kwargs\n\u001b[1;32m   1638\u001b[0m     )\n",
      "\u001b[0;31mBadRequest\u001b[0m: 400 Name gmv_paid_online not found inside t; failed to parse view 'fulfillment-dwh-production.curated_data_filtered_mkt.bima_order_profitability' at [9:7]; reason: invalidQuery, location: query, message: Name gmv_paid_online not found inside t; failed to parse view 'fulfillment-dwh-production.curated_data_filtered_mkt.bima_order_profitability' at [9:7]\n\nLocation: US\nJob ID: 216e5f9e-c5a3-4230-9f67-199d9273171b\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # GET DATA FROM DB\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    raw_data = extract_data(\"logistics-customer-staging\",mkt_data, dps_data)\n",
    "\n",
    "    # ------------------------------------------------------------------------------\n",
    "    # CLEAN DATA\n",
    "    # ------------------------------------------------------------------------------\n",
    "\n",
    "    #make sure 0's are converted to NaN's where applicable\n",
    "    raw_data_cleaned = apply_nan_to_orders(raw_data)\n",
    "    \n",
    "    #remove customers with no data in the pre period or the post period or in both periods\n",
    "    needed_cols = ['analytical_profit_pre', 'analytical_profit_post','orders_pre','orders_post']\n",
    "    raw_data_final = drop_missing_data(raw_data_cleaned, needed_cols)\n",
    "\n",
    "    #create csv with data\n",
    "    #create_csv(raw_data_final, \"profitable_growth_raw\")\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Calculate Sustainable Growth\n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    sustainable_df = create_growth_dataframe(raw_data_final)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------\n",
    "    # Push Data to DB\n",
    "    # ------------------------------------------------------------------------------\n",
    "    \n",
    "    schema = [\n",
    "            bigquery.SchemaField('entity', 'STRING', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('sustainable_growth', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('percentage_change_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('incremental_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_total_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_total_orders_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('t_stat_orders', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('p_value_orders', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('percentage_change_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('incremental_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_total_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_total_flgp_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('t_stat_flgp', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('p_value_flgp', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_flgp_per_order_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_flgp_per_order_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('holdout_orders_per_user_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('non_holdout_orders_per_user_cuped', 'FLOAT', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('metric_used', 'STRING', mode='REQUIRED'),\n",
    "            bigquery.SchemaField('updated_date', 'DATE', mode='REQUIRED'),\n",
    "        ]\n",
    "\n",
    "    folder = \"shazeb\"\n",
    "    table_name = \"abc_performance_backup\"\n",
    "\n",
    "    # folder = \"long_term_pricing\"\n",
    "    # table_name = \"abc_performance\"\n",
    "    push_data_to_bigquery('logistics-data-storage-staging', sustainable_df, folder, table_name, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/_default.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">76</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> UserWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a </span><span style=\"color: #808000; text-decoration-color: #808000\">\"quota exceeded\"</span><span style=\"color: #808000; text-decoration-color: #808000\"> or </span><span style=\"color: #808000; text-decoration-color: #808000\">\"API not enabled\"</span><span style=\"color: #808000; text-decoration-color: #808000\"> error. See the following page for troubleshooting: </span><span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/\u001b[0m\u001b[1;33m_default.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m76\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \u001b[0m\u001b[33m\"quota exceeded\"\u001b[0m\u001b[33m or \u001b[0m\u001b[33m\"API not enabled\"\u001b[0m\u001b[33m error. See the following page for troubleshooting: \u001b[0m\u001b[4;33mhttps://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-02-20T01:28:49.334+0100\u001b[0m] {\u001b[34m_default.py:\u001b[0m683} \u001b[33mWARNING\u001b[0m - \u001b[33mNo project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\u001b[0m\n",
      "Dataset Dataset(DatasetReference('logistics-data-storage-staging', 'shazeb')) already exists.\n",
      "table abc_performance_backup created.\n",
      "Successfully inserted 52 rows into shazeb.abc_performance_backup.\n",
      "[\u001b[34m2025-02-20T01:28:51.264+0100\u001b[0m] {\u001b[34m4238489762.py:\u001b[0m76} INFO\u001b[0m - Data inserted successfully into BigQuery table.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# project_id = \"logistics-data-storage-staging\"\n",
    "# dataset_id = \"shazeb\"\n",
    "# table_name = \"abc_performance_backup\"\n",
    "# push_data_to_bigquery('logistics-data-storage-staging', sustainable_df, folder, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-20 20:26:27,142 WARNING:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n",
      "Table shazeb.abc_performance_backup deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# project_id = 'logistics-data-storage-staging'\n",
    "# try:\n",
    "#     bigquery_client = bigquery.Client(project=project_id)\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Failed to initialize BigQuery client for project {project_id_new}: {e}\")\n",
    "#     raise e\n",
    "\n",
    "# drop_table(bigquery_client, \"shazeb\", \"abc_performance_backup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/_default.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">76</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> UserWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a </span><span style=\"color: #808000; text-decoration-color: #808000\">\"quota exceeded\"</span><span style=\"color: #808000; text-decoration-color: #808000\"> or </span><span style=\"color: #808000; text-decoration-color: #808000\">\"API not enabled\"</span><span style=\"color: #808000; text-decoration-color: #808000\"> error. See the following page for troubleshooting: </span><span style=\"color: #808000; text-decoration-color: #808000; text-decoration: underline\">https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/Users/shazeb.asad/global_pricing/.venv/lib/python3.12/site-packages/google/auth/\u001b[0m\u001b[1;33m_default.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m76\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \u001b[0m\u001b[33m\"quota exceeded\"\u001b[0m\u001b[33m or \u001b[0m\u001b[33m\"API not enabled\"\u001b[0m\u001b[33m error. See the following page for troubleshooting: \u001b[0m\u001b[4;33mhttps://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-02-17T22:52:38.979+0100\u001b[0m] {\u001b[34m_default.py:\u001b[0m683} \u001b[33mWARNING\u001b[0m - \u001b[33mNo project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\u001b[0m\n",
      "Table shazeb.abc_performance deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "# project_id = 'logistics-data-storage-staging'\n",
    "# try:\n",
    "#     bigquery_client = bigquery.Client(project=project_id)\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"Failed to initialize BigQuery client for project {project_id_new}: {e}\")\n",
    "#     raise e\n",
    "\n",
    "# drop_table(bigquery_client, \"shazeb\", \"abc_performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
